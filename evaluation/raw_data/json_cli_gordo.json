{
  "responseHeader":{
    "status":0,
    "QTime":16},
  "response":{"numFound":185,"start":0,"numFoundExact":true,"docs":[
      {
        "story_id":21858952,
        "story_author":"pcr910303",
        "story_descendants":38,
        "story_score":117,
        "story_time":"2019-12-22T19:07:21Z",
        "story_title":"JSON on the Command Line with Jq",
        "search":["JSON on the Command Line with Jq",
          "Normal",
          "https://shapeshed.com/jq-json/",
          "HomePostsContactLast updated Saturday, Nov 16, 2019A series of how to examples on using jq, a command-line JSON processorEstimated reading time: 4 minutesTable of contentsHow to pretty print JSONHow to use pipes with jqHow to find a key and valueHow to find items in an arrayHow to combine filtersHow to transform JSONHow to transform data values within JSONHow to remove keys from JSONHow to map valuesHow to wrangle JSON how you wantFurther readingjq is a fantastic command-line JSON processor. It plays nice with UNIX pipes and offers extensive functionality for interrogating, manipulating and working with JSON file.How to pretty print JSONjq can do a lot but probably the highest frequency use for most users is to pretty print JSON either from a file or after a network call. Suppose that we have a file names.json containing the following json.[{\"id\": 1, \"name\": \"Arthur\", \"age\": \"21\"},{\"id\": 2, \"name\": \"Richard\", \"age\": \"32\"}] jq can pretty print this file using the . filter. This takes the entire input and sends it to standard output. Unless told not to jq will pretty print making JSON readable.jq '.' names.json [ { \"id\": 1, \"name\": \"Arthur\", \"age\": \"21\" }, { \"id\": 2, \"name\": \"Richard\", \"age\": \"32\" } ] How to use pipes with jqBecause jq is UNIX friendly it is possible to pipe data in and out of it. This can be useful for using jq as a filter or interacting with other tools. In the following pipeline cat pipes the file into jq and this is piped onto less. This can be very useful for viewing large JSON files.cat names.json | jq '.' | less How to find a key and valueTo find a key and value jq can filter based on keys and return the value. Suppose we have the following simple JSON document saved as dog.json.{ \"name\": \"Buster\", \"breed\": \"Golden Retriever\", \"age\": \"4\", \"owner\": { \"name\": \"Sally\" }, \"likes\": [ \"bones\", \"balls\", \"dog biscuits\" ] } jq can retrieve values from this document by passing key names.jq '.name' \"Buster\" Multiple keys can by passed separated by commas.jq '.breed,.age' \"Golden Retriever\" \"4\" To search for nested objects chain values using the dot operator just as you would in JavaScript.jq '.owner.name' \"Sally\" How to find items in an arrayTo search for items in arrays use bracket syntax with the index starting at 0.jq '.likes[0]' \"bones\" Multiple elements of an array may also be returned.echo '[\"a\",\"b\",\"c\",\"d\",\"e\"]' | jq '.[2:4]' [ \"c\", \"d\" ] How to combine filtersjq can combine filters to search within a selection. For the following JSON document suppose that the names need to be filtered.[ { \"id\": 1, \"name\": \"Arthur\", \"age\": \"21\" }, { \"id\": 2, \"name\": \"Richard\", \"age\": \"32\" } ] This can be achieved with a pipe with the jq filter.jq '.[] | .name' names.json \"Arthur\" \"Richard\" How to transform JSONjq can be used for more than just reading values from a JSON object. It can also transform JSON into new data structures. Returning to the dog.json example earlier a new array can be created containing the name and likes as follows.jq '[.name, .likes[]]' dog.json [ \"Buster\", \"Bones\", \"balls\", \"dog biscuits\" ] This can be very useful in data transform pipelines to shift JSON data from one structure to another.How to transform data values within JSONjq can also operate on data within JSON objects. Suppose the following JSON file exists and is saved as inventory.json.{ \"eggs\": 2, \"cheese\": 1, \"milk\": 1 } jq can be used to perform basic arithmetic on number values.jq '.eggs + 1' inventory.json 3 How to remove keys from JSONjq can remove keys from JSON objects. This outputs the JSON object without the deleted key. Suppose the following JSON is saved as cheeses.json.{ \"maroilles\": \"stinky\", \"goat\": \"mild\" } jq can remove keys as follows leaving just wonderful stinky cheese.jq 'del(.goat)' cheeses.json { \"maroilles\": \"stinky\" } How to map valuesjq can map values and perform an operation on each one. In the following example each item in an array is mapped and has two subtracted.echo '[12,14,15]' | jq 'map(.-2)' [ 10, 12, 13 ] How to wrangle JSON how you wantjq has many more advanced features to help manipulating and wrangling JSON however you want to. For more run man jq.Further readingjq project pagejq manualjq is sed for JSONbash that JSON (jq)Parsing JSON with jqHave an update or suggestion for this article? You can edit it here and send me a pull request.TagsRecent Posts ",
          "Thread from 2016: <a href=\"https://news.ycombinator.com/item?id=13090604\" rel=\"nofollow\">https://news.ycombinator.com/item?id=13090604</a><p>2015: <a href=\"https://news.ycombinator.com/item?id=9446980\" rel=\"nofollow\">https://news.ycombinator.com/item?id=9446980</a><p>2014: <a href=\"https://news.ycombinator.com/item?id=7895076\" rel=\"nofollow\">https://news.ycombinator.com/item?id=7895076</a><p>2013: <a href=\"https://news.ycombinator.com/item?id=5734683\" rel=\"nofollow\">https://news.ycombinator.com/item?id=5734683</a><p>2012: <a href=\"https://news.ycombinator.com/item?id=4985250\" rel=\"nofollow\">https://news.ycombinator.com/item?id=4985250</a><p><a href=\"https://news.ycombinator.com/item?id=4679933\" rel=\"nofollow\">https://news.ycombinator.com/item?id=4679933</a><p>(for the curious)",
          "Every time I try to do something involved with jq (i.e. more than a couple commands deep) and inevitably end up wandering through the manual, I get a persistent feeling that jq could be cleaned up quite a bit. That it either has structure throughout that I fail to grasp, or that it's really rather disorganized and convoluted in places. This is exacerbated by me never being able to find anything in the manual without skimming half of it each single time.<p>E.g.:<p>Filtering and transforming objects by reaching a few levels deeper in them seems to be way easier to slap together in Python's list comprehensions or some functional notation. Stuff like [{ * * x, qwe: x.qwe*2} for x in a.b if x.y.z == 1].<p>Add nested lists to the above, and I can spend another fifteen minutes writing a one-liner (i.e. `a` is a list of objects with lists in some field).<p>I once had to deal with a conditional structure where a field might be one object or a list of them. Hoo boy.<p>Every time I want to check the number of entries my filters print out, I need to wrap them in an array. Repeat this a couple dozen times during writing a complex query. Weirdly, this is where strict structure gets in the way―and while I understand the reasoning, I feel that something could be done for such basic need. (Like, aggregate filters should be able to aggregate instead of acting on individual items? Maybe it's already in the language, but I'm not in the mood to go over the manual again.)<p>Variables seem to be bolted on as an afterthought, or at least the syntax doesn't exactly accommodate them. Meanwhile, they're necessary to implement some filters omitted in the language. Compare that to Lisp's simple `(let)`. IIRC the manual also says that jq has some semblance of functions, but I'm afraid to think about them with this syntax.<p>I like the idea a lot, but execution not so much. Frankly I'll probably end up throwing together a script in Lumo or something, that will accept Lisp expressions and feed JSON structure to them. (I'd use Fennel, but JSON has actual null while Lua... doesn't.)<p>Btw, I have pretty much the same sentiment about Git. Git structures, great. Git tools, oy vey. Maybe I need Lisp for Git, or at least Python for Git."],
        "story_type":"Normal",
        "url_raw":"https://shapeshed.com/jq-json/",
        "comments.comment_id":[21859335,
          21860107],
        "comments.comment_author":["dang",
          "aasasd"],
        "comments.comment_descendants":[0,
          5],
        "comments.comment_time":["2019-12-22T20:11:17Z",
          "2019-12-22T22:34:41Z"],
        "comments.comment_text":["Thread from 2016: <a href=\"https://news.ycombinator.com/item?id=13090604\" rel=\"nofollow\">https://news.ycombinator.com/item?id=13090604</a><p>2015: <a href=\"https://news.ycombinator.com/item?id=9446980\" rel=\"nofollow\">https://news.ycombinator.com/item?id=9446980</a><p>2014: <a href=\"https://news.ycombinator.com/item?id=7895076\" rel=\"nofollow\">https://news.ycombinator.com/item?id=7895076</a><p>2013: <a href=\"https://news.ycombinator.com/item?id=5734683\" rel=\"nofollow\">https://news.ycombinator.com/item?id=5734683</a><p>2012: <a href=\"https://news.ycombinator.com/item?id=4985250\" rel=\"nofollow\">https://news.ycombinator.com/item?id=4985250</a><p><a href=\"https://news.ycombinator.com/item?id=4679933\" rel=\"nofollow\">https://news.ycombinator.com/item?id=4679933</a><p>(for the curious)",
          "Every time I try to do something involved with jq (i.e. more than a couple commands deep) and inevitably end up wandering through the manual, I get a persistent feeling that jq could be cleaned up quite a bit. That it either has structure throughout that I fail to grasp, or that it's really rather disorganized and convoluted in places. This is exacerbated by me never being able to find anything in the manual without skimming half of it each single time.<p>E.g.:<p>Filtering and transforming objects by reaching a few levels deeper in them seems to be way easier to slap together in Python's list comprehensions or some functional notation. Stuff like [{ * * x, qwe: x.qwe*2} for x in a.b if x.y.z == 1].<p>Add nested lists to the above, and I can spend another fifteen minutes writing a one-liner (i.e. `a` is a list of objects with lists in some field).<p>I once had to deal with a conditional structure where a field might be one object or a list of them. Hoo boy.<p>Every time I want to check the number of entries my filters print out, I need to wrap them in an array. Repeat this a couple dozen times during writing a complex query. Weirdly, this is where strict structure gets in the way―and while I understand the reasoning, I feel that something could be done for such basic need. (Like, aggregate filters should be able to aggregate instead of acting on individual items? Maybe it's already in the language, but I'm not in the mood to go over the manual again.)<p>Variables seem to be bolted on as an afterthought, or at least the syntax doesn't exactly accommodate them. Meanwhile, they're necessary to implement some filters omitted in the language. Compare that to Lisp's simple `(let)`. IIRC the manual also says that jq has some semblance of functions, but I'm afraid to think about them with this syntax.<p>I like the idea a lot, but execution not so much. Frankly I'll probably end up throwing together a script in Lumo or something, that will accept Lisp expressions and feed JSON structure to them. (I'd use Fennel, but JSON has actual null while Lua... doesn't.)<p>Btw, I have pretty much the same sentiment about Git. Git structures, great. Git tools, oy vey. Maybe I need Lisp for Git, or at least Python for Git."],
        "id":"a6253e53-b308-410c-9f87-de4aad819cfc",
        "url_text":"HomePostsContactLast updated Saturday, Nov 16, 2019A series of how to examples on using jq, a command-line JSON processorEstimated reading time: 4 minutesTable of contentsHow to pretty print JSONHow to use pipes with jqHow to find a key and valueHow to find items in an arrayHow to combine filtersHow to transform JSONHow to transform data values within JSONHow to remove keys from JSONHow to map valuesHow to wrangle JSON how you wantFurther readingjq is a fantastic command-line JSON processor. It plays nice with UNIX pipes and offers extensive functionality for interrogating, manipulating and working with JSON file.How to pretty print JSONjq can do a lot but probably the highest frequency use for most users is to pretty print JSON either from a file or after a network call. Suppose that we have a file names.json containing the following json.[{\"id\": 1, \"name\": \"Arthur\", \"age\": \"21\"},{\"id\": 2, \"name\": \"Richard\", \"age\": \"32\"}] jq can pretty print this file using the . filter. This takes the entire input and sends it to standard output. Unless told not to jq will pretty print making JSON readable.jq '.' names.json [ { \"id\": 1, \"name\": \"Arthur\", \"age\": \"21\" }, { \"id\": 2, \"name\": \"Richard\", \"age\": \"32\" } ] How to use pipes with jqBecause jq is UNIX friendly it is possible to pipe data in and out of it. This can be useful for using jq as a filter or interacting with other tools. In the following pipeline cat pipes the file into jq and this is piped onto less. This can be very useful for viewing large JSON files.cat names.json | jq '.' | less How to find a key and valueTo find a key and value jq can filter based on keys and return the value. Suppose we have the following simple JSON document saved as dog.json.{ \"name\": \"Buster\", \"breed\": \"Golden Retriever\", \"age\": \"4\", \"owner\": { \"name\": \"Sally\" }, \"likes\": [ \"bones\", \"balls\", \"dog biscuits\" ] } jq can retrieve values from this document by passing key names.jq '.name' \"Buster\" Multiple keys can by passed separated by commas.jq '.breed,.age' \"Golden Retriever\" \"4\" To search for nested objects chain values using the dot operator just as you would in JavaScript.jq '.owner.name' \"Sally\" How to find items in an arrayTo search for items in arrays use bracket syntax with the index starting at 0.jq '.likes[0]' \"bones\" Multiple elements of an array may also be returned.echo '[\"a\",\"b\",\"c\",\"d\",\"e\"]' | jq '.[2:4]' [ \"c\", \"d\" ] How to combine filtersjq can combine filters to search within a selection. For the following JSON document suppose that the names need to be filtered.[ { \"id\": 1, \"name\": \"Arthur\", \"age\": \"21\" }, { \"id\": 2, \"name\": \"Richard\", \"age\": \"32\" } ] This can be achieved with a pipe with the jq filter.jq '.[] | .name' names.json \"Arthur\" \"Richard\" How to transform JSONjq can be used for more than just reading values from a JSON object. It can also transform JSON into new data structures. Returning to the dog.json example earlier a new array can be created containing the name and likes as follows.jq '[.name, .likes[]]' dog.json [ \"Buster\", \"Bones\", \"balls\", \"dog biscuits\" ] This can be very useful in data transform pipelines to shift JSON data from one structure to another.How to transform data values within JSONjq can also operate on data within JSON objects. Suppose the following JSON file exists and is saved as inventory.json.{ \"eggs\": 2, \"cheese\": 1, \"milk\": 1 } jq can be used to perform basic arithmetic on number values.jq '.eggs + 1' inventory.json 3 How to remove keys from JSONjq can remove keys from JSON objects. This outputs the JSON object without the deleted key. Suppose the following JSON is saved as cheeses.json.{ \"maroilles\": \"stinky\", \"goat\": \"mild\" } jq can remove keys as follows leaving just wonderful stinky cheese.jq 'del(.goat)' cheeses.json { \"maroilles\": \"stinky\" } How to map valuesjq can map values and perform an operation on each one. In the following example each item in an array is mapped and has two subtracted.echo '[12,14,15]' | jq 'map(.-2)' [ 10, 12, 13 ] How to wrangle JSON how you wantjq has many more advanced features to help manipulating and wrangling JSON however you want to. For more run man jq.Further readingjq project pagejq manualjq is sed for JSONbash that JSON (jq)Parsing JSON with jqHave an update or suggestion for this article? You can edit it here and send me a pull request.TagsRecent Posts ",
        "_version_":1718938254307229696},
      {
        "story_id":21240641,
        "story_author":"sharkdp",
        "story_descendants":25,
        "story_score":96,
        "story_time":"2019-10-13T14:50:12Z",
        "story_title":"Show HN: Hyperfine – a command-line benchmarking tool",
        "search":["Show HN: Hyperfine – a command-line benchmarking tool",
          "ShowHN",
          "https://github.com/sharkdp/hyperfine",
          " A command-line benchmarking tool. Demo: Benchmarking fd and find: Features Statistical analysis across multiple runs. Support for arbitrary shell commands. Constant feedback about the benchmark progress and current estimates. Warmup runs can be executed before the actual benchmark. Cache-clearing commands can be set up before each timing run. Statistical outlier detection to detect interference from other programs and caching effects. Export results to various formats: CSV, JSON, Markdown, AsciiDoc. Parameterized benchmarks (e.g. vary the number of threads). Cross-platform Usage Basic benchmark To run a benchmark, you can simply call hyperfine <command>.... The argument(s) can be any shell command. For example: Hyperfine will automatically determine the number of runs to perform for each command. By default, it will perform at least 10 benchmarking runs. To change this, you can use the -m/--min-runs option: hyperfine --min-runs 5 'sleep 0.2' 'sleep 3.2' Warmup runs and preparation commands If the program execution time is limited by disk I/O, the benchmarking results can be heavily influenced by disk caches and whether they are cold or warm. If you want to run the benchmark on a warm cache, you can use the -w/--warmup option to perform a certain number of program executions before the actual benchmark: hyperfine --warmup 3 'grep -R TODO *' Conversely, if you want to run the benchmark for a cold cache, you can use the -p/--prepare option to run a special command before each timing run. For example, to clear harddisk caches on Linux, you can run sync; echo 3 | sudo tee /proc/sys/vm/drop_caches To use this specific command with Hyperfine, call sudo -v to temporarily gain sudo permissions and then call: hyperfine --prepare 'sync; echo 3 | sudo tee /proc/sys/vm/drop_caches' 'grep -R TODO *' Parameterized benchmarks If you want to run a benchmark where only a single parameter is varied (say, the number of threads), you can use the -P/--parameter-scan option and call: hyperfine --prepare 'make clean' --parameter-scan num_threads 1 12 'make -j {num_threads}' This also works with decimal numbers. The -D/--parameter-step-size option can be used to control the step size: hyperfine --parameter-scan delay 0.3 0.7 -D 0.2 'sleep {delay}' This runs sleep 0.3, sleep 0.5 and sleep 0.7. Shell functions and aliases If you are using bash, you can export shell functions to directly benchmark them with hyperfine: $ my_function() { sleep 1; } $ export -f my_function $ hyperfine my_function If you are using a different shell, or if you want to benchmark shell aliases, you may try to put them in a separate file: echo 'my_function() { sleep 1 }' > /tmp/my_function.sh echo 'alias my_alias=\"sleep 1\"' > /tmp/my_alias.sh hyperfine 'source /tmp/my_function.sh; eval my_function' hyperfine 'source /tmp/my_alias.sh; eval my_alias' Export results Hyperfine has multiple options for exporting benchmark results: CSV, JSON, Markdown (see --help text for details). To export results to Markdown, for example, you can use the --export-markdown option that will create tables like this: Command Mean [s] Min [s] Max [s] Relative find . -iregex '.*[0-9]\\.jpg$' 2.275 0.046 2.243 2.397 9.79 0.22 find . -iname '*[0-9].jpg' 1.427 0.026 1.405 1.468 6.14 0.13 fd -HI '.*[0-9]\\.jpg$' 0.232 0.002 0.230 0.236 1.00 The JSON output is useful if you want to analyze the benchmark results in more detail. See the scripts/ folder for some examples. Installation On Ubuntu Download the appropriate .deb package from the Release page and install it via dpkg: wget https://github.com/sharkdp/hyperfine/releases/download/v1.12.0/hyperfine_1.12.0_amd64.deb sudo dpkg -i hyperfine_1.12.0_amd64.deb On Fedora On Fedora, hyperfine can be installed from the official repositories: On Alpine Linux On Alpine Linux, hyperfine can be installed from the official repositories: On Arch Linux On Arch Linux, hyperfine can be installed from the official repositories: On Funtoo Linux On Funtoo Linux, hyperfine can be installed from core-kit: emerge app-benchmarks/hyperfine On NixOS On NixOS, hyperfine can be installed from the official repositories: On Void Linux Hyperfine can be installed via xbps xbps-install -S hyperfine On macOS Hyperfine can be installed via Homebrew: Or you can install using MacPorts: sudo port selfupdate sudo port install hyperfine On FreeBSD Hyperfine can be installed via pkg: On OpenBSD With conda Hyperfine can be installed via conda from the conda-forge channel: conda install -c conda-forge hyperfine With cargo (Linux, macOS, Windows) Hyperfine can be installed via cargo: Make sure that you use Rust 1.46 or higher. From binaries (Linux, macOS, Windows) Download the corresponding archive from the Release page. Alternative tools Hyperfine is inspired by bench. Integration with other tools Chronologer is a tool that uses hyperfine to visualize changes in benchmark timings across your Git history. Make sure to check out the scripts folder in this repository for a set of tools to work with hyperfine benchmark results. Origin of the name The name hyperfine was chosen in reference to the hyperfine levels of caesium 133 which play a crucial role in the definition of our base unit of time the second. License hyperfine is dual-licensed under the terms of the MIT License and the Apache License 2.0. See the LICENSE-APACHE and LICENSE-MIT files for details. ",
          "I have submitted \"hyperfine\" 1.5 years ago when it just came out. Since then, the program has gained functionality (statistical outlier detection, result export, parametrized benchmarks) and maturity.<p>Old discussion: <a href=\"https://news.ycombinator.com/item?id=16193225\" rel=\"nofollow\">https://news.ycombinator.com/item?id=16193225</a><p>Looking forward to your feedback!",
          "Most -- nearly all -- benchmarking tools like this work from a normality assumption, i.e. assume that results follow the normal distribution, or is close to it. Some do this on blind faith, others argue from the CLT that \"with infinite samples, the mean is normally distributed, so surely it must be also with finite number of samples, at least a little?\"<p>In fact, performance numbers (latencies) often follow a heavy-tailed distribution. For these, you need a literal shitload of samples to get even a slightly normal mean. For these, the sample mean, the sample variance, the sample centiles -- they all severely underestimate the true values.<p>What's worse is when these tools start to remove \"outliers\". With a heavy-tailed distribution, the majority of samples don't contribute very much at all to the expectation. The strongest signal is found in the extreme values. The strongest signal is found in the stuff that is thrown out. The junk that's left is the noise, the stuff that doesn't tell you very much about what you're dealing with.<p>I stand firm in my belief that unless you can prove how CLT applies to your input distributions, you should not assume normality.<p>And if you don't know what you are doing, stop reporting means. Stop reporting centiles. Report the maximum value. That's a really boring thing to hear, but it is nearly always statistically  and analytically meaningful, so it is a good default."],
        "story_type":"ShowHN",
        "url_raw":"https://github.com/sharkdp/hyperfine",
        "comments.comment_id":[21240684,
          21251645],
        "comments.comment_author":["sharkdp",
          "kqr"],
        "comments.comment_descendants":[1,
          5],
        "comments.comment_time":["2019-10-13T14:57:05Z",
          "2019-10-14T19:09:09Z"],
        "comments.comment_text":["I have submitted \"hyperfine\" 1.5 years ago when it just came out. Since then, the program has gained functionality (statistical outlier detection, result export, parametrized benchmarks) and maturity.<p>Old discussion: <a href=\"https://news.ycombinator.com/item?id=16193225\" rel=\"nofollow\">https://news.ycombinator.com/item?id=16193225</a><p>Looking forward to your feedback!",
          "Most -- nearly all -- benchmarking tools like this work from a normality assumption, i.e. assume that results follow the normal distribution, or is close to it. Some do this on blind faith, others argue from the CLT that \"with infinite samples, the mean is normally distributed, so surely it must be also with finite number of samples, at least a little?\"<p>In fact, performance numbers (latencies) often follow a heavy-tailed distribution. For these, you need a literal shitload of samples to get even a slightly normal mean. For these, the sample mean, the sample variance, the sample centiles -- they all severely underestimate the true values.<p>What's worse is when these tools start to remove \"outliers\". With a heavy-tailed distribution, the majority of samples don't contribute very much at all to the expectation. The strongest signal is found in the extreme values. The strongest signal is found in the stuff that is thrown out. The junk that's left is the noise, the stuff that doesn't tell you very much about what you're dealing with.<p>I stand firm in my belief that unless you can prove how CLT applies to your input distributions, you should not assume normality.<p>And if you don't know what you are doing, stop reporting means. Stop reporting centiles. Report the maximum value. That's a really boring thing to hear, but it is nearly always statistically  and analytically meaningful, so it is a good default."],
        "id":"aeb9489b-badf-43a7-b8f7-9f3ca265f913",
        "url_text":" A command-line benchmarking tool. Demo: Benchmarking fd and find: Features Statistical analysis across multiple runs. Support for arbitrary shell commands. Constant feedback about the benchmark progress and current estimates. Warmup runs can be executed before the actual benchmark. Cache-clearing commands can be set up before each timing run. Statistical outlier detection to detect interference from other programs and caching effects. Export results to various formats: CSV, JSON, Markdown, AsciiDoc. Parameterized benchmarks (e.g. vary the number of threads). Cross-platform Usage Basic benchmark To run a benchmark, you can simply call hyperfine <command>.... The argument(s) can be any shell command. For example: Hyperfine will automatically determine the number of runs to perform for each command. By default, it will perform at least 10 benchmarking runs. To change this, you can use the -m/--min-runs option: hyperfine --min-runs 5 'sleep 0.2' 'sleep 3.2' Warmup runs and preparation commands If the program execution time is limited by disk I/O, the benchmarking results can be heavily influenced by disk caches and whether they are cold or warm. If you want to run the benchmark on a warm cache, you can use the -w/--warmup option to perform a certain number of program executions before the actual benchmark: hyperfine --warmup 3 'grep -R TODO *' Conversely, if you want to run the benchmark for a cold cache, you can use the -p/--prepare option to run a special command before each timing run. For example, to clear harddisk caches on Linux, you can run sync; echo 3 | sudo tee /proc/sys/vm/drop_caches To use this specific command with Hyperfine, call sudo -v to temporarily gain sudo permissions and then call: hyperfine --prepare 'sync; echo 3 | sudo tee /proc/sys/vm/drop_caches' 'grep -R TODO *' Parameterized benchmarks If you want to run a benchmark where only a single parameter is varied (say, the number of threads), you can use the -P/--parameter-scan option and call: hyperfine --prepare 'make clean' --parameter-scan num_threads 1 12 'make -j {num_threads}' This also works with decimal numbers. The -D/--parameter-step-size option can be used to control the step size: hyperfine --parameter-scan delay 0.3 0.7 -D 0.2 'sleep {delay}' This runs sleep 0.3, sleep 0.5 and sleep 0.7. Shell functions and aliases If you are using bash, you can export shell functions to directly benchmark them with hyperfine: $ my_function() { sleep 1; } $ export -f my_function $ hyperfine my_function If you are using a different shell, or if you want to benchmark shell aliases, you may try to put them in a separate file: echo 'my_function() { sleep 1 }' > /tmp/my_function.sh echo 'alias my_alias=\"sleep 1\"' > /tmp/my_alias.sh hyperfine 'source /tmp/my_function.sh; eval my_function' hyperfine 'source /tmp/my_alias.sh; eval my_alias' Export results Hyperfine has multiple options for exporting benchmark results: CSV, JSON, Markdown (see --help text for details). To export results to Markdown, for example, you can use the --export-markdown option that will create tables like this: Command Mean [s] Min [s] Max [s] Relative find . -iregex '.*[0-9]\\.jpg$' 2.275 0.046 2.243 2.397 9.79 0.22 find . -iname '*[0-9].jpg' 1.427 0.026 1.405 1.468 6.14 0.13 fd -HI '.*[0-9]\\.jpg$' 0.232 0.002 0.230 0.236 1.00 The JSON output is useful if you want to analyze the benchmark results in more detail. See the scripts/ folder for some examples. Installation On Ubuntu Download the appropriate .deb package from the Release page and install it via dpkg: wget https://github.com/sharkdp/hyperfine/releases/download/v1.12.0/hyperfine_1.12.0_amd64.deb sudo dpkg -i hyperfine_1.12.0_amd64.deb On Fedora On Fedora, hyperfine can be installed from the official repositories: On Alpine Linux On Alpine Linux, hyperfine can be installed from the official repositories: On Arch Linux On Arch Linux, hyperfine can be installed from the official repositories: On Funtoo Linux On Funtoo Linux, hyperfine can be installed from core-kit: emerge app-benchmarks/hyperfine On NixOS On NixOS, hyperfine can be installed from the official repositories: On Void Linux Hyperfine can be installed via xbps xbps-install -S hyperfine On macOS Hyperfine can be installed via Homebrew: Or you can install using MacPorts: sudo port selfupdate sudo port install hyperfine On FreeBSD Hyperfine can be installed via pkg: On OpenBSD With conda Hyperfine can be installed via conda from the conda-forge channel: conda install -c conda-forge hyperfine With cargo (Linux, macOS, Windows) Hyperfine can be installed via cargo: Make sure that you use Rust 1.46 or higher. From binaries (Linux, macOS, Windows) Download the corresponding archive from the Release page. Alternative tools Hyperfine is inspired by bench. Integration with other tools Chronologer is a tool that uses hyperfine to visualize changes in benchmark timings across your Git history. Make sure to check out the scripts folder in this repository for a set of tools to work with hyperfine benchmark results. Origin of the name The name hyperfine was chosen in reference to the hyperfine levels of caesium 133 which play a crucial role in the definition of our base unit of time the second. License hyperfine is dual-licensed under the terms of the MIT License and the Apache License 2.0. See the LICENSE-APACHE and LICENSE-MIT files for details. ",
        "_version_":1718938230013820928},
      {
        "story_id":21363121,
        "story_author":"signa11",
        "story_descendants":102,
        "story_score":636,
        "story_time":"2019-10-26T11:52:57Z",
        "story_title":"An Illustrated Guide to Useful Command Line Tools",
        "search":["An Illustrated Guide to Useful Command Line Tools",
          "Normal",
          "https://www.wezm.net/technical/2019/10/useful-command-line-tools/",
          "Published on Sat, 26 October 2019 Inspired by a similar post by Ben Boyter this a list of useful command line tools that I use. Its not a list of every tool I use. These are tools that are new or typically not part of a standard POSIX command line environment. This post is a living document and will be updated over time. It should be obvious that I have a strong preference for fast tools without a large runtime dependency like Python or node.js. Most of these tools are portable to *BSD, Linux, macOS. Many also work on Windows. For OSes that ship up to date software many are available via the system package repository. Last updated: 31 Oct 2019 About my CLI environment: I use the zsh shell, Pragmata Pro font, and base16 default dark color scheme. My prompt is generated by promptline. Table of Contents Alacritty Terminal emulator alt Find alternate files bat cat with syntax highlighting bb System monitor chars Unicode character search dot Dot files manager dust Disk usage analyser eva Calculator exa Replacement for ls fd Replacement for find hexyl Hex viewer hyperfine Benchmarking tool jq awk/XPath for JSON mdcat Render Markdown in the terminal pass Password manager Podman Docker alternative Restic Encrypted backup tool ripgrep Fast, intelligent grep shotgun Take screenshots skim Fuzzy finder slop Graphical region selection Syncthing Decentralised file synchronisation tig TUI for git titlecase Convert text to title case Universal Ctags Maintained ctags fork watchexec Run commands in response to file system changes z Jump to directories zola Static site compiler Changelog The changelog for this page Alacritty Alacritty is fast terminal emulator. Whilst not strictly a command line tool, it does host everything I do in the command line. It is the terminal emulator in use in all the screenshots on this page. Homepage alt alt is a tool for finding the alternate to a file. E.g. the header for an implementation or the test for an implementation. I use it paired with Neovim to easily toggle between tests and implementation. $ alt app/models/page.rb spec/models/page_spec.rb Homepage bat bat is an alternative to the common (mis)use of cat to print a file to the terminal. It supports syntax highlighting and git integration. Homepage bb bb is system monitor like top. It shows overall CPU and memory usage as well as detailed information per process. Homepage chars chars shows information about Unicode characters matching a search term. Homepage dot dot is a dotfiles manager. It maintains a set of symlinks according to a mappings file. I use it to manage my dotfiles. Homepage dust dust is an alternative du -sh. It calculates the size of a directory tree, printing a summary of the largest items. Homepage exa exa is a replacement for ls with sensible defaults and added features like a tree view, git integration, and optional icons. I have ls aliased to exa in my shell. Homepage eva eva is a command line calculator similar to bc, with syntax highlighting and persistent history. Homepage fd fd is an alternative to find and has a more user friendly command line interface and respects ignore files, like .gitignore. The combination of its speed and ignore file support make it excellent for searching for files in git repositories. Homepage hexyl hexyl is a hex viewer that uses Unicode characters and colour to make the output more readable. Homepage hyperfine hyperfine command line benchmarking tool. It allows you to benchmark commands with warmup and statistical analysis. Homepage jq jq is kind of like awk for JSON. It lets you transform and extract information from JSON documents. Homepage mdcat mdcat renders Markdown files in the terminal. In supported terminals (not Alacritty) links are clickable (without the url being visible like in a web browser) and images are rendered. Homepage pass pass is a password manager that uses GPG to store the passwords. I use it with the passff Firefox extension and Pass for iOS on my phone. Homepage Podman podman is an alternative to Docker that does not require a daemon. Containers are run as the user running Podman so files written into the host dont end up owned by root. The CLI is largely compatible with the docker CLI. Homepage Restic restic is a backup tool that performs client side encryption, de-duplication and supports a variety of local and remote storage backends. Homepage ripgrep ripgrep (rg) recursively searches file trees for content in files matching a regular expression. Its extremely fast, and respects ignore files and binary files by default. Homepage shotgun shotgun is a tool for taking screenshots on X.org based environments. All the screenshots in this post were taken with it. It pairs well with slop. $ shotgun $(slop -c 0,0,0,0.75 -l -f \"-i %i -g %g\") eva.png Homepage skim skim is a fuzzy finder. It can be used to fuzzy match input fed to it. I use it with Neovim and zsh for fuzzy matching file names. Homepage slop slop (Select Operation) presents a UI to select a region of the screen or a window and prints the region to stdout. Works well with shotgun. $ slop -c 0,0,0,0.75 -l -f \"-i %i -g %g\" -i 8389044 -g 1464x1008+291+818 Homepage Syncthing Syncthing is a decentralised file synchronisation tool. Like Dropbox but self hosted and without the need for a central third-party file store. Homepage tig tig is a ncurses TUI for git. Its great for reviewing and staging changes, viewing history and diffs. Homepage titlecase titlecase is a little tool I wrote to format text using a title case format described by John Gruber. It correctly handles punctuation, and words like iPhone. I use it to obtain consistent titles on all my blog posts. $ echo 'an illustrated guide to useful command line tools' | titlecase An Illustrated Guide to Useful Command Line Tools I typically use it from within Neovim where selected text is piped through it in-place. This is done by creating a visual selection and then typing: :!titlecase. Homepage Universal Ctags Universal Ctags is a fork of exuberant ctags that is actively maintained. ctags is used to generate a tags file that vim and other tools can use to navigate to the definition of symbols in files. $ ctags --recurse src Homepage watchexec watchexec is a file and directory watcher that can run commands in response to file-system changes. Handy for auto running tests or restarting a development web server when source files change. # run command on file change $ watchexec -w content cobalt build # kill and restart server on file change $ watchexec -w src -s SIGINT -r 'cargo run' Homepage z z tracks your most used directories and allows you to jump to them with a partial name. Homepage zola zola is a full-featured very fast static site compiler. Homepage Changelog 31 Oct 2019 Add bb, and brief descriptions to the table of contents 28 Oct 2019 Add hyperfine Comments Comments on Lobsters Comments on Hacker News Previous Post: What I Learnt Building a Lobsters TUI in Rust ",
          "I quite like this list, there are a number of utilities here that I already use on a daily basis. There are also a few utilities that I like that weren't on this list, or some alternatives to what was shown. Some off the top of my head:<p>- nnn[0] (C) - A terminal file manager, similar to ranger. It allows you to navigate directories, manipulate files, analyze disk usage, and fuzzy open files.<p>- ncdu[1] (C) - ncurses disk analyzer. Similar to du -sh, but allows for directory navigation as well.<p>- z.lua[2] (Lua) - It's an alternative to the z utility mentioned in the article. I haven't done the benchmarks, but they claim to be faster than z. I'm mostly just including it because it's what I use.<p>[0] <a href=\"https://github.com/jarun/nnn\" rel=\"nofollow\">https://github.com/jarun/nnn</a>\n[1] <a href=\"https://dev.yorhel.nl/ncdu\" rel=\"nofollow\">https://dev.yorhel.nl/ncdu</a>\n[2] <a href=\"https://github.com/skywind3000/z.lua\" rel=\"nofollow\">https://github.com/skywind3000/z.lua</a>",
          "An observation: I have always taken the meaning of the word \"illustrated\" to specifically refer to non-lexical graphics. Searching the dictionary definition of the word, I find a looser definition that applies to \"examples\" intended to aid an explanation.<p>This is a similar cognitive dissonance to when I first learned that \"Visual Basic\" and \"Visual Studio\" meant that the syntax of the displayed code was highlighted, not graphically represented in a non-lexical way."],
        "story_type":"Normal",
        "url_raw":"https://www.wezm.net/technical/2019/10/useful-command-line-tools/",
        "comments.comment_id":[21363660,
          21364159],
        "comments.comment_author":["aquova",
          "atonalfreerider"],
        "comments.comment_descendants":[4,
          2],
        "comments.comment_time":["2019-10-26T13:52:33Z",
          "2019-10-26T15:18:17Z"],
        "comments.comment_text":["I quite like this list, there are a number of utilities here that I already use on a daily basis. There are also a few utilities that I like that weren't on this list, or some alternatives to what was shown. Some off the top of my head:<p>- nnn[0] (C) - A terminal file manager, similar to ranger. It allows you to navigate directories, manipulate files, analyze disk usage, and fuzzy open files.<p>- ncdu[1] (C) - ncurses disk analyzer. Similar to du -sh, but allows for directory navigation as well.<p>- z.lua[2] (Lua) - It's an alternative to the z utility mentioned in the article. I haven't done the benchmarks, but they claim to be faster than z. I'm mostly just including it because it's what I use.<p>[0] <a href=\"https://github.com/jarun/nnn\" rel=\"nofollow\">https://github.com/jarun/nnn</a>\n[1] <a href=\"https://dev.yorhel.nl/ncdu\" rel=\"nofollow\">https://dev.yorhel.nl/ncdu</a>\n[2] <a href=\"https://github.com/skywind3000/z.lua\" rel=\"nofollow\">https://github.com/skywind3000/z.lua</a>",
          "An observation: I have always taken the meaning of the word \"illustrated\" to specifically refer to non-lexical graphics. Searching the dictionary definition of the word, I find a looser definition that applies to \"examples\" intended to aid an explanation.<p>This is a similar cognitive dissonance to when I first learned that \"Visual Basic\" and \"Visual Studio\" meant that the syntax of the displayed code was highlighted, not graphically represented in a non-lexical way."],
        "id":"7f110ca5-b4af-4144-bf3b-c3b5eee5da65",
        "url_text":"Published on Sat, 26 October 2019 Inspired by a similar post by Ben Boyter this a list of useful command line tools that I use. Its not a list of every tool I use. These are tools that are new or typically not part of a standard POSIX command line environment. This post is a living document and will be updated over time. It should be obvious that I have a strong preference for fast tools without a large runtime dependency like Python or node.js. Most of these tools are portable to *BSD, Linux, macOS. Many also work on Windows. For OSes that ship up to date software many are available via the system package repository. Last updated: 31 Oct 2019 About my CLI environment: I use the zsh shell, Pragmata Pro font, and base16 default dark color scheme. My prompt is generated by promptline. Table of Contents Alacritty Terminal emulator alt Find alternate files bat cat with syntax highlighting bb System monitor chars Unicode character search dot Dot files manager dust Disk usage analyser eva Calculator exa Replacement for ls fd Replacement for find hexyl Hex viewer hyperfine Benchmarking tool jq awk/XPath for JSON mdcat Render Markdown in the terminal pass Password manager Podman Docker alternative Restic Encrypted backup tool ripgrep Fast, intelligent grep shotgun Take screenshots skim Fuzzy finder slop Graphical region selection Syncthing Decentralised file synchronisation tig TUI for git titlecase Convert text to title case Universal Ctags Maintained ctags fork watchexec Run commands in response to file system changes z Jump to directories zola Static site compiler Changelog The changelog for this page Alacritty Alacritty is fast terminal emulator. Whilst not strictly a command line tool, it does host everything I do in the command line. It is the terminal emulator in use in all the screenshots on this page. Homepage alt alt is a tool for finding the alternate to a file. E.g. the header for an implementation or the test for an implementation. I use it paired with Neovim to easily toggle between tests and implementation. $ alt app/models/page.rb spec/models/page_spec.rb Homepage bat bat is an alternative to the common (mis)use of cat to print a file to the terminal. It supports syntax highlighting and git integration. Homepage bb bb is system monitor like top. It shows overall CPU and memory usage as well as detailed information per process. Homepage chars chars shows information about Unicode characters matching a search term. Homepage dot dot is a dotfiles manager. It maintains a set of symlinks according to a mappings file. I use it to manage my dotfiles. Homepage dust dust is an alternative du -sh. It calculates the size of a directory tree, printing a summary of the largest items. Homepage exa exa is a replacement for ls with sensible defaults and added features like a tree view, git integration, and optional icons. I have ls aliased to exa in my shell. Homepage eva eva is a command line calculator similar to bc, with syntax highlighting and persistent history. Homepage fd fd is an alternative to find and has a more user friendly command line interface and respects ignore files, like .gitignore. The combination of its speed and ignore file support make it excellent for searching for files in git repositories. Homepage hexyl hexyl is a hex viewer that uses Unicode characters and colour to make the output more readable. Homepage hyperfine hyperfine command line benchmarking tool. It allows you to benchmark commands with warmup and statistical analysis. Homepage jq jq is kind of like awk for JSON. It lets you transform and extract information from JSON documents. Homepage mdcat mdcat renders Markdown files in the terminal. In supported terminals (not Alacritty) links are clickable (without the url being visible like in a web browser) and images are rendered. Homepage pass pass is a password manager that uses GPG to store the passwords. I use it with the passff Firefox extension and Pass for iOS on my phone. Homepage Podman podman is an alternative to Docker that does not require a daemon. Containers are run as the user running Podman so files written into the host dont end up owned by root. The CLI is largely compatible with the docker CLI. Homepage Restic restic is a backup tool that performs client side encryption, de-duplication and supports a variety of local and remote storage backends. Homepage ripgrep ripgrep (rg) recursively searches file trees for content in files matching a regular expression. Its extremely fast, and respects ignore files and binary files by default. Homepage shotgun shotgun is a tool for taking screenshots on X.org based environments. All the screenshots in this post were taken with it. It pairs well with slop. $ shotgun $(slop -c 0,0,0,0.75 -l -f \"-i %i -g %g\") eva.png Homepage skim skim is a fuzzy finder. It can be used to fuzzy match input fed to it. I use it with Neovim and zsh for fuzzy matching file names. Homepage slop slop (Select Operation) presents a UI to select a region of the screen or a window and prints the region to stdout. Works well with shotgun. $ slop -c 0,0,0,0.75 -l -f \"-i %i -g %g\" -i 8389044 -g 1464x1008+291+818 Homepage Syncthing Syncthing is a decentralised file synchronisation tool. Like Dropbox but self hosted and without the need for a central third-party file store. Homepage tig tig is a ncurses TUI for git. Its great for reviewing and staging changes, viewing history and diffs. Homepage titlecase titlecase is a little tool I wrote to format text using a title case format described by John Gruber. It correctly handles punctuation, and words like iPhone. I use it to obtain consistent titles on all my blog posts. $ echo 'an illustrated guide to useful command line tools' | titlecase An Illustrated Guide to Useful Command Line Tools I typically use it from within Neovim where selected text is piped through it in-place. This is done by creating a visual selection and then typing: :!titlecase. Homepage Universal Ctags Universal Ctags is a fork of exuberant ctags that is actively maintained. ctags is used to generate a tags file that vim and other tools can use to navigate to the definition of symbols in files. $ ctags --recurse src Homepage watchexec watchexec is a file and directory watcher that can run commands in response to file-system changes. Handy for auto running tests or restarting a development web server when source files change. # run command on file change $ watchexec -w content cobalt build # kill and restart server on file change $ watchexec -w src -s SIGINT -r 'cargo run' Homepage z z tracks your most used directories and allows you to jump to them with a partial name. Homepage zola zola is a full-featured very fast static site compiler. Homepage Changelog 31 Oct 2019 Add bb, and brief descriptions to the table of contents 28 Oct 2019 Add hyperfine Comments Comments on Lobsters Comments on Hacker News Previous Post: What I Learnt Building a Lobsters TUI in Rust ",
        "_version_":1718938234006798336},
      {
        "story_id":19988548,
        "story_author":"axiomdata316",
        "story_descendants":169,
        "story_score":597,
        "story_time":"2019-05-23T04:42:59Z",
        "story_title":"The Art of Command Line (2015)",
        "search":["The Art of Command Line (2015)",
          "Normal",
          "https://github.com/jlevy/the-art-of-command-line",
          " etina Deutsch English Espaol Franais Indonesia Italiano polski Portugus Romn Slovenina The Art of Command Line Note: I'm planning to revise this and looking for a new co-author to help with expanding this into a more comprehensive guide. While it's very popular, it could be broader and a bit deeper. If you like to write and are close to being an expert on this material and willing to consider helping, please drop me a note at josh (0x40) holloway.com. jlevy, Holloway. Thank you! Meta Basics Everyday use Processing files and data System debugging One-liners Obscure but useful macOS only Windows only More resources Disclaimer Fluency on the command line is a skill often neglected or considered arcane, but it improves your flexibility and productivity as an engineer in both obvious and subtle ways. This is a selection of notes and tips on using the command-line that we've found useful when working on Linux. Some tips are elementary, and some are fairly specific, sophisticated, or obscure. This page is not long, but if you can use and recall all the items here, you know a lot. This work is the result of many authors and translators. Some of this originally appeared on Quora, but it has since moved to GitHub, where people more talented than the original author have made numerous improvements. Please submit a question if you have a question related to the command line. Please contribute if you see an error or something that could be better! Meta Scope: This guide is for both beginners and experienced users. The goals are breadth (everything important), specificity (give concrete examples of the most common case), and brevity (avoid things that aren't essential or digressions you can easily look up elsewhere). Every tip is essential in some situation or significantly saves time over alternatives. This is written for Linux, with the exception of the \"macOS only\" and \"Windows only\" sections. Many of the other items apply or can be installed on other Unices or macOS (or even Cygwin). The focus is on interactive Bash, though many tips apply to other shells and to general Bash scripting. It includes both \"standard\" Unix commands as well as ones that require special package installs -- so long as they are important enough to merit inclusion. Notes: To keep this to one page, content is implicitly included by reference. You're smart enough to look up more detail elsewhere once you know the idea or command to Google. Use apt, yum, dnf, pacman, pip or brew (as appropriate) to install new programs. Use Explainshell to get a helpful breakdown of what commands, options, pipes etc. do. Basics Learn basic Bash. Actually, type man bash and at least skim the whole thing; it's pretty easy to follow and not that long. Alternate shells can be nice, but Bash is powerful and always available (learning only zsh, fish, etc., while tempting on your own laptop, restricts you in many situations, such as using existing servers). Learn at least one text-based editor well. The nano editor is one of the simplest for basic editing (opening, editing, saving, searching). However, for the power user in a text terminal, there is no substitute for Vim (vi), the hard-to-learn but venerable, fast, and full-featured editor. Many people also use the classic Emacs, particularly for larger editing tasks. (Of course, any modern software developer working on an extensive project is unlikely to use only a pure text-based editor and should also be familiar with modern graphical IDEs and tools.) Finding documentation: Know how to read official documentation with man (for the inquisitive, man man lists the section numbers, e.g. 1 is \"regular\" commands, 5 is files/conventions, and 8 are for administration). Find man pages with apropos. Know that some commands are not executables, but Bash builtins, and that you can get help on them with help and help -d. You can find out whether a command is an executable, shell builtin or an alias by using type command. curl cheat.sh/command will give a brief \"cheat sheet\" with common examples of how to use a shell command. Learn about redirection of output and input using > and < and pipes using |. Know > overwrites the output file and >> appends. Learn about stdout and stderr. Learn about file glob expansion with * (and perhaps ? and [...]) and quoting and the difference between double \" and single ' quotes. (See more on variable expansion below.) Be familiar with Bash job management: &, ctrl-z, ctrl-c, jobs, fg, bg, kill, etc. Know ssh, and the basics of passwordless authentication, via ssh-agent, ssh-add, etc. Basic file management: ls and ls -l (in particular, learn what every column in ls -l means), less, head, tail and tail -f (or even better, less +F), ln and ln -s (learn the differences and advantages of hard versus soft links), chown, chmod, du (for a quick summary of disk usage: du -hs *). For filesystem management, df, mount, fdisk, mkfs, lsblk. Learn what an inode is (ls -i or df -i). Basic network management: ip or ifconfig, dig, traceroute, route. Learn and use a version control management system, such as git. Know regular expressions well, and the various flags to grep/egrep. The -i, -o, -v, -A, -B, and -C options are worth knowing. Learn to use apt-get, yum, dnf or pacman (depending on distro) to find and install packages. And make sure you have pip to install Python-based command-line tools (a few below are easiest to install via pip). Everyday use In Bash, use Tab to complete arguments or list all available commands and ctrl-r to search through command history (after pressing, type to search, press ctrl-r repeatedly to cycle through more matches, press Enter to execute the found command, or hit the right arrow to put the result in the current line to allow editing). In Bash, use ctrl-w to delete the last word, and ctrl-u to delete the content from current cursor back to the start of the line. Use alt-b and alt-f to move by word, ctrl-a to move cursor to beginning of line, ctrl-e to move cursor to end of line, ctrl-k to kill to the end of the line, ctrl-l to clear the screen. See man readline for all the default keybindings in Bash. There are a lot. For example alt-. cycles through previous arguments, and alt-* expands a glob. Alternatively, if you love vi-style key-bindings, use set -o vi (and set -o emacs to put it back). For editing long commands, after setting your editor (for example export EDITOR=vim), ctrl-x ctrl-e will open the current command in an editor for multi-line editing. Or in vi style, escape-v. To see recent commands, use history. Follow with !n (where n is the command number) to execute again. There are also many abbreviations you can use, the most useful probably being !$ for last argument and !! for last command (see \"HISTORY EXPANSION\" in the man page). However, these are often easily replaced with ctrl-r and alt-.. Go to your home directory with cd. Access files relative to your home directory with the ~ prefix (e.g. ~/.bashrc). In sh scripts refer to the home directory as $HOME. To go back to the previous working directory: cd -. If you are halfway through typing a command but change your mind, hit alt-# to add a # at the beginning and enter it as a comment (or use ctrl-a, #, enter). You can then return to it later via command history. Use xargs (or parallel). It's very powerful. Note you can control how many items execute per line (-L) as well as parallelism (-P). If you're not sure if it'll do the right thing, use xargs echo first. Also, -I{} is handy. Examples: find . -name '*.py' | xargs grep some_function cat hosts | xargs -I{} ssh root@{} hostname pstree -p is a helpful display of the process tree. Use pgrep and pkill to find or signal processes by name (-f is helpful). Know the various signals you can send processes. For example, to suspend a process, use kill -STOP [pid]. For the full list, see man 7 signal Use nohup or disown if you want a background process to keep running forever. Check what processes are listening via netstat -lntp or ss -plat (for TCP; add -u for UDP) or lsof -iTCP -sTCP:LISTEN -P -n (which also works on macOS). See also lsof and fuser for open sockets and files. See uptime or w to know how long the system has been running. Use alias to create shortcuts for commonly used commands. For example, alias ll='ls -latr' creates a new alias ll. Save aliases, shell settings, and functions you commonly use in ~/.bashrc, and arrange for login shells to source it. This will make your setup available in all your shell sessions. Put the settings of environment variables as well as commands that should be executed when you login in ~/.bash_profile. Separate configuration will be needed for shells you launch from graphical environment logins and cron jobs. Synchronize your configuration files (e.g. .bashrc and .bash_profile) among various computers with Git. Understand that care is needed when variables and filenames include whitespace. Surround your Bash variables with quotes, e.g. \"$FOO\". Prefer the -0 or -print0 options to enable null characters to delimit filenames, e.g. locate -0 pattern | xargs -0 ls -al or find / -print0 -type d | xargs -0 ls -al. To iterate on filenames containing whitespace in a for loop, set your IFS to be a newline only using IFS=$'\\n'. In Bash scripts, use set -x (or the variant set -v, which logs raw input, including unexpanded variables and comments) for debugging output. Use strict modes unless you have a good reason not to: Use set -e to abort on errors (nonzero exit code). Use set -u to detect unset variable usages. Consider set -o pipefail too, to abort on errors within pipes (though read up on it more if you do, as this topic is a bit subtle). For more involved scripts, also use trap on EXIT or ERR. A useful habit is to start a script like this, which will make it detect and abort on common errors and print a message: set -euo pipefail trap \"echo 'error: Script failed: see failed command above'\" ERR In Bash scripts, subshells (written with parentheses) are convenient ways to group commands. A common example is to temporarily move to a different working directory, e.g. # do something in current dir (cd /some/other/dir && other-command) # continue in original dir In Bash, note there are lots of kinds of variable expansion. Checking a variable exists: ${name:?error message}. For example, if a Bash script requires a single argument, just write input_file=${1:?usage: $0 input_file}. Using a default value if a variable is empty: ${name:-default}. If you want to have an additional (optional) parameter added to the previous example, you can use something like output_file=${2:-logfile}. If $2 is omitted and thus empty, output_file will be set to logfile. Arithmetic expansion: i=$(( (i + 1) % 5 )). Sequences: {1..10}. Trimming of strings: ${var%suffix} and ${var#prefix}. For example if var=foo.pdf, then echo ${var%.pdf}.txt prints foo.txt. Brace expansion using {...} can reduce having to re-type similar text and automate combinations of items. This is helpful in examples like mv foo.{txt,pdf} some-dir (which moves both files), cp somefile{,.bak} (which expands to cp somefile somefile.bak) or mkdir -p test-{a,b,c}/subtest-{1,2,3} (which expands all possible combinations and creates a directory tree). Brace expansion is performed before any other expansion. The order of expansions is: brace expansion; tilde expansion, parameter and variable expansion, arithmetic expansion, and command substitution (done in a left-to-right fashion); word splitting; and filename expansion. (For example, a range like {1..20} cannot be expressed with variables using {$a..$b}. Use seq or a for loop instead, e.g., seq $a $b or for((i=a; i<=b; i++)); do ... ; done.) The output of a command can be treated like a file via <(some command) (known as process substitution). For example, compare local /etc/hosts with a remote one: diff /etc/hosts <(ssh somehost cat /etc/hosts) When writing scripts you may want to put all of your code in curly braces. If the closing brace is missing, your script will be prevented from executing due to a syntax error. This makes sense when your script is going to be downloaded from the web, since it prevents partially downloaded scripts from executing: A \"here document\" allows redirection of multiple lines of input as if from a file: cat <<EOF input on multiple lines EOF In Bash, redirect both standard output and standard error via: some-command >logfile 2>&1 or some-command &>logfile. Often, to ensure a command does not leave an open file handle to standard input, tying it to the terminal you are in, it is also good practice to add </dev/null. Use man ascii for a good ASCII table, with hex and decimal values. For general encoding info, man unicode, man utf-8, and man latin1 are helpful. Use screen or tmux to multiplex the screen, especially useful on remote ssh sessions and to detach and re-attach to a session. byobu can enhance screen or tmux by providing more information and easier management. A more minimal alternative for session persistence only is dtach. In ssh, knowing how to port tunnel with -L or -D (and occasionally -R) is useful, e.g. to access web sites from a remote server. It can be useful to make a few optimizations to your ssh configuration; for example, this ~/.ssh/config contains settings to avoid dropped connections in certain network environments, uses compression (which is helpful with scp over low-bandwidth connections), and multiplex channels to the same server with a local control file: TCPKeepAlive=yes ServerAliveInterval=15 ServerAliveCountMax=6 Compression=yes ControlMaster auto ControlPath /tmp/%r@%h:%p ControlPersist yes A few other options relevant to ssh are security sensitive and should be enabled with care, e.g. per subnet or host or in trusted networks: StrictHostKeyChecking=no, ForwardAgent=yes Consider mosh an alternative to ssh that uses UDP, avoiding dropped connections and adding convenience on the road (requires server-side setup). To get the permissions on a file in octal form, which is useful for system configuration but not available in ls and easy to bungle, use something like stat -c '%A %a %n' /etc/timezone For interactive selection of values from the output of another command, use percol or fzf. For interaction with files based on the output of another command (like git), use fpp (PathPicker). For a simple web server for all files in the current directory (and subdirs), available to anyone on your network, use: python -m SimpleHTTPServer 7777 (for port 7777 and Python 2) and python -m http.server 7777 (for port 7777 and Python 3). For running a command as another user, use sudo. Defaults to running as root; use -u to specify another user. Use -i to login as that user (you will be asked for your password). For switching the shell to another user, use su username or su - username. The latter with \"-\" gets an environment as if another user just logged in. Omitting the username defaults to root. You will be asked for the password of the user you are switching to. Know about the 128K limit on command lines. This \"Argument list too long\" error is common when wildcard matching large numbers of files. (When this happens alternatives like find and xargs may help.) For a basic calculator (and of course access to Python in general), use the python interpreter. For example, Processing files and data To locate a file by name in the current directory, find . -iname '*something*' (or similar). To find a file anywhere by name, use locate something (but bear in mind updatedb may not have indexed recently created files). For general searching through source or data files, there are several options more advanced or faster than grep -r, including (in rough order from older to newer) ack, ag (\"the silver searcher\"), and rg (ripgrep). To convert HTML to text: lynx -dump -stdin For Markdown, HTML, and all kinds of document conversion, try pandoc. For example, to convert a Markdown document to Word format: pandoc README.md --from markdown --to docx -o temp.docx If you must handle XML, xmlstarlet is old but good. For JSON, use jq. For interactive use, also see jid and jiq. For YAML, use shyaml. For Excel or CSV files, csvkit provides in2csv, csvcut, csvjoin, csvgrep, etc. For Amazon S3, s3cmd is convenient and s4cmd is faster. Amazon's aws and the improved saws are essential for other AWS-related tasks. Know about sort and uniq, including uniq's -u and -d options -- see one-liners below. See also comm. Know about cut, paste, and join to manipulate text files. Many people use cut but forget about join. Know about wc to count newlines (-l), characters (-m), words (-w) and bytes (-c). Know about tee to copy from stdin to a file and also to stdout, as in ls -al | tee file.txt. For more complex calculations, including grouping, reversing fields, and statistical calculations, consider datamash. Know that locale affects a lot of command line tools in subtle ways, including sorting order (collation) and performance. Most Linux installations will set LANG or other locale variables to a local setting like US English. But be aware sorting will change if you change locale. And know i18n routines can make sort or other commands run many times slower. In some situations (such as the set operations or uniqueness operations below) you can safely ignore slow i18n routines entirely and use traditional byte-based sort order, using export LC_ALL=C. You can set a specific command's environment by prefixing its invocation with the environment variable settings, as in TZ=Pacific/Fiji date. Know basic awk and sed for simple data munging. See One-liners for examples. To replace all occurrences of a string in place, in one or more files: perl -pi.bak -e 's/old-string/new-string/g' my-files-*.txt To rename multiple files and/or search and replace within files, try repren. (In some cases the rename command also allows multiple renames, but be careful as its functionality is not the same on all Linux distributions.) # Full rename of filenames, directories, and contents foo -> bar: repren --full --preserve-case --from foo --to bar . # Recover backup files whatever.bak -> whatever: repren --renames --from '(.*)\\.bak' --to '\\1' *.bak # Same as above, using rename, if available: rename 's/\\.bak$//' *.bak As the man page says, rsync really is a fast and extraordinarily versatile file copying tool. It's known for synchronizing between machines but is equally useful locally. When security restrictions allow, using rsync instead of scp allows recovery of a transfer without restarting from scratch. It also is among the fastest ways to delete large numbers of files: mkdir empty && rsync -r --delete empty/ some-dir && rmdir some-dir For monitoring progress when processing files, use pv, pycp, pmonitor, progress, rsync --progress, or, for block-level copying, dd status=progress. Use shuf to shuffle or select random lines from a file. Know sort's options. For numbers, use -n, or -h for handling human-readable numbers (e.g. from du -h). Know how keys work (-t and -k). In particular, watch out that you need to write -k1,1 to sort by only the first field; -k1 means sort according to the whole line. Stable sort (sort -s) can be useful. For example, to sort first by field 2, then secondarily by field 1, you can use sort -k1,1 | sort -s -k2,2. If you ever need to write a tab literal in a command line in Bash (e.g. for the -t argument to sort), press ctrl-v [Tab] or write $'\\t' (the latter is better as you can copy/paste it). The standard tools for patching source code are diff and patch. See also diffstat for summary statistics of a diff and sdiff for a side-by-side diff. Note diff -r works for entire directories. Use diff -r tree1 tree2 | diffstat for a summary of changes. Use vimdiff to compare and edit files. For binary files, use hd, hexdump or xxd for simple hex dumps and bvi, hexedit or biew for binary editing. Also for binary files, strings (plus grep, etc.) lets you find bits of text. For binary diffs (delta compression), use xdelta3. To convert text encodings, try iconv. Or uconv for more advanced use; it supports some advanced Unicode things. For example: # Displays hex codes or actual names of characters (useful for debugging): uconv -f utf-8 -t utf-8 -x '::Any-Hex;' < input.txt uconv -f utf-8 -t utf-8 -x '::Any-Name;' < input.txt # Lowercase and removes all accents (by expanding and dropping them): uconv -f utf-8 -t utf-8 -x '::Any-Lower; ::Any-NFD; [:Nonspacing Mark:] >; ::Any-NFC;' < input.txt > output.txt To split files into pieces, see split (to split by size) and csplit (to split by a pattern). Date and time: To get the current date and time in the helpful ISO 8601 format, use date -u +\"%Y-%m-%dT%H:%M:%SZ\" (other options are problematic). To manipulate date and time expressions, use dateadd, datediff, strptime etc. from dateutils. Use zless, zmore, zcat, and zgrep to operate on compressed files. File attributes are settable via chattr and offer a lower-level alternative to file permissions. For example, to protect against accidental file deletion the immutable flag: sudo chattr +i /critical/directory/or/file Use getfacl and setfacl to save and restore file permissions. For example: getfacl -R /some/path > permissions.txt setfacl --restore=permissions.txt To create empty files quickly, use truncate (creates sparse file), fallocate (ext4, xfs, btrfs and ocfs2 filesystems), xfs_mkfile (almost any filesystems, comes in xfsprogs package), mkfile (for Unix-like systems like Solaris, Mac OS). System debugging For web debugging, curl and curl -I are handy, or their wget equivalents, or the more modern httpie. To know current cpu/disk status, the classic tools are top (or the better htop), iostat, and iotop. Use iostat -mxz 15 for basic CPU and detailed per-partition disk stats and performance insight. For network connection details, use netstat and ss. For a quick overview of what's happening on a system, dstat is especially useful. For broadest overview with details, use glances. To know memory status, run and understand the output of free and vmstat. In particular, be aware the \"cached\" value is memory held by the Linux kernel as file cache, so effectively counts toward the \"free\" value. Java system debugging is a different kettle of fish, but a simple trick on Oracle's and some other JVMs is that you can run kill -3 <pid> and a full stack trace and heap summary (including generational garbage collection details, which can be highly informative) will be dumped to stderr/logs. The JDK's jps, jstat, jstack, jmap are useful. SJK tools are more advanced. Use mtr as a better traceroute, to identify network issues. For looking at why a disk is full, ncdu saves time over the usual commands like du -sh *. To find which socket or process is using bandwidth, try iftop or nethogs. The ab tool (comes with Apache) is helpful for quick-and-dirty checking of web server performance. For more complex load testing, try siege. For more serious network debugging, wireshark, tshark, or ngrep. Know about strace and ltrace. These can be helpful if a program is failing, hanging, or crashing, and you don't know why, or if you want to get a general idea of performance. Note the profiling option (-c), and the ability to attach to a running process (-p). Use trace child option (-f) to avoid missing important calls. Know about ldd to check shared libraries etc but never run it on untrusted files. Know how to connect to a running process with gdb and get its stack traces. Use /proc. It's amazingly helpful sometimes when debugging live problems. Examples: /proc/cpuinfo, /proc/meminfo, /proc/cmdline, /proc/xxx/cwd, /proc/xxx/exe, /proc/xxx/fd/, /proc/xxx/smaps (where xxx is the process id or pid). When debugging why something went wrong in the past, sar can be very helpful. It shows historic statistics on CPU, memory, network, etc. For deeper systems and performance analyses, look at stap (SystemTap), perf, and sysdig. Check what OS you're on with uname or uname -a (general Unix/kernel info) or lsb_release -a (Linux distro info). Use dmesg whenever something's acting really funny (it could be hardware or driver issues). If you delete a file and it doesn't free up expected disk space as reported by du, check whether the file is in use by a process: lsof | grep deleted | grep \"filename-of-my-big-file\" One-liners A few examples of piecing together commands: It is remarkably helpful sometimes that you can do set intersection, union, and difference of text files via sort/uniq. Suppose a and b are text files that are already uniqued. This is fast, and works on files of arbitrary size, up to many gigabytes. (Sort is not limited by memory, though you may need to use the -T option if /tmp is on a small root partition.) See also the note about LC_ALL above and sort's -u option (left out for clarity below). sort a b | uniq > c # c is a union b sort a b | uniq -d > c # c is a intersect b sort a b b | uniq -u > c # c is set difference a - b Pretty-print two JSON files, normalizing their syntax, then coloring and paginating the result: diff <(jq --sort-keys . < file1.json) <(jq --sort-keys . < file2.json) | colordiff | less -R Use grep . * to quickly examine the contents of all files in a directory (so each line is paired with the filename), or head -100 * (so each file has a heading). This can be useful for directories filled with config settings like those in /sys, /proc, /etc. Summing all numbers in the third column of a text file (this is probably 3X faster and 3X less code than equivalent Python): awk '{ x += $3 } END { print x }' myfile To see sizes/dates on a tree of files, this is like a recursive ls -l but is easier to read than ls -lR: Say you have a text file, like a web server log, and a certain value that appears on some lines, such as an acct_id parameter that is present in the URL. If you want a tally of how many requests for each acct_id: egrep -o 'acct_id=[0-9]+' access.log | cut -d= -f2 | sort | uniq -c | sort -rn To continuously monitor changes, use watch, e.g. check changes to files in a directory with watch -d -n 2 'ls -rtlh | tail' or to network settings while troubleshooting your wifi settings with watch -d -n 2 ifconfig. Run this function to get a random tip from this document (parses Markdown and extracts an item): function taocl() { curl -s https://raw.githubusercontent.com/jlevy/the-art-of-command-line/master/README.md | sed '/cowsay[.]png/d' | pandoc -f markdown -t html | xmlstarlet fo --html --dropdtd | xmlstarlet sel -t -v \"(html/body/ul/li[count(p)>0])[$RANDOM mod last()+1]\" | xmlstarlet unesc | fmt -80 | iconv -t US } Obscure but useful expr: perform arithmetic or boolean operations or evaluate regular expressions m4: simple macro processor yes: print a string a lot cal: nice calendar env: run a command (useful in scripts) printenv: print out environment variables (useful in debugging and scripts) look: find English words (or lines in a file) beginning with a string cut, paste and join: data manipulation fmt: format text paragraphs pr: format text into pages/columns fold: wrap lines of text column: format text fields into aligned, fixed-width columns or tables expand and unexpand: convert between tabs and spaces nl: add line numbers seq: print numbers bc: calculator factor: factor integers gpg: encrypt and sign files toe: table of terminfo entries nc: network debugging and data transfer socat: socket relay and tcp port forwarder (similar to netcat) slurm: network traffic visualization dd: moving data between files or devices file: identify type of a file tree: display directories and subdirectories as a nesting tree; like ls but recursive stat: file info time: execute and time a command timeout: execute a command for specified amount of time and stop the process when the specified amount of time completes. lockfile: create semaphore file that can only be removed by rm -f logrotate: rotate, compress and mail logs. watch: run a command repeatedly, showing results and/or highlighting changes when-changed: runs any command you specify whenever it sees file changed. See inotifywait and entr as well. tac: print files in reverse comm: compare sorted files line by line strings: extract text from binary files tr: character translation or manipulation iconv or uconv: conversion for text encodings split and csplit: splitting files sponge: read all input before writing it, useful for reading from then writing to the same file, e.g., grep -v something some-file | sponge some-file units: unit conversions and calculations; converts furlongs per fortnight to twips per blink (see also /usr/share/units/definitions.units) apg: generates random passwords xz: high-ratio file compression ldd: dynamic library info nm: symbols from object files ab or wrk: benchmarking web servers strace: system call debugging mtr: better traceroute for network debugging cssh: visual concurrent shell rsync: sync files and folders over SSH or in local file system wireshark and tshark: packet capture and network debugging ngrep: grep for the network layer host and dig: DNS lookups lsof: process file descriptor and socket info dstat: useful system stats glances: high level, multi-subsystem overview iostat: Disk usage stats mpstat: CPU usage stats vmstat: Memory usage stats htop: improved version of top last: login history w: who's logged on id: user/group identity info sar: historic system stats iftop or nethogs: network utilization by socket or process ss: socket statistics dmesg: boot and system error messages sysctl: view and configure Linux kernel parameters at run time hdparm: SATA/ATA disk manipulation/performance lsblk: list block devices: a tree view of your disks and disk partitions lshw, lscpu, lspci, lsusb, dmidecode: hardware information, including CPU, BIOS, RAID, graphics, devices, etc. lsmod and modinfo: List and show details of kernel modules. fortune, ddate, and sl: um, well, it depends on whether you consider steam locomotives and Zippy quotations \"useful\" macOS only These are items relevant only on macOS. Package management with brew (Homebrew) and/or port (MacPorts). These can be used to install on macOS many of the above commands. Copy output of any command to a desktop app with pbcopy and paste input from one with pbpaste. To enable the Option key in macOS Terminal as an alt key (such as used in the commands above like alt-b, alt-f, etc.), open Preferences -> Profiles -> Keyboard and select \"Use Option as Meta key\". To open a file with a desktop app, use open or open -a /Applications/Whatever.app. Spotlight: Search files with mdfind and list metadata (such as photo EXIF info) with mdls. Be aware macOS is based on BSD Unix, and many commands (for example ps, ls, tail, awk, sed) have many subtle variations from Linux, which is largely influenced by System V-style Unix and GNU tools. You can often tell the difference by noting a man page has the heading \"BSD General Commands Manual.\" In some cases GNU versions can be installed, too (such as gawk and gsed for GNU awk and sed). If writing cross-platform Bash scripts, avoid such commands (for example, consider Python or perl) or test carefully. To get macOS release information, use sw_vers. Windows only These items are relevant only on Windows. Ways to obtain Unix tools under Windows Access the power of the Unix shell under Microsoft Windows by installing Cygwin. Most of the things described in this document will work out of the box. On Windows 10, you can use Windows Subsystem for Linux (WSL), which provides a familiar Bash environment with Unix command line utilities. If you mainly want to use GNU developer tools (such as GCC) on Windows, consider MinGW and its MSYS package, which provides utilities such as bash, gawk, make and grep. MSYS doesn't have all the features compared to Cygwin. MinGW is particularly useful for creating native Windows ports of Unix tools. Another option to get Unix look and feel under Windows is Cash. Note that only very few Unix commands and command-line options are available in this environment. Useful Windows command-line tools You can perform and script most Windows system administration tasks from the command line by learning and using wmic. Native command-line Windows networking tools you may find useful include ping, ipconfig, tracert, and netstat. You can perform many useful Windows tasks by invoking the Rundll32 command. Cygwin tips and tricks Install additional Unix programs with the Cygwin's package manager. Use mintty as your command-line window. Access the Windows clipboard through /dev/clipboard. Run cygstart to open an arbitrary file through its registered application. Access the Windows registry with regtool. Note that a C:\\ Windows drive path becomes /cygdrive/c under Cygwin, and that Cygwin's / appears under C:\\cygwin on Windows. Convert between Cygwin and Windows-style file paths with cygpath. This is most useful in scripts that invoke Windows programs. More resources awesome-shell: A curated list of shell tools and resources. awesome-osx-command-line: A more in-depth guide for the macOS command line. Strict mode for writing better shell scripts. shellcheck: A shell script static analysis tool. Essentially, lint for bash/sh/zsh. Filenames and Pathnames in Shell: The sadly complex minutiae on how to handle filenames correctly in shell scripts. Data Science at the Command Line: More commands and tools helpful for doing data science, from the book of the same name Disclaimer With the exception of very small tasks, code is written so others can read it. With power comes responsibility. The fact you can do something in Bash doesn't necessarily mean you should! ;) License This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. ",
          "> Learn basic Bash. Actually, type man bash and at least skim the whole thing; it's pretty easy to follow and not that long<p>Skimming <i>The Grapes of Wrath</i> would be shorter. Thank you, but no thank you.",
          "<i>Many people also use the classic Emacs, particularly for larger editing tasks. (Of course, any modern software developer working on an extensive project is unlikely to use only a pure text-based editor and should also be familiar with modern graphical IDEs and tools.)</i><p>Huh?! Unlikely?!"],
        "story_type":"Normal",
        "url_raw":"https://github.com/jlevy/the-art-of-command-line",
        "comments.comment_id":[19989314,
          19989338],
        "comments.comment_author":["smith-kyle",
          "molteanu"],
        "comments.comment_descendants":[8,
          4],
        "comments.comment_time":["2019-05-23T07:16:13Z",
          "2019-05-23T07:21:38Z"],
        "comments.comment_text":["> Learn basic Bash. Actually, type man bash and at least skim the whole thing; it's pretty easy to follow and not that long<p>Skimming <i>The Grapes of Wrath</i> would be shorter. Thank you, but no thank you.",
          "<i>Many people also use the classic Emacs, particularly for larger editing tasks. (Of course, any modern software developer working on an extensive project is unlikely to use only a pure text-based editor and should also be familiar with modern graphical IDEs and tools.)</i><p>Huh?! Unlikely?!"],
        "id":"cbc1bb3f-ce6d-4be6-8e7d-836188798fc5",
        "url_text":" etina Deutsch English Espaol Franais Indonesia Italiano polski Portugus Romn Slovenina The Art of Command Line Note: I'm planning to revise this and looking for a new co-author to help with expanding this into a more comprehensive guide. While it's very popular, it could be broader and a bit deeper. If you like to write and are close to being an expert on this material and willing to consider helping, please drop me a note at josh (0x40) holloway.com. jlevy, Holloway. Thank you! Meta Basics Everyday use Processing files and data System debugging One-liners Obscure but useful macOS only Windows only More resources Disclaimer Fluency on the command line is a skill often neglected or considered arcane, but it improves your flexibility and productivity as an engineer in both obvious and subtle ways. This is a selection of notes and tips on using the command-line that we've found useful when working on Linux. Some tips are elementary, and some are fairly specific, sophisticated, or obscure. This page is not long, but if you can use and recall all the items here, you know a lot. This work is the result of many authors and translators. Some of this originally appeared on Quora, but it has since moved to GitHub, where people more talented than the original author have made numerous improvements. Please submit a question if you have a question related to the command line. Please contribute if you see an error or something that could be better! Meta Scope: This guide is for both beginners and experienced users. The goals are breadth (everything important), specificity (give concrete examples of the most common case), and brevity (avoid things that aren't essential or digressions you can easily look up elsewhere). Every tip is essential in some situation or significantly saves time over alternatives. This is written for Linux, with the exception of the \"macOS only\" and \"Windows only\" sections. Many of the other items apply or can be installed on other Unices or macOS (or even Cygwin). The focus is on interactive Bash, though many tips apply to other shells and to general Bash scripting. It includes both \"standard\" Unix commands as well as ones that require special package installs -- so long as they are important enough to merit inclusion. Notes: To keep this to one page, content is implicitly included by reference. You're smart enough to look up more detail elsewhere once you know the idea or command to Google. Use apt, yum, dnf, pacman, pip or brew (as appropriate) to install new programs. Use Explainshell to get a helpful breakdown of what commands, options, pipes etc. do. Basics Learn basic Bash. Actually, type man bash and at least skim the whole thing; it's pretty easy to follow and not that long. Alternate shells can be nice, but Bash is powerful and always available (learning only zsh, fish, etc., while tempting on your own laptop, restricts you in many situations, such as using existing servers). Learn at least one text-based editor well. The nano editor is one of the simplest for basic editing (opening, editing, saving, searching). However, for the power user in a text terminal, there is no substitute for Vim (vi), the hard-to-learn but venerable, fast, and full-featured editor. Many people also use the classic Emacs, particularly for larger editing tasks. (Of course, any modern software developer working on an extensive project is unlikely to use only a pure text-based editor and should also be familiar with modern graphical IDEs and tools.) Finding documentation: Know how to read official documentation with man (for the inquisitive, man man lists the section numbers, e.g. 1 is \"regular\" commands, 5 is files/conventions, and 8 are for administration). Find man pages with apropos. Know that some commands are not executables, but Bash builtins, and that you can get help on them with help and help -d. You can find out whether a command is an executable, shell builtin or an alias by using type command. curl cheat.sh/command will give a brief \"cheat sheet\" with common examples of how to use a shell command. Learn about redirection of output and input using > and < and pipes using |. Know > overwrites the output file and >> appends. Learn about stdout and stderr. Learn about file glob expansion with * (and perhaps ? and [...]) and quoting and the difference between double \" and single ' quotes. (See more on variable expansion below.) Be familiar with Bash job management: &, ctrl-z, ctrl-c, jobs, fg, bg, kill, etc. Know ssh, and the basics of passwordless authentication, via ssh-agent, ssh-add, etc. Basic file management: ls and ls -l (in particular, learn what every column in ls -l means), less, head, tail and tail -f (or even better, less +F), ln and ln -s (learn the differences and advantages of hard versus soft links), chown, chmod, du (for a quick summary of disk usage: du -hs *). For filesystem management, df, mount, fdisk, mkfs, lsblk. Learn what an inode is (ls -i or df -i). Basic network management: ip or ifconfig, dig, traceroute, route. Learn and use a version control management system, such as git. Know regular expressions well, and the various flags to grep/egrep. The -i, -o, -v, -A, -B, and -C options are worth knowing. Learn to use apt-get, yum, dnf or pacman (depending on distro) to find and install packages. And make sure you have pip to install Python-based command-line tools (a few below are easiest to install via pip). Everyday use In Bash, use Tab to complete arguments or list all available commands and ctrl-r to search through command history (after pressing, type to search, press ctrl-r repeatedly to cycle through more matches, press Enter to execute the found command, or hit the right arrow to put the result in the current line to allow editing). In Bash, use ctrl-w to delete the last word, and ctrl-u to delete the content from current cursor back to the start of the line. Use alt-b and alt-f to move by word, ctrl-a to move cursor to beginning of line, ctrl-e to move cursor to end of line, ctrl-k to kill to the end of the line, ctrl-l to clear the screen. See man readline for all the default keybindings in Bash. There are a lot. For example alt-. cycles through previous arguments, and alt-* expands a glob. Alternatively, if you love vi-style key-bindings, use set -o vi (and set -o emacs to put it back). For editing long commands, after setting your editor (for example export EDITOR=vim), ctrl-x ctrl-e will open the current command in an editor for multi-line editing. Or in vi style, escape-v. To see recent commands, use history. Follow with !n (where n is the command number) to execute again. There are also many abbreviations you can use, the most useful probably being !$ for last argument and !! for last command (see \"HISTORY EXPANSION\" in the man page). However, these are often easily replaced with ctrl-r and alt-.. Go to your home directory with cd. Access files relative to your home directory with the ~ prefix (e.g. ~/.bashrc). In sh scripts refer to the home directory as $HOME. To go back to the previous working directory: cd -. If you are halfway through typing a command but change your mind, hit alt-# to add a # at the beginning and enter it as a comment (or use ctrl-a, #, enter). You can then return to it later via command history. Use xargs (or parallel). It's very powerful. Note you can control how many items execute per line (-L) as well as parallelism (-P). If you're not sure if it'll do the right thing, use xargs echo first. Also, -I{} is handy. Examples: find . -name '*.py' | xargs grep some_function cat hosts | xargs -I{} ssh root@{} hostname pstree -p is a helpful display of the process tree. Use pgrep and pkill to find or signal processes by name (-f is helpful). Know the various signals you can send processes. For example, to suspend a process, use kill -STOP [pid]. For the full list, see man 7 signal Use nohup or disown if you want a background process to keep running forever. Check what processes are listening via netstat -lntp or ss -plat (for TCP; add -u for UDP) or lsof -iTCP -sTCP:LISTEN -P -n (which also works on macOS). See also lsof and fuser for open sockets and files. See uptime or w to know how long the system has been running. Use alias to create shortcuts for commonly used commands. For example, alias ll='ls -latr' creates a new alias ll. Save aliases, shell settings, and functions you commonly use in ~/.bashrc, and arrange for login shells to source it. This will make your setup available in all your shell sessions. Put the settings of environment variables as well as commands that should be executed when you login in ~/.bash_profile. Separate configuration will be needed for shells you launch from graphical environment logins and cron jobs. Synchronize your configuration files (e.g. .bashrc and .bash_profile) among various computers with Git. Understand that care is needed when variables and filenames include whitespace. Surround your Bash variables with quotes, e.g. \"$FOO\". Prefer the -0 or -print0 options to enable null characters to delimit filenames, e.g. locate -0 pattern | xargs -0 ls -al or find / -print0 -type d | xargs -0 ls -al. To iterate on filenames containing whitespace in a for loop, set your IFS to be a newline only using IFS=$'\\n'. In Bash scripts, use set -x (or the variant set -v, which logs raw input, including unexpanded variables and comments) for debugging output. Use strict modes unless you have a good reason not to: Use set -e to abort on errors (nonzero exit code). Use set -u to detect unset variable usages. Consider set -o pipefail too, to abort on errors within pipes (though read up on it more if you do, as this topic is a bit subtle). For more involved scripts, also use trap on EXIT or ERR. A useful habit is to start a script like this, which will make it detect and abort on common errors and print a message: set -euo pipefail trap \"echo 'error: Script failed: see failed command above'\" ERR In Bash scripts, subshells (written with parentheses) are convenient ways to group commands. A common example is to temporarily move to a different working directory, e.g. # do something in current dir (cd /some/other/dir && other-command) # continue in original dir In Bash, note there are lots of kinds of variable expansion. Checking a variable exists: ${name:?error message}. For example, if a Bash script requires a single argument, just write input_file=${1:?usage: $0 input_file}. Using a default value if a variable is empty: ${name:-default}. If you want to have an additional (optional) parameter added to the previous example, you can use something like output_file=${2:-logfile}. If $2 is omitted and thus empty, output_file will be set to logfile. Arithmetic expansion: i=$(( (i + 1) % 5 )). Sequences: {1..10}. Trimming of strings: ${var%suffix} and ${var#prefix}. For example if var=foo.pdf, then echo ${var%.pdf}.txt prints foo.txt. Brace expansion using {...} can reduce having to re-type similar text and automate combinations of items. This is helpful in examples like mv foo.{txt,pdf} some-dir (which moves both files), cp somefile{,.bak} (which expands to cp somefile somefile.bak) or mkdir -p test-{a,b,c}/subtest-{1,2,3} (which expands all possible combinations and creates a directory tree). Brace expansion is performed before any other expansion. The order of expansions is: brace expansion; tilde expansion, parameter and variable expansion, arithmetic expansion, and command substitution (done in a left-to-right fashion); word splitting; and filename expansion. (For example, a range like {1..20} cannot be expressed with variables using {$a..$b}. Use seq or a for loop instead, e.g., seq $a $b or for((i=a; i<=b; i++)); do ... ; done.) The output of a command can be treated like a file via <(some command) (known as process substitution). For example, compare local /etc/hosts with a remote one: diff /etc/hosts <(ssh somehost cat /etc/hosts) When writing scripts you may want to put all of your code in curly braces. If the closing brace is missing, your script will be prevented from executing due to a syntax error. This makes sense when your script is going to be downloaded from the web, since it prevents partially downloaded scripts from executing: A \"here document\" allows redirection of multiple lines of input as if from a file: cat <<EOF input on multiple lines EOF In Bash, redirect both standard output and standard error via: some-command >logfile 2>&1 or some-command &>logfile. Often, to ensure a command does not leave an open file handle to standard input, tying it to the terminal you are in, it is also good practice to add </dev/null. Use man ascii for a good ASCII table, with hex and decimal values. For general encoding info, man unicode, man utf-8, and man latin1 are helpful. Use screen or tmux to multiplex the screen, especially useful on remote ssh sessions and to detach and re-attach to a session. byobu can enhance screen or tmux by providing more information and easier management. A more minimal alternative for session persistence only is dtach. In ssh, knowing how to port tunnel with -L or -D (and occasionally -R) is useful, e.g. to access web sites from a remote server. It can be useful to make a few optimizations to your ssh configuration; for example, this ~/.ssh/config contains settings to avoid dropped connections in certain network environments, uses compression (which is helpful with scp over low-bandwidth connections), and multiplex channels to the same server with a local control file: TCPKeepAlive=yes ServerAliveInterval=15 ServerAliveCountMax=6 Compression=yes ControlMaster auto ControlPath /tmp/%r@%h:%p ControlPersist yes A few other options relevant to ssh are security sensitive and should be enabled with care, e.g. per subnet or host or in trusted networks: StrictHostKeyChecking=no, ForwardAgent=yes Consider mosh an alternative to ssh that uses UDP, avoiding dropped connections and adding convenience on the road (requires server-side setup). To get the permissions on a file in octal form, which is useful for system configuration but not available in ls and easy to bungle, use something like stat -c '%A %a %n' /etc/timezone For interactive selection of values from the output of another command, use percol or fzf. For interaction with files based on the output of another command (like git), use fpp (PathPicker). For a simple web server for all files in the current directory (and subdirs), available to anyone on your network, use: python -m SimpleHTTPServer 7777 (for port 7777 and Python 2) and python -m http.server 7777 (for port 7777 and Python 3). For running a command as another user, use sudo. Defaults to running as root; use -u to specify another user. Use -i to login as that user (you will be asked for your password). For switching the shell to another user, use su username or su - username. The latter with \"-\" gets an environment as if another user just logged in. Omitting the username defaults to root. You will be asked for the password of the user you are switching to. Know about the 128K limit on command lines. This \"Argument list too long\" error is common when wildcard matching large numbers of files. (When this happens alternatives like find and xargs may help.) For a basic calculator (and of course access to Python in general), use the python interpreter. For example, Processing files and data To locate a file by name in the current directory, find . -iname '*something*' (or similar). To find a file anywhere by name, use locate something (but bear in mind updatedb may not have indexed recently created files). For general searching through source or data files, there are several options more advanced or faster than grep -r, including (in rough order from older to newer) ack, ag (\"the silver searcher\"), and rg (ripgrep). To convert HTML to text: lynx -dump -stdin For Markdown, HTML, and all kinds of document conversion, try pandoc. For example, to convert a Markdown document to Word format: pandoc README.md --from markdown --to docx -o temp.docx If you must handle XML, xmlstarlet is old but good. For JSON, use jq. For interactive use, also see jid and jiq. For YAML, use shyaml. For Excel or CSV files, csvkit provides in2csv, csvcut, csvjoin, csvgrep, etc. For Amazon S3, s3cmd is convenient and s4cmd is faster. Amazon's aws and the improved saws are essential for other AWS-related tasks. Know about sort and uniq, including uniq's -u and -d options -- see one-liners below. See also comm. Know about cut, paste, and join to manipulate text files. Many people use cut but forget about join. Know about wc to count newlines (-l), characters (-m), words (-w) and bytes (-c). Know about tee to copy from stdin to a file and also to stdout, as in ls -al | tee file.txt. For more complex calculations, including grouping, reversing fields, and statistical calculations, consider datamash. Know that locale affects a lot of command line tools in subtle ways, including sorting order (collation) and performance. Most Linux installations will set LANG or other locale variables to a local setting like US English. But be aware sorting will change if you change locale. And know i18n routines can make sort or other commands run many times slower. In some situations (such as the set operations or uniqueness operations below) you can safely ignore slow i18n routines entirely and use traditional byte-based sort order, using export LC_ALL=C. You can set a specific command's environment by prefixing its invocation with the environment variable settings, as in TZ=Pacific/Fiji date. Know basic awk and sed for simple data munging. See One-liners for examples. To replace all occurrences of a string in place, in one or more files: perl -pi.bak -e 's/old-string/new-string/g' my-files-*.txt To rename multiple files and/or search and replace within files, try repren. (In some cases the rename command also allows multiple renames, but be careful as its functionality is not the same on all Linux distributions.) # Full rename of filenames, directories, and contents foo -> bar: repren --full --preserve-case --from foo --to bar . # Recover backup files whatever.bak -> whatever: repren --renames --from '(.*)\\.bak' --to '\\1' *.bak # Same as above, using rename, if available: rename 's/\\.bak$//' *.bak As the man page says, rsync really is a fast and extraordinarily versatile file copying tool. It's known for synchronizing between machines but is equally useful locally. When security restrictions allow, using rsync instead of scp allows recovery of a transfer without restarting from scratch. It also is among the fastest ways to delete large numbers of files: mkdir empty && rsync -r --delete empty/ some-dir && rmdir some-dir For monitoring progress when processing files, use pv, pycp, pmonitor, progress, rsync --progress, or, for block-level copying, dd status=progress. Use shuf to shuffle or select random lines from a file. Know sort's options. For numbers, use -n, or -h for handling human-readable numbers (e.g. from du -h). Know how keys work (-t and -k). In particular, watch out that you need to write -k1,1 to sort by only the first field; -k1 means sort according to the whole line. Stable sort (sort -s) can be useful. For example, to sort first by field 2, then secondarily by field 1, you can use sort -k1,1 | sort -s -k2,2. If you ever need to write a tab literal in a command line in Bash (e.g. for the -t argument to sort), press ctrl-v [Tab] or write $'\\t' (the latter is better as you can copy/paste it). The standard tools for patching source code are diff and patch. See also diffstat for summary statistics of a diff and sdiff for a side-by-side diff. Note diff -r works for entire directories. Use diff -r tree1 tree2 | diffstat for a summary of changes. Use vimdiff to compare and edit files. For binary files, use hd, hexdump or xxd for simple hex dumps and bvi, hexedit or biew for binary editing. Also for binary files, strings (plus grep, etc.) lets you find bits of text. For binary diffs (delta compression), use xdelta3. To convert text encodings, try iconv. Or uconv for more advanced use; it supports some advanced Unicode things. For example: # Displays hex codes or actual names of characters (useful for debugging): uconv -f utf-8 -t utf-8 -x '::Any-Hex;' < input.txt uconv -f utf-8 -t utf-8 -x '::Any-Name;' < input.txt # Lowercase and removes all accents (by expanding and dropping them): uconv -f utf-8 -t utf-8 -x '::Any-Lower; ::Any-NFD; [:Nonspacing Mark:] >; ::Any-NFC;' < input.txt > output.txt To split files into pieces, see split (to split by size) and csplit (to split by a pattern). Date and time: To get the current date and time in the helpful ISO 8601 format, use date -u +\"%Y-%m-%dT%H:%M:%SZ\" (other options are problematic). To manipulate date and time expressions, use dateadd, datediff, strptime etc. from dateutils. Use zless, zmore, zcat, and zgrep to operate on compressed files. File attributes are settable via chattr and offer a lower-level alternative to file permissions. For example, to protect against accidental file deletion the immutable flag: sudo chattr +i /critical/directory/or/file Use getfacl and setfacl to save and restore file permissions. For example: getfacl -R /some/path > permissions.txt setfacl --restore=permissions.txt To create empty files quickly, use truncate (creates sparse file), fallocate (ext4, xfs, btrfs and ocfs2 filesystems), xfs_mkfile (almost any filesystems, comes in xfsprogs package), mkfile (for Unix-like systems like Solaris, Mac OS). System debugging For web debugging, curl and curl -I are handy, or their wget equivalents, or the more modern httpie. To know current cpu/disk status, the classic tools are top (or the better htop), iostat, and iotop. Use iostat -mxz 15 for basic CPU and detailed per-partition disk stats and performance insight. For network connection details, use netstat and ss. For a quick overview of what's happening on a system, dstat is especially useful. For broadest overview with details, use glances. To know memory status, run and understand the output of free and vmstat. In particular, be aware the \"cached\" value is memory held by the Linux kernel as file cache, so effectively counts toward the \"free\" value. Java system debugging is a different kettle of fish, but a simple trick on Oracle's and some other JVMs is that you can run kill -3 <pid> and a full stack trace and heap summary (including generational garbage collection details, which can be highly informative) will be dumped to stderr/logs. The JDK's jps, jstat, jstack, jmap are useful. SJK tools are more advanced. Use mtr as a better traceroute, to identify network issues. For looking at why a disk is full, ncdu saves time over the usual commands like du -sh *. To find which socket or process is using bandwidth, try iftop or nethogs. The ab tool (comes with Apache) is helpful for quick-and-dirty checking of web server performance. For more complex load testing, try siege. For more serious network debugging, wireshark, tshark, or ngrep. Know about strace and ltrace. These can be helpful if a program is failing, hanging, or crashing, and you don't know why, or if you want to get a general idea of performance. Note the profiling option (-c), and the ability to attach to a running process (-p). Use trace child option (-f) to avoid missing important calls. Know about ldd to check shared libraries etc but never run it on untrusted files. Know how to connect to a running process with gdb and get its stack traces. Use /proc. It's amazingly helpful sometimes when debugging live problems. Examples: /proc/cpuinfo, /proc/meminfo, /proc/cmdline, /proc/xxx/cwd, /proc/xxx/exe, /proc/xxx/fd/, /proc/xxx/smaps (where xxx is the process id or pid). When debugging why something went wrong in the past, sar can be very helpful. It shows historic statistics on CPU, memory, network, etc. For deeper systems and performance analyses, look at stap (SystemTap), perf, and sysdig. Check what OS you're on with uname or uname -a (general Unix/kernel info) or lsb_release -a (Linux distro info). Use dmesg whenever something's acting really funny (it could be hardware or driver issues). If you delete a file and it doesn't free up expected disk space as reported by du, check whether the file is in use by a process: lsof | grep deleted | grep \"filename-of-my-big-file\" One-liners A few examples of piecing together commands: It is remarkably helpful sometimes that you can do set intersection, union, and difference of text files via sort/uniq. Suppose a and b are text files that are already uniqued. This is fast, and works on files of arbitrary size, up to many gigabytes. (Sort is not limited by memory, though you may need to use the -T option if /tmp is on a small root partition.) See also the note about LC_ALL above and sort's -u option (left out for clarity below). sort a b | uniq > c # c is a union b sort a b | uniq -d > c # c is a intersect b sort a b b | uniq -u > c # c is set difference a - b Pretty-print two JSON files, normalizing their syntax, then coloring and paginating the result: diff <(jq --sort-keys . < file1.json) <(jq --sort-keys . < file2.json) | colordiff | less -R Use grep . * to quickly examine the contents of all files in a directory (so each line is paired with the filename), or head -100 * (so each file has a heading). This can be useful for directories filled with config settings like those in /sys, /proc, /etc. Summing all numbers in the third column of a text file (this is probably 3X faster and 3X less code than equivalent Python): awk '{ x += $3 } END { print x }' myfile To see sizes/dates on a tree of files, this is like a recursive ls -l but is easier to read than ls -lR: Say you have a text file, like a web server log, and a certain value that appears on some lines, such as an acct_id parameter that is present in the URL. If you want a tally of how many requests for each acct_id: egrep -o 'acct_id=[0-9]+' access.log | cut -d= -f2 | sort | uniq -c | sort -rn To continuously monitor changes, use watch, e.g. check changes to files in a directory with watch -d -n 2 'ls -rtlh | tail' or to network settings while troubleshooting your wifi settings with watch -d -n 2 ifconfig. Run this function to get a random tip from this document (parses Markdown and extracts an item): function taocl() { curl -s https://raw.githubusercontent.com/jlevy/the-art-of-command-line/master/README.md | sed '/cowsay[.]png/d' | pandoc -f markdown -t html | xmlstarlet fo --html --dropdtd | xmlstarlet sel -t -v \"(html/body/ul/li[count(p)>0])[$RANDOM mod last()+1]\" | xmlstarlet unesc | fmt -80 | iconv -t US } Obscure but useful expr: perform arithmetic or boolean operations or evaluate regular expressions m4: simple macro processor yes: print a string a lot cal: nice calendar env: run a command (useful in scripts) printenv: print out environment variables (useful in debugging and scripts) look: find English words (or lines in a file) beginning with a string cut, paste and join: data manipulation fmt: format text paragraphs pr: format text into pages/columns fold: wrap lines of text column: format text fields into aligned, fixed-width columns or tables expand and unexpand: convert between tabs and spaces nl: add line numbers seq: print numbers bc: calculator factor: factor integers gpg: encrypt and sign files toe: table of terminfo entries nc: network debugging and data transfer socat: socket relay and tcp port forwarder (similar to netcat) slurm: network traffic visualization dd: moving data between files or devices file: identify type of a file tree: display directories and subdirectories as a nesting tree; like ls but recursive stat: file info time: execute and time a command timeout: execute a command for specified amount of time and stop the process when the specified amount of time completes. lockfile: create semaphore file that can only be removed by rm -f logrotate: rotate, compress and mail logs. watch: run a command repeatedly, showing results and/or highlighting changes when-changed: runs any command you specify whenever it sees file changed. See inotifywait and entr as well. tac: print files in reverse comm: compare sorted files line by line strings: extract text from binary files tr: character translation or manipulation iconv or uconv: conversion for text encodings split and csplit: splitting files sponge: read all input before writing it, useful for reading from then writing to the same file, e.g., grep -v something some-file | sponge some-file units: unit conversions and calculations; converts furlongs per fortnight to twips per blink (see also /usr/share/units/definitions.units) apg: generates random passwords xz: high-ratio file compression ldd: dynamic library info nm: symbols from object files ab or wrk: benchmarking web servers strace: system call debugging mtr: better traceroute for network debugging cssh: visual concurrent shell rsync: sync files and folders over SSH or in local file system wireshark and tshark: packet capture and network debugging ngrep: grep for the network layer host and dig: DNS lookups lsof: process file descriptor and socket info dstat: useful system stats glances: high level, multi-subsystem overview iostat: Disk usage stats mpstat: CPU usage stats vmstat: Memory usage stats htop: improved version of top last: login history w: who's logged on id: user/group identity info sar: historic system stats iftop or nethogs: network utilization by socket or process ss: socket statistics dmesg: boot and system error messages sysctl: view and configure Linux kernel parameters at run time hdparm: SATA/ATA disk manipulation/performance lsblk: list block devices: a tree view of your disks and disk partitions lshw, lscpu, lspci, lsusb, dmidecode: hardware information, including CPU, BIOS, RAID, graphics, devices, etc. lsmod and modinfo: List and show details of kernel modules. fortune, ddate, and sl: um, well, it depends on whether you consider steam locomotives and Zippy quotations \"useful\" macOS only These are items relevant only on macOS. Package management with brew (Homebrew) and/or port (MacPorts). These can be used to install on macOS many of the above commands. Copy output of any command to a desktop app with pbcopy and paste input from one with pbpaste. To enable the Option key in macOS Terminal as an alt key (such as used in the commands above like alt-b, alt-f, etc.), open Preferences -> Profiles -> Keyboard and select \"Use Option as Meta key\". To open a file with a desktop app, use open or open -a /Applications/Whatever.app. Spotlight: Search files with mdfind and list metadata (such as photo EXIF info) with mdls. Be aware macOS is based on BSD Unix, and many commands (for example ps, ls, tail, awk, sed) have many subtle variations from Linux, which is largely influenced by System V-style Unix and GNU tools. You can often tell the difference by noting a man page has the heading \"BSD General Commands Manual.\" In some cases GNU versions can be installed, too (such as gawk and gsed for GNU awk and sed). If writing cross-platform Bash scripts, avoid such commands (for example, consider Python or perl) or test carefully. To get macOS release information, use sw_vers. Windows only These items are relevant only on Windows. Ways to obtain Unix tools under Windows Access the power of the Unix shell under Microsoft Windows by installing Cygwin. Most of the things described in this document will work out of the box. On Windows 10, you can use Windows Subsystem for Linux (WSL), which provides a familiar Bash environment with Unix command line utilities. If you mainly want to use GNU developer tools (such as GCC) on Windows, consider MinGW and its MSYS package, which provides utilities such as bash, gawk, make and grep. MSYS doesn't have all the features compared to Cygwin. MinGW is particularly useful for creating native Windows ports of Unix tools. Another option to get Unix look and feel under Windows is Cash. Note that only very few Unix commands and command-line options are available in this environment. Useful Windows command-line tools You can perform and script most Windows system administration tasks from the command line by learning and using wmic. Native command-line Windows networking tools you may find useful include ping, ipconfig, tracert, and netstat. You can perform many useful Windows tasks by invoking the Rundll32 command. Cygwin tips and tricks Install additional Unix programs with the Cygwin's package manager. Use mintty as your command-line window. Access the Windows clipboard through /dev/clipboard. Run cygstart to open an arbitrary file through its registered application. Access the Windows registry with regtool. Note that a C:\\ Windows drive path becomes /cygdrive/c under Cygwin, and that Cygwin's / appears under C:\\cygwin on Windows. Convert between Cygwin and Windows-style file paths with cygpath. This is most useful in scripts that invoke Windows programs. More resources awesome-shell: A curated list of shell tools and resources. awesome-osx-command-line: A more in-depth guide for the macOS command line. Strict mode for writing better shell scripts. shellcheck: A shell script static analysis tool. Essentially, lint for bash/sh/zsh. Filenames and Pathnames in Shell: The sadly complex minutiae on how to handle filenames correctly in shell scripts. Data Science at the Command Line: More commands and tools helpful for doing data science, from the book of the same name Disclaimer With the exception of very small tasks, code is written so others can read it. With power comes responsibility. The fact you can do something in Bash doesn't necessarily mean you should! ;) License This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. ",
        "_version_":1718938182667468800},
      {
        "story_id":18964961,
        "story_author":"anuragsoni",
        "story_descendants":96,
        "story_score":120,
        "story_time":"2019-01-22T00:32:08Z",
        "story_title":"A command-line installer for Windows",
        "search":["A command-line installer for Windows",
          "Normal",
          "https://scoop.sh/",
          "A command-line installer for Windows Scoop installs the tools you know and love Get comfortable on the Windows command line Looking for familiar Unix tools? Tired of Powershells Verb-Noun verbosity? Scoop helps you get the programs you need, with a minimal amount of point-and-clicking. Say goodbye to permission pop-ups Scoop installs programs to your home directory by default. So you dont need admin permissions to install programs, and you wont see UAC popups every time you need to add or remove a program. Scoop reads the README for you Not sure whether you need 32-bit or 64-bit? Cant remember that command you have to type after you install to get that other thing you need? Scoop has you covered. Just scoop install and youll be ready to work in no time. Demo Installs in seconds Make sure PowerShell 5 (or later, include PowerShell Core) and .NET Framework 4.5 (or later) are installed. Then run: Invoke-Expression (New-Object System.Net.WebClient).DownloadString('https://get.scoop.sh') # or shorter iwr -useb get.scoop.sh | iex Note: if you get an error you might need to change the execution policy (i.e. enable Powershell) with Set-ExecutionPolicy RemoteSigned -scope CurrentUser ",
          "Reading the headline made of think of installing Windows via the command-line (say via imagex). Which is actually very useful in certain tricky situations when you mess up an install.<p>Once I didn't have enough space on a flash drive to install Windows via an ISO...but I did have a liveboot ubuntu system and 2 spare drives. Interestingly enough imagex and WIM tools were ported to linux, and it was a surprisingly easy experience.",
          "I struggle with all the alternate 'installation managers' for Windows. Microsoft has already done this with their MSI system (Although even they have basically abandoned it in favour of the APPX stuff). MSI was a good format, and could be installed silently with the right switches (/q/b), support alternate installation options via MST files, and even patches via MSP files. Even the specific DLL dependencies ('DLL Hell') got fixed via manifest files/attributes. Microsoft published the MSI specification early on, and encouraged everyone to support it.<p>However, the core problem with Windows software is there isn't a primary software repository for all approved and tested software, like there is for *nix platforms. Ideally that's what needs to be fixed first before yet more client-end installers get created.<p>To fix this, based on the current landscape, all third-party client-end installers need to support all existing third-party repositories.  Even better, everyone agrees on a standard JSON format (or whatever) for their repository manifest and all third-party installers understand that. Then just like Linux et al. all you have to do is add the new repository URL to your installers config, and all the packages advertised within are immediately available.<p>It seems really simple to fix - but people have to co-operate."],
        "story_type":"Normal",
        "url_raw":"https://scoop.sh/",
        "url_text":"A command-line installer for Windows Scoop installs the tools you know and love Get comfortable on the Windows command line Looking for familiar Unix tools? Tired of Powershells Verb-Noun verbosity? Scoop helps you get the programs you need, with a minimal amount of point-and-clicking. Say goodbye to permission pop-ups Scoop installs programs to your home directory by default. So you dont need admin permissions to install programs, and you wont see UAC popups every time you need to add or remove a program. Scoop reads the README for you Not sure whether you need 32-bit or 64-bit? Cant remember that command you have to type after you install to get that other thing you need? Scoop has you covered. Just scoop install and youll be ready to work in no time. Demo Installs in seconds Make sure PowerShell 5 (or later, include PowerShell Core) and .NET Framework 4.5 (or later) are installed. Then run: Invoke-Expression (New-Object System.Net.WebClient).DownloadString('https://get.scoop.sh') # or shorter iwr -useb get.scoop.sh | iex Note: if you get an error you might need to change the execution policy (i.e. enable Powershell) with Set-ExecutionPolicy RemoteSigned -scope CurrentUser ",
        "comments.comment_id":[18965786,
          18966728],
        "comments.comment_author":["pmalynin",
          "Jaruzel"],
        "comments.comment_descendants":[3,
          11],
        "comments.comment_time":["2019-01-22T03:28:33Z",
          "2019-01-22T07:56:21Z"],
        "comments.comment_text":["Reading the headline made of think of installing Windows via the command-line (say via imagex). Which is actually very useful in certain tricky situations when you mess up an install.<p>Once I didn't have enough space on a flash drive to install Windows via an ISO...but I did have a liveboot ubuntu system and 2 spare drives. Interestingly enough imagex and WIM tools were ported to linux, and it was a surprisingly easy experience.",
          "I struggle with all the alternate 'installation managers' for Windows. Microsoft has already done this with their MSI system (Although even they have basically abandoned it in favour of the APPX stuff). MSI was a good format, and could be installed silently with the right switches (/q/b), support alternate installation options via MST files, and even patches via MSP files. Even the specific DLL dependencies ('DLL Hell') got fixed via manifest files/attributes. Microsoft published the MSI specification early on, and encouraged everyone to support it.<p>However, the core problem with Windows software is there isn't a primary software repository for all approved and tested software, like there is for *nix platforms. Ideally that's what needs to be fixed first before yet more client-end installers get created.<p>To fix this, based on the current landscape, all third-party client-end installers need to support all existing third-party repositories.  Even better, everyone agrees on a standard JSON format (or whatever) for their repository manifest and all third-party installers understand that. Then just like Linux et al. all you have to do is add the new repository URL to your installers config, and all the packages advertised within are immediately available.<p>It seems really simple to fix - but people have to co-operate."],
        "id":"89fbba8f-cb6b-4c01-95bb-a5267416e087",
        "_version_":1718938140336455680},
      {
        "story_id":20372770,
        "story_author":"maxfan8",
        "story_descendants":34,
        "story_score":137,
        "story_time":"2019-07-06T23:57:16Z",
        "story_title":"Hub: Use GitHub from the Command Line",
        "search":["Hub: Use GitHub from the Command Line",
          "Normal",
          "https://hub.github.com/",
          "hub: use GitHub from the command-line hub is an extension to command-line git that helps you do everyday GitHub tasks without ever leaving the terminal. Read the full documentation: man hub, or visit this project on GitHub. brew install hub hub version git version 2.25.0 hub version 2.14.2 git config --global hub.protocol https Staying productive on the command-line hub makes it easy to clone or create repositories, browse project pages, list known issues, ensure your local branches stay up to date, and share logs or code snippets via Gist. hub clone dotfiles git clone git://github.com/YOUR_USER/dotfiles.git hub clone github/hub git clone git://github.com/github/hub.git cd myproject hub sync hub issue --limit 10 hub browse -- issues open https://github.com/github/hub/issues hub browse rbenv/ruby-build wiki open https://github.com/rbenv/ruby-build/wiki hub gist create --copy build.log (the URL of the new private gist copied to clipboard) Starting a new project has never been easier: git init git add . git commit -m \"And so, it begins.\" hub create (creates a new GitHub repository with the name of the current directory) git push -u origin HEAD Lowering the barrier to contributing to open-source Whether you are beginner or an experienced contributor to open-source, hub makes it easier to fork repositories, check the CI status of a branch, and even submit pull requests from the same environment where you write & commit your code. hub clone octocat/Spoon-Knife cd Spoon-Knife git checkout -b feature git commit -am \"done with feature\" hub fork --remote-name origin (forking repo on GitHub...) git remote add origin git@github.com:YOUR_USER/Spoon-Knife.git git push origin feature hub ci-status --verbose hub pull-request (opens a text editor for your pull request message) Automating tasks for fun and profit Scripting is much easier now that you can list or create issues, pull requests, and GitHub Releases in the format of your choice. hub issue --assignee YOUR_USER --labels urgent hub pr list --limit 20 --base develop --format='%t [%H] | %U%n' hub release create --copy -F release-notes.txt v2.3.0 (the URL of the new release copied to clipboard) Drop down to the API level Even if hub doesn't support the exact feature you need, you can use hub api to manually make requests against any GitHub APIeven GraphQLand have hub handle authentication, JSON encoding/decoding, and pagination for you. hub api repos/{owner}/{repo}/issues/123/comments --field body=@mycomment.txt REPO=\"github/hub\" SHA=\"b0db79db\" hub api graphql --flat -f q=\"repo:$REPO type:pr $SHA\" -f query=' query($q: String!) { search(query: $q, type: ISSUE, first: 3) { nodes { ... on PullRequest { url } } } } ' | awk '/\\.url/ { print $2 }' See hub-api-utils for more examples. Designed for open-source maintainers Maintaining a project is easier when you can easily fetch from other forks, check out pull requests, close issues, and even cherry-pick commits by URL. hub fetch mislav,cehoffman git remote add mislav git://github.com/mislav/hub.git git remote add cehoffman git://github.com/cehoffman/hub.git git fetch --multiple mislav cehoffman hub pr checkout 134 (creates a new branch with the contents of the pull request) git push hub issue update 134 --state closed hub am -3 https://github.com/github/hub/pull/134 hub cherry-pick https://github.com/xoebus/hub/commit/177eeb8 hub compare v0.9..v1.0 hub compare --url feature | pbcopy Using GitHub for work Save time at work by opening pull requests for code reviews and pushing to multiple remotes at once. Even GitHub Enterprise is supported. git config --global --add hub.host my.example.org hub issue transfer 123 NEWREPO git push origin feature hub pull-request --copy -F prepared-message.md (the URL of the new pull request copied to clipboard) hub push production,staging See the full reference documentation to learn more. ",
          "Hub is essential for anyone who works with GitHub repositories, IMO.<p>It gives you, if nothing else, an important shortcut: `hub checkout github.com/repo/pulls/1234` automatically adds a remote, fetches, and checks out the branch of a given pull request. Great for local testing/validation, when needed.",
          "While it is pretty cool, using such tool increases general lock-in to GitHub, in terms of both habits and potential use of it for automation of processes.<p>I wish there was an open standard for operations that hub allows to do and all major Git forges [1], including open source ones, such as Gogs/Gitea and GitLab, supported it. In that case having a command-line tool that, like Git itself, is not tied to a particular vendor, but allows to do what hub does, could have been indispensable.<p>[1] <a href=\"https://en.wikipedia.org/wiki/Forge_(software)\" rel=\"nofollow\">https://en.wikipedia.org/wiki/Forge_(software)</a>"],
        "story_type":"Normal",
        "url_raw":"https://hub.github.com/",
        "url_text":"hub: use GitHub from the command-line hub is an extension to command-line git that helps you do everyday GitHub tasks without ever leaving the terminal. Read the full documentation: man hub, or visit this project on GitHub. brew install hub hub version git version 2.25.0 hub version 2.14.2 git config --global hub.protocol https Staying productive on the command-line hub makes it easy to clone or create repositories, browse project pages, list known issues, ensure your local branches stay up to date, and share logs or code snippets via Gist. hub clone dotfiles git clone git://github.com/YOUR_USER/dotfiles.git hub clone github/hub git clone git://github.com/github/hub.git cd myproject hub sync hub issue --limit 10 hub browse -- issues open https://github.com/github/hub/issues hub browse rbenv/ruby-build wiki open https://github.com/rbenv/ruby-build/wiki hub gist create --copy build.log (the URL of the new private gist copied to clipboard) Starting a new project has never been easier: git init git add . git commit -m \"And so, it begins.\" hub create (creates a new GitHub repository with the name of the current directory) git push -u origin HEAD Lowering the barrier to contributing to open-source Whether you are beginner or an experienced contributor to open-source, hub makes it easier to fork repositories, check the CI status of a branch, and even submit pull requests from the same environment where you write & commit your code. hub clone octocat/Spoon-Knife cd Spoon-Knife git checkout -b feature git commit -am \"done with feature\" hub fork --remote-name origin (forking repo on GitHub...) git remote add origin git@github.com:YOUR_USER/Spoon-Knife.git git push origin feature hub ci-status --verbose hub pull-request (opens a text editor for your pull request message) Automating tasks for fun and profit Scripting is much easier now that you can list or create issues, pull requests, and GitHub Releases in the format of your choice. hub issue --assignee YOUR_USER --labels urgent hub pr list --limit 20 --base develop --format='%t [%H] | %U%n' hub release create --copy -F release-notes.txt v2.3.0 (the URL of the new release copied to clipboard) Drop down to the API level Even if hub doesn't support the exact feature you need, you can use hub api to manually make requests against any GitHub APIeven GraphQLand have hub handle authentication, JSON encoding/decoding, and pagination for you. hub api repos/{owner}/{repo}/issues/123/comments --field body=@mycomment.txt REPO=\"github/hub\" SHA=\"b0db79db\" hub api graphql --flat -f q=\"repo:$REPO type:pr $SHA\" -f query=' query($q: String!) { search(query: $q, type: ISSUE, first: 3) { nodes { ... on PullRequest { url } } } } ' | awk '/\\.url/ { print $2 }' See hub-api-utils for more examples. Designed for open-source maintainers Maintaining a project is easier when you can easily fetch from other forks, check out pull requests, close issues, and even cherry-pick commits by URL. hub fetch mislav,cehoffman git remote add mislav git://github.com/mislav/hub.git git remote add cehoffman git://github.com/cehoffman/hub.git git fetch --multiple mislav cehoffman hub pr checkout 134 (creates a new branch with the contents of the pull request) git push hub issue update 134 --state closed hub am -3 https://github.com/github/hub/pull/134 hub cherry-pick https://github.com/xoebus/hub/commit/177eeb8 hub compare v0.9..v1.0 hub compare --url feature | pbcopy Using GitHub for work Save time at work by opening pull requests for code reviews and pushing to multiple remotes at once. Even GitHub Enterprise is supported. git config --global --add hub.host my.example.org hub issue transfer 123 NEWREPO git push origin feature hub pull-request --copy -F prepared-message.md (the URL of the new pull request copied to clipboard) hub push production,staging See the full reference documentation to learn more. ",
        "comments.comment_id":[20373720,
          20374106],
        "comments.comment_author":["fcarraldo",
          "inlineint"],
        "comments.comment_descendants":[4,
          4],
        "comments.comment_time":["2019-07-07T05:39:09Z",
          "2019-07-07T08:21:52Z"],
        "comments.comment_text":["Hub is essential for anyone who works with GitHub repositories, IMO.<p>It gives you, if nothing else, an important shortcut: `hub checkout github.com/repo/pulls/1234` automatically adds a remote, fetches, and checks out the branch of a given pull request. Great for local testing/validation, when needed.",
          "While it is pretty cool, using such tool increases general lock-in to GitHub, in terms of both habits and potential use of it for automation of processes.<p>I wish there was an open standard for operations that hub allows to do and all major Git forges [1], including open source ones, such as Gogs/Gitea and GitLab, supported it. In that case having a command-line tool that, like Git itself, is not tied to a particular vendor, but allows to do what hub does, could have been indispensable.<p>[1] <a href=\"https://en.wikipedia.org/wiki/Forge_(software)\" rel=\"nofollow\">https://en.wikipedia.org/wiki/Forge_(software)</a>"],
        "id":"1720c1f1-e107-4b47-9517-95a92ca4cfab",
        "_version_":1718938196177321984},
      {
        "story_id":21572308,
        "story_author":"dragondax",
        "story_descendants":47,
        "story_score":98,
        "story_time":"2019-11-19T12:23:01Z",
        "story_title":"Run an Internet Speed Test from the Command Line",
        "search":["Run an Internet Speed Test from the Command Line",
          "Normal",
          "https://www.putorius.net/speed-test-command-line.html",
          "We have all used tools like speedtest.net to test upload and download speeds. Whether it was to test the WiFi in that coffee shop (I use my own tether, never unknown hot spots), preparing for a LAN party (do people still do that?), or just a step in troubleshooting, we have all been there. For one reason or another you simply think you are being cheated of bandwidth, so you want independent verification of your speeds. This typically means opening a browser and going to a website to test your connection. But what if you want to run a speed test on a remote server? In this article we will discuss running an internet speed test from the Linux command line, and skipping the browser. There is something about the raw efficiency of the command line that I am really attracted to. As I discussed in the article 5 Command Line Tool to Break Your Dependence on the GUI, I try my best to stay away from the browser. It usually creates an unnecessary distraction. The internet is designed to grab your attention like a laser pointer does to a cat. So lets get started, and figure out one more way to stay away from the GUI. Different Speed Test Packages There are a few different tools you can use to run a speed test from the command line. To make things even more confusing the two most popular share the same exact name, but both use the speedtest.net service. Unofficial Speedtest-CLI Python Script The first one is an independently written Python script that is simple to install and use. It is available in the default repositories for some popular Linux distributions. Pros: Easy to installWide AvailabilityFull list of serversCan specify upload test, download test, or both Cons: Minimal output format optionsNo verbose output option Jump to Installing speedtest-cli Python Script or How to Use speedtest-cli Python Script. Official Ookla Speedtest CLI The second tool is built by Ookla, the people who bring you the speedtest.net website and service. Installing it requires you to add a repo for your package manager. But the maintainers offer simple instructions for installation. Pros: Official release from OoklaMore robust formatting optionsOutput easier to read, better layoutVerbose output availableHas repo making it easy to get updates Cons: Use limited to nearby serversCannot specify download or upload only Jump to Official Ookla Speedtest CLI The Speedtest-cli Python Script This is an easy way to get started running a speed test on the Linux command line. Installing the Speedtest-cli Python Script Simply use your package manager to install the package. Install on Fedora using DNF sudo dnf install speedtest-cli Ubuntu or Debian using APT sudo apt-get install speedtest-cli CentOS/Red Hat 7 / 8 Unfortunately, CentOS does not offer the rpm in their repos. It can still be easily installed. Change to /usr/bin directory to make command available to all users: cd /usr/bin Install dependencies: sudo yum install -y python wget Fetch script from github: sudo wget https://raw.githubusercontent.com/sivel/speedtest-cli/master/speedtest.py -O speedtest-cli Make script executable: sudo chmod +x speedtest-cli Or just copy and paste the whole thing below as a single line: cd /usr/bin; sudo yum install -y python wget && wget https://raw.githubusercontent.com/sivel/speedtest-cli/master/speedtest.py -O speedtest-cli && sudo chmod +x speedtest-cli How to Use the Python Script to Run a Speed Test The most basic usage is to simply run the command. It will automatically select the best server based on ping responses. Speedtest-cli Python Script Options There are several options available to change the default behavior. Here we will outline the most popular options. List Available Speed Test Servers You can use the list option to find a list of available servers to run your test against. At the time of writing this list is pretty extensive with 8829 possible servers. NOTE: The servers are sorted by distance, closest first. [[emailprotected] ~]$ speedtest-cli --list Retrieving speedtest.net configuration 4847) Hotwire Fision (Philadelphia, PA, United States) [10.92 km] 10979) School District of Philadelphia (Philadelphia, PA, United States) [10.92 km] ...OUTPUT TRUNCATED... Specify Specific Server to Test Against Once you have found the server you want to test against, you can use the server <SERVER ID> to select it. The server ID is the first column in the output of the list option above. [[emailprotected] ~]$ speedtest-cli --server 4847 Retrieving speedtest.net configuration Testing from Verizon Fios (x.x.x.x) Retrieving speedtest.net server list Selecting best server based on ping Hosted by School District of Philadelphia (Philadelphia, PA) [10.92 km]: 25.033 ms Testing download speed.. Download: 384.07 Mbit/s Testing upload speed Upload: 417.93 Mbit/s Only Test Upload or Download Speeds The option is actually designed to exclude a test. But since there are only two options it is effectively the same as selecting only one. To run only the download test, you exclude the upload, and vice versa. [[emailprotected] ~]$ speedtest-cli --no-upload Retrieving speedtest.net configuration Testing from Verizon Fios (x.x.x.x) Retrieving speedtest.net server list Selecting best server based on ping Hosted by KamaTera INC (New Jersey, NJ) [62.36 km]: 19.785 ms Testing download speed.. Download: 600.89 Mbit/s Skipping upload test Format Output in JSON or CSV You can specify the output format in JSON or CSV. You also have the opton to use CSV with a custom delimiter. This is handy if you are going to use the output in some other script or application. [[emailprotected] ~]$ speedtest-cli --json {\"download\": 597726146.0529929, \"upload\": 562476134.8046777, \"ping\": 17.004, \"server\": {\"url\": \"http://speedtest.us-ny2.kamatera.com:8080/speedtest/upload.php\", \"lat\": \"40.0583\", \"lon\": \"-74.4057\", \"name\": \"New Jersey, NJ\", \"country\": \"United States\", \"cc\": \"US\", \"sponsor\": \"KamaTera INC\", \"id\": \"11612\" ...OUTPUT TRUNCATED... Using CSV with a custom delimiter. The default delimiter is a comma, which is implied by the name CSV. Here we use the csv-delimiter option to change the delimiter to a pipe character. [[emailprotected] ~]$ speedtest-cli --csv --csv-delimiter \"|\" 11612|KamaTera INC|New Jersey, NJ|2019-11-17T14:51:53.636981Z|62.35865439150934|8.546|588013638.8767571|512001168.48230773||x.x.x.x The Official Ookla Speedtest CLI The official Speedtest CLI (Command Line Interface) from Ookla is a little more robust. It has all of the options of the python script and more. There are also several output formats not available with the unofficial python script. Ooklas speedtest is also a little easier on the eyes. It spreads the information out which makes it easier to read and displays a neat little progress bar. A URL you can use to share the results is also displayed by default. Installing the Official Speedtest CLI Install Speedtest CLI on Ubuntu / Debian: The Speedtest CLI from Ookla is supported on Ubuntu (xenial & bionic) and Debian (jessie, stretch, buster). $ sudo apt-get install gnupg1 apt-transport-https dirmngr $ export INSTALL_KEY=379CE192D401AB61 $ export DEB_DISTRO=$(lsb_release -sc) $ sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys $INSTALL_KEY $ echo \"deb https://ookla.bintray.com/debian ${DEB_DISTRO} main\" | sudo tee /etc/apt/sources.list.d/speedtest.list $ sudo apt-get update $ sudo apt-get install speedtest Install Speedtest CLI on Fedora / Redhat / CentOS: Fedora has moved on to DNF for package management, but is still compatible with YUM. These instructions were tested on Fedora 31, CentOS 7 and Red Hat 8. $ sudo yum install wget $ wget https://bintray.com/ookla/rhel/rpm -O bintray-ookla-rhel.repo $ sudo mv bintray-ookla-rhel.repo /etc/yum.repos.d/ $ sudo yum install speedtest How to Use the Official Speedtest CLI Once installed you can simply call the utility by typing speedtest at the command line. This will give you all the default information that you would see on the web version of speedtest.net. Official Speedtest CLI Options The options available in the official release are more robust. Here we will outline the popular options and how to use them. List Available Speed Test Servers Using the -L (servers) option will give you a list of servers available to run a test against. This option will only show you servers that are nearby. What exactly determines nearby is undefined. But for me it looks like they are staying in the tri-state area (PA, NJ, DE). [[emailprotected] ~]$ speedtest -L Closest servers: ID Name Location Country 4847 Hotwire Fision Philadelphia, PA United States 10979 School District of Philadelphia Philadelphia, PA United States 9840 Comcast New Castle, DE United States 11612 KamaTera INC New Jersey, NJ United States ...OUTPUT TRUNCATED... Optionally, you can use the -o (host) option and specify the FQDN of the server instead of the ID. But oddly, I dont see a way to get the FQDN of the servers on the list. I am guessing this option is available for using a custom server. I havent found a way to list all servers. If you are looking to test against a server on the other side of the country, you will have to find it another way. Select Specific Server to Run Speed Test Against You can use the -s (server-id) option to select a server to use from the list. You must supply the server ID with this option. The server ID is the number in the first column of the list output above. [[emailprotected] ~]$ speedtest -s 4847 Speedtest by Ookla Server: Hotwire Fision - Philadelphia, PA (id = 4847) Change Unit Used for Speed Output The -u (unit) option can display the speed output in many different formats. Decimal prefix, bits per second: bps, kbps, Mbps, Gbps Decimal prefix, bytes per second: B/s, kB/s, MB/s, GB/s Binary prefix, bits per second: kibps, Mibps, Gibps Binary prefix, bytes per second: kiB/s, MiB/s, GiB/s [[emailprotected] ~]$ speedtest -u MiB/s Speedtest by Ookla Server: Hotwire Fision - Philadelphia, PA (id = 4847) ISP: Verizon Fios Latency: 10.67 ms ( 0.95 ms jitter) Download: 63.45 MiB/s (data used: 700.2 MiB) ...OUTPUT TRUNCATED... Output Formatting Options The Ookla Speedtest CLI offers decent options for output formats. Human Readable DefaultCSV Comma Separated ValueTSV Tab Separated ValueJSON JavaScript Object NotationJSONL JSON LinesJSON-PRETTY JSON Pretty Printed Here is an example using json-pretty. [[emailprotected] ~]$ speedtest -f json-pretty { \"type\": \"result\", \"timestamp\": \"2019-11-17T16:42:06Z\", \"ping\": { \"jitter\": 0.29899999999999999, \"latency\": 17.474 }, \"download\": { \"bandwidth\": 92184614, \"bytes\": 491967724, \"elapsed\": 5303 }, \"upload\": { \"bandwidth\": 45010100, \"bytes\": 313859035, \"elapsed\": 6714 }, ...OUTPUT TRUNCATED... Conclusion Running a speed test from the command line may not be something that is needed on a daily basis for most people. However, it may prove useful in some troubleshooting situations. In this article we cover how to run a speed test from the command line using two similar tools. The unofficial python script and the official Ookla Speedtest CLI. We discussed installing, using and setting options for each one. This should be enough to get you started. For more information on these tools, visit their respective home pages found in the resources section below. Resources and Links: Ookla Speedtest CLISpeedtest-cli (Python Version) on GitHub ",
          "Note that some ISPs prioritise traffic to known speedtest targets, turning off traffic shaping rules that might otherwise slow bulk transfers. When this happens it means you are testing the likely maximum throughput of your connection not necessarily the throughput you will see more generally.<p>This is why Netflix started fast.com - because it draws data from the same distribution points as their video streaming apps it means you can't prioritise the speedtest without also doing so for the video traffic or (more likely) you can't de-prioritise the video traffic without also getting bad scores in that particular speedtest. From Netflix's point of view it is an answer to people contacting support with \"my speedtest results are fine, the problem must be your servers\" when they are experiencing video lag/drops and other such problems and the issue is due to ISP traffic shaping or the ISP simply not having enough backhaul bandwidth.<p>A more reliable test might be taking part in a busy public torrent: that way you are testing against arbitrary locations so your ISP can't be setting different shaping rules for them. Just remember to throttle upstream when testing downstream and vice-versa or saturation in the other direction will slow control packets that will in turn give you lower results for the one you are testing. This may fall into another trap though: unless you limit the number of active streams it may be an unrealistic test as more generally most processes use a small number of streams (or just a single one), and if you limit the number of streams too much you might get a lower result because each swarm member you connect to may be fairly saturated and sharing its bandwidth amongst many connections.",
          "speedtest-cli is <i>garbage</i> if you have >100Mbps Speeds. The dev refuses to acknowledge this: <a href=\"https://github.com/sivel/speedtest-cli/issues/226\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/226</a><p>Also not just me:\n<a href=\"https://github.com/sivel/speedtest-cli/issues/649\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/649</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/648\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/648</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/641\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/641</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/616\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/616</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/601\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/601</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/588\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/588</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/546\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/546</a>"],
        "story_type":"Normal",
        "url_raw":"https://www.putorius.net/speed-test-command-line.html",
        "comments.comment_id":[21582731,
          21583805],
        "comments.comment_author":["dspillett",
          "virtuallynathan"],
        "comments.comment_descendants":[5,
          4],
        "comments.comment_time":["2019-11-20T10:49:21Z",
          "2019-11-20T13:46:04Z"],
        "comments.comment_text":["Note that some ISPs prioritise traffic to known speedtest targets, turning off traffic shaping rules that might otherwise slow bulk transfers. When this happens it means you are testing the likely maximum throughput of your connection not necessarily the throughput you will see more generally.<p>This is why Netflix started fast.com - because it draws data from the same distribution points as their video streaming apps it means you can't prioritise the speedtest without also doing so for the video traffic or (more likely) you can't de-prioritise the video traffic without also getting bad scores in that particular speedtest. From Netflix's point of view it is an answer to people contacting support with \"my speedtest results are fine, the problem must be your servers\" when they are experiencing video lag/drops and other such problems and the issue is due to ISP traffic shaping or the ISP simply not having enough backhaul bandwidth.<p>A more reliable test might be taking part in a busy public torrent: that way you are testing against arbitrary locations so your ISP can't be setting different shaping rules for them. Just remember to throttle upstream when testing downstream and vice-versa or saturation in the other direction will slow control packets that will in turn give you lower results for the one you are testing. This may fall into another trap though: unless you limit the number of active streams it may be an unrealistic test as more generally most processes use a small number of streams (or just a single one), and if you limit the number of streams too much you might get a lower result because each swarm member you connect to may be fairly saturated and sharing its bandwidth amongst many connections.",
          "speedtest-cli is <i>garbage</i> if you have >100Mbps Speeds. The dev refuses to acknowledge this: <a href=\"https://github.com/sivel/speedtest-cli/issues/226\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/226</a><p>Also not just me:\n<a href=\"https://github.com/sivel/speedtest-cli/issues/649\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/649</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/648\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/648</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/641\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/641</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/616\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/616</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/601\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/601</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/588\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/588</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/546\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/546</a>"],
        "id":"30879246-c02d-42a1-ac2f-756feb1bb7d6",
        "url_text":"We have all used tools like speedtest.net to test upload and download speeds. Whether it was to test the WiFi in that coffee shop (I use my own tether, never unknown hot spots), preparing for a LAN party (do people still do that?), or just a step in troubleshooting, we have all been there. For one reason or another you simply think you are being cheated of bandwidth, so you want independent verification of your speeds. This typically means opening a browser and going to a website to test your connection. But what if you want to run a speed test on a remote server? In this article we will discuss running an internet speed test from the Linux command line, and skipping the browser. There is something about the raw efficiency of the command line that I am really attracted to. As I discussed in the article 5 Command Line Tool to Break Your Dependence on the GUI, I try my best to stay away from the browser. It usually creates an unnecessary distraction. The internet is designed to grab your attention like a laser pointer does to a cat. So lets get started, and figure out one more way to stay away from the GUI. Different Speed Test Packages There are a few different tools you can use to run a speed test from the command line. To make things even more confusing the two most popular share the same exact name, but both use the speedtest.net service. Unofficial Speedtest-CLI Python Script The first one is an independently written Python script that is simple to install and use. It is available in the default repositories for some popular Linux distributions. Pros: Easy to installWide AvailabilityFull list of serversCan specify upload test, download test, or both Cons: Minimal output format optionsNo verbose output option Jump to Installing speedtest-cli Python Script or How to Use speedtest-cli Python Script. Official Ookla Speedtest CLI The second tool is built by Ookla, the people who bring you the speedtest.net website and service. Installing it requires you to add a repo for your package manager. But the maintainers offer simple instructions for installation. Pros: Official release from OoklaMore robust formatting optionsOutput easier to read, better layoutVerbose output availableHas repo making it easy to get updates Cons: Use limited to nearby serversCannot specify download or upload only Jump to Official Ookla Speedtest CLI The Speedtest-cli Python Script This is an easy way to get started running a speed test on the Linux command line. Installing the Speedtest-cli Python Script Simply use your package manager to install the package. Install on Fedora using DNF sudo dnf install speedtest-cli Ubuntu or Debian using APT sudo apt-get install speedtest-cli CentOS/Red Hat 7 / 8 Unfortunately, CentOS does not offer the rpm in their repos. It can still be easily installed. Change to /usr/bin directory to make command available to all users: cd /usr/bin Install dependencies: sudo yum install -y python wget Fetch script from github: sudo wget https://raw.githubusercontent.com/sivel/speedtest-cli/master/speedtest.py -O speedtest-cli Make script executable: sudo chmod +x speedtest-cli Or just copy and paste the whole thing below as a single line: cd /usr/bin; sudo yum install -y python wget && wget https://raw.githubusercontent.com/sivel/speedtest-cli/master/speedtest.py -O speedtest-cli && sudo chmod +x speedtest-cli How to Use the Python Script to Run a Speed Test The most basic usage is to simply run the command. It will automatically select the best server based on ping responses. Speedtest-cli Python Script Options There are several options available to change the default behavior. Here we will outline the most popular options. List Available Speed Test Servers You can use the list option to find a list of available servers to run your test against. At the time of writing this list is pretty extensive with 8829 possible servers. NOTE: The servers are sorted by distance, closest first. [[emailprotected] ~]$ speedtest-cli --list Retrieving speedtest.net configuration 4847) Hotwire Fision (Philadelphia, PA, United States) [10.92 km] 10979) School District of Philadelphia (Philadelphia, PA, United States) [10.92 km] ...OUTPUT TRUNCATED... Specify Specific Server to Test Against Once you have found the server you want to test against, you can use the server <SERVER ID> to select it. The server ID is the first column in the output of the list option above. [[emailprotected] ~]$ speedtest-cli --server 4847 Retrieving speedtest.net configuration Testing from Verizon Fios (x.x.x.x) Retrieving speedtest.net server list Selecting best server based on ping Hosted by School District of Philadelphia (Philadelphia, PA) [10.92 km]: 25.033 ms Testing download speed.. Download: 384.07 Mbit/s Testing upload speed Upload: 417.93 Mbit/s Only Test Upload or Download Speeds The option is actually designed to exclude a test. But since there are only two options it is effectively the same as selecting only one. To run only the download test, you exclude the upload, and vice versa. [[emailprotected] ~]$ speedtest-cli --no-upload Retrieving speedtest.net configuration Testing from Verizon Fios (x.x.x.x) Retrieving speedtest.net server list Selecting best server based on ping Hosted by KamaTera INC (New Jersey, NJ) [62.36 km]: 19.785 ms Testing download speed.. Download: 600.89 Mbit/s Skipping upload test Format Output in JSON or CSV You can specify the output format in JSON or CSV. You also have the opton to use CSV with a custom delimiter. This is handy if you are going to use the output in some other script or application. [[emailprotected] ~]$ speedtest-cli --json {\"download\": 597726146.0529929, \"upload\": 562476134.8046777, \"ping\": 17.004, \"server\": {\"url\": \"http://speedtest.us-ny2.kamatera.com:8080/speedtest/upload.php\", \"lat\": \"40.0583\", \"lon\": \"-74.4057\", \"name\": \"New Jersey, NJ\", \"country\": \"United States\", \"cc\": \"US\", \"sponsor\": \"KamaTera INC\", \"id\": \"11612\" ...OUTPUT TRUNCATED... Using CSV with a custom delimiter. The default delimiter is a comma, which is implied by the name CSV. Here we use the csv-delimiter option to change the delimiter to a pipe character. [[emailprotected] ~]$ speedtest-cli --csv --csv-delimiter \"|\" 11612|KamaTera INC|New Jersey, NJ|2019-11-17T14:51:53.636981Z|62.35865439150934|8.546|588013638.8767571|512001168.48230773||x.x.x.x The Official Ookla Speedtest CLI The official Speedtest CLI (Command Line Interface) from Ookla is a little more robust. It has all of the options of the python script and more. There are also several output formats not available with the unofficial python script. Ooklas speedtest is also a little easier on the eyes. It spreads the information out which makes it easier to read and displays a neat little progress bar. A URL you can use to share the results is also displayed by default. Installing the Official Speedtest CLI Install Speedtest CLI on Ubuntu / Debian: The Speedtest CLI from Ookla is supported on Ubuntu (xenial & bionic) and Debian (jessie, stretch, buster). $ sudo apt-get install gnupg1 apt-transport-https dirmngr $ export INSTALL_KEY=379CE192D401AB61 $ export DEB_DISTRO=$(lsb_release -sc) $ sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys $INSTALL_KEY $ echo \"deb https://ookla.bintray.com/debian ${DEB_DISTRO} main\" | sudo tee /etc/apt/sources.list.d/speedtest.list $ sudo apt-get update $ sudo apt-get install speedtest Install Speedtest CLI on Fedora / Redhat / CentOS: Fedora has moved on to DNF for package management, but is still compatible with YUM. These instructions were tested on Fedora 31, CentOS 7 and Red Hat 8. $ sudo yum install wget $ wget https://bintray.com/ookla/rhel/rpm -O bintray-ookla-rhel.repo $ sudo mv bintray-ookla-rhel.repo /etc/yum.repos.d/ $ sudo yum install speedtest How to Use the Official Speedtest CLI Once installed you can simply call the utility by typing speedtest at the command line. This will give you all the default information that you would see on the web version of speedtest.net. Official Speedtest CLI Options The options available in the official release are more robust. Here we will outline the popular options and how to use them. List Available Speed Test Servers Using the -L (servers) option will give you a list of servers available to run a test against. This option will only show you servers that are nearby. What exactly determines nearby is undefined. But for me it looks like they are staying in the tri-state area (PA, NJ, DE). [[emailprotected] ~]$ speedtest -L Closest servers: ID Name Location Country 4847 Hotwire Fision Philadelphia, PA United States 10979 School District of Philadelphia Philadelphia, PA United States 9840 Comcast New Castle, DE United States 11612 KamaTera INC New Jersey, NJ United States ...OUTPUT TRUNCATED... Optionally, you can use the -o (host) option and specify the FQDN of the server instead of the ID. But oddly, I dont see a way to get the FQDN of the servers on the list. I am guessing this option is available for using a custom server. I havent found a way to list all servers. If you are looking to test against a server on the other side of the country, you will have to find it another way. Select Specific Server to Run Speed Test Against You can use the -s (server-id) option to select a server to use from the list. You must supply the server ID with this option. The server ID is the number in the first column of the list output above. [[emailprotected] ~]$ speedtest -s 4847 Speedtest by Ookla Server: Hotwire Fision - Philadelphia, PA (id = 4847) Change Unit Used for Speed Output The -u (unit) option can display the speed output in many different formats. Decimal prefix, bits per second: bps, kbps, Mbps, Gbps Decimal prefix, bytes per second: B/s, kB/s, MB/s, GB/s Binary prefix, bits per second: kibps, Mibps, Gibps Binary prefix, bytes per second: kiB/s, MiB/s, GiB/s [[emailprotected] ~]$ speedtest -u MiB/s Speedtest by Ookla Server: Hotwire Fision - Philadelphia, PA (id = 4847) ISP: Verizon Fios Latency: 10.67 ms ( 0.95 ms jitter) Download: 63.45 MiB/s (data used: 700.2 MiB) ...OUTPUT TRUNCATED... Output Formatting Options The Ookla Speedtest CLI offers decent options for output formats. Human Readable DefaultCSV Comma Separated ValueTSV Tab Separated ValueJSON JavaScript Object NotationJSONL JSON LinesJSON-PRETTY JSON Pretty Printed Here is an example using json-pretty. [[emailprotected] ~]$ speedtest -f json-pretty { \"type\": \"result\", \"timestamp\": \"2019-11-17T16:42:06Z\", \"ping\": { \"jitter\": 0.29899999999999999, \"latency\": 17.474 }, \"download\": { \"bandwidth\": 92184614, \"bytes\": 491967724, \"elapsed\": 5303 }, \"upload\": { \"bandwidth\": 45010100, \"bytes\": 313859035, \"elapsed\": 6714 }, ...OUTPUT TRUNCATED... Conclusion Running a speed test from the command line may not be something that is needed on a daily basis for most people. However, it may prove useful in some troubleshooting situations. In this article we cover how to run a speed test from the command line using two similar tools. The unofficial python script and the official Ookla Speedtest CLI. We discussed installing, using and setting options for each one. This should be enough to get you started. For more information on these tools, visit their respective home pages found in the resources section below. Resources and Links: Ookla Speedtest CLISpeedtest-cli (Python Version) on GitHub ",
        "_version_":1718938242632384512},
      {
        "story_id":19271135,
        "story_author":"ColinWright",
        "story_descendants":169,
        "story_score":187,
        "story_time":"2019-02-28T13:23:06Z",
        "story_title":"The hard part in becoming a command line wizard",
        "search":["The hard part in becoming a command line wizard",
          "Normal",
          "https://www.johndcook.com/blog/2019/02/18/command-line-wizard/",
          "Ive long been impressed by shell one-liners. They seem like magical incantations. Pipe a few terse commands together,et voil! Out pops the solution to a problem that would seem to require pages of code. Are these one-liners real or mythology? To some extent, theyre both. Below Ill give a famous real example. Then Ill argue that even though such examples do occur, they may create unrealistic expectations. Bentleys exercise In 1986, Jon Bentley posted the following exercise: Given a text file and an integer k, print the k most common words in the file (and the number of their occurrences) in decreasing frequency. Donald Knuth wrote an elegant program in response. Knuths program runs for 17 pages in his book Literate Programming. McIlroys solution is short enough to quote below [1]. tr -cs A-Za-z ' ' | tr A-Z a-z | sort | uniq -c | sort -rn | sed ${1}q McIlroys response to Knuth was like Abraham Lincolns response to Edward Everett at Gettysburg. Lincolns famous address was 50x shorter than that of the orator who preceded him [2]. (Update: Theres more to the story. See [3].) Knuth and McIlroy had very different objectives and placed different constraints on themselves, and so their solutions are not directly comparable. But McIlroys solution has become famous. Knuths solution is remembered, if at all, as the verbose program that McIlroy responded to. The stereotype of a Unix wizard is someone who could improvise programs like the one above. Maybe McIlroy carefully thought about his program for days, looking for the most elegant solution. That would seem plausible,but in fact he says the script was written on the spot and worked on the first try. He said that the script was similar to one he had written a year before, but it still counts as an improvisation. Why cant I write scripts like that? McIlroys script was a real example of the kind of wizardry attributed to Unix adepts. Why cant more people quickly improvise scripts like that? The exercise that Bentley posed was the kind of problem that programmers like McIlroy solved routinely at the time. The tools he piped together were developed precisely for such problems. McIlroy didnt see his solution as extraordinary but said Old UNIX hands know instinctively how to solve this one in a jiffy. The traditional Unix toolbox is full of utilities for text manipulation. Not only are they useful, but they compose well. This composability depends not only on the tools themselves, but also the shell environment they were designed to operate in. (The latter is why some utilities dont work as well when ported to other operating systems, even if the functionality is duplicated.) Bentleys exercise was clearly text-based: given a text file, produce a text file. What about problems that are not text manipulation? The trick to being productive from a command line is to turn problems into text manipulation problems. The output of a shell command is text. Programs are text. Once you get into the necessary mindset, everything is text. This may not be the most efficient approach to a given problem, but its a possible strategy. The hard part on the path to becoming a command line wizard, or any kind of wizard, is thinking about how to apply existing tools to your particular problems. You could memorize McIlroys script and be prepared next time you need to report word frequencies, but applying thespirit of his script to your particular problems takes work. Reading one-liners that other people have developed for their work may be inspiring, or intimidating, but theyre no substitute for thinking hard about your particular work. Repetition You get faster at anything with repetition. Maybe you dont solve any particular kind of problem often enough to be fluent at solving it. If someone can solve a problem by quickly typing a one-liner in a shell, maybe they are clever, or maybe their job is repetitive. Or maybe both: maybe theyve found a way to make semi-repetitive tasks repetitive enough to automate. One way to become more productive is to split semi-repetitive tasks into more creative and more repetitive parts. More command line posts Hipster computing Improving on the Unix shell [1] The odd-looking line break is a quoted newline. [2] Everetts speech contained 13,607 words while Lincolns Gettysburg Address contained 272, a ratio of almost exactly 50 to 1. [3] See Hillel Waynes post Donald Knuth was Framed. Heres an excerpt: Most of the eight pages arent because Knuth is doing LP [literate programming], but because hes Donald Knuth: One page is him setting up the problem (what do we mean by word? What if multiple words share the same frequency?) and one page is just the index. Another page is just about working around specific Pascal issues no modern language has, like how do we read in an integer and how do we identify letters when Pascals character set is poorly defined. Then theres almost four pages of handrolling a hash trie. The eight pages refers to the length of the original publication. I described the paper as 17 pages because that the length in the book where I found it. ",
          "The Lisp philosophy is very similar to the unix philosophy, but with one crucial modification: instead of \"everything is text\" the mantra becomes \"everything is an S-expression\".<p>The reason s-expressions are better than text is that they are more expressive.  They allow arbitrary levels of nesting whereas text doesn't.  Text naturally develops a record structure with only two levels of hierarchy: records separated by newlines, containing fields separated by some other delimiter (usually spaces, tabs or commas, sometimes pipes, rarely anything else).  Going any deeper than that is not \"native\" for text.  That's why, say, parsing HTML is \"hard\" for the unix mindset.  But a Lisper naturally sees HTML (and XML and JSON and, well, just about everything) as just an S-expression with a more cumbersome syntax.",
          "I like the \"everything is text\" mentality, but really, the \"hard part\" of the GNU/Linux CLI is getting things done <i>when you don't already know how</i>.<p>If you've memorized the hundreds of different invocation arguments for find, sed, grep, tr, sort, uniq, cat, xargs, etc. -- then yes, you can be very fast. If not -- be prepared to RTFM for about 4 different programs to get your job done.<p>That's an incredible hurdle and it really only makes sense if you are using these tools daily/weekly. I'm still not completely convinced that its \"worth it\" to master these tools, when general-purpose scripting languages are <i>almost</i> as succinct and obviously way more powerful. That's speaking as someone who's written a fair amount of Bash.<p>In contrast, I think tools with higher discoverability and self-documenting nature (arguably, Excel) usually offer a more user-friendly way of doing the same thing, unless you are doing it <i>a lot</i>."],
        "story_type":"Normal",
        "url_raw":"https://www.johndcook.com/blog/2019/02/18/command-line-wizard/",
        "comments.comment_id":[19272544,
          19274054],
        "comments.comment_author":["lisper",
          "l_t"],
        "comments.comment_descendants":[2,
          3],
        "comments.comment_time":["2019-02-28T16:31:30Z",
          "2019-02-28T19:02:34Z"],
        "comments.comment_text":["The Lisp philosophy is very similar to the unix philosophy, but with one crucial modification: instead of \"everything is text\" the mantra becomes \"everything is an S-expression\".<p>The reason s-expressions are better than text is that they are more expressive.  They allow arbitrary levels of nesting whereas text doesn't.  Text naturally develops a record structure with only two levels of hierarchy: records separated by newlines, containing fields separated by some other delimiter (usually spaces, tabs or commas, sometimes pipes, rarely anything else).  Going any deeper than that is not \"native\" for text.  That's why, say, parsing HTML is \"hard\" for the unix mindset.  But a Lisper naturally sees HTML (and XML and JSON and, well, just about everything) as just an S-expression with a more cumbersome syntax.",
          "I like the \"everything is text\" mentality, but really, the \"hard part\" of the GNU/Linux CLI is getting things done <i>when you don't already know how</i>.<p>If you've memorized the hundreds of different invocation arguments for find, sed, grep, tr, sort, uniq, cat, xargs, etc. -- then yes, you can be very fast. If not -- be prepared to RTFM for about 4 different programs to get your job done.<p>That's an incredible hurdle and it really only makes sense if you are using these tools daily/weekly. I'm still not completely convinced that its \"worth it\" to master these tools, when general-purpose scripting languages are <i>almost</i> as succinct and obviously way more powerful. That's speaking as someone who's written a fair amount of Bash.<p>In contrast, I think tools with higher discoverability and self-documenting nature (arguably, Excel) usually offer a more user-friendly way of doing the same thing, unless you are doing it <i>a lot</i>."],
        "id":"f1e20a2b-8480-439d-91fe-3a523d3037fe",
        "url_text":"Ive long been impressed by shell one-liners. They seem like magical incantations. Pipe a few terse commands together,et voil! Out pops the solution to a problem that would seem to require pages of code. Are these one-liners real or mythology? To some extent, theyre both. Below Ill give a famous real example. Then Ill argue that even though such examples do occur, they may create unrealistic expectations. Bentleys exercise In 1986, Jon Bentley posted the following exercise: Given a text file and an integer k, print the k most common words in the file (and the number of their occurrences) in decreasing frequency. Donald Knuth wrote an elegant program in response. Knuths program runs for 17 pages in his book Literate Programming. McIlroys solution is short enough to quote below [1]. tr -cs A-Za-z ' ' | tr A-Z a-z | sort | uniq -c | sort -rn | sed ${1}q McIlroys response to Knuth was like Abraham Lincolns response to Edward Everett at Gettysburg. Lincolns famous address was 50x shorter than that of the orator who preceded him [2]. (Update: Theres more to the story. See [3].) Knuth and McIlroy had very different objectives and placed different constraints on themselves, and so their solutions are not directly comparable. But McIlroys solution has become famous. Knuths solution is remembered, if at all, as the verbose program that McIlroy responded to. The stereotype of a Unix wizard is someone who could improvise programs like the one above. Maybe McIlroy carefully thought about his program for days, looking for the most elegant solution. That would seem plausible,but in fact he says the script was written on the spot and worked on the first try. He said that the script was similar to one he had written a year before, but it still counts as an improvisation. Why cant I write scripts like that? McIlroys script was a real example of the kind of wizardry attributed to Unix adepts. Why cant more people quickly improvise scripts like that? The exercise that Bentley posed was the kind of problem that programmers like McIlroy solved routinely at the time. The tools he piped together were developed precisely for such problems. McIlroy didnt see his solution as extraordinary but said Old UNIX hands know instinctively how to solve this one in a jiffy. The traditional Unix toolbox is full of utilities for text manipulation. Not only are they useful, but they compose well. This composability depends not only on the tools themselves, but also the shell environment they were designed to operate in. (The latter is why some utilities dont work as well when ported to other operating systems, even if the functionality is duplicated.) Bentleys exercise was clearly text-based: given a text file, produce a text file. What about problems that are not text manipulation? The trick to being productive from a command line is to turn problems into text manipulation problems. The output of a shell command is text. Programs are text. Once you get into the necessary mindset, everything is text. This may not be the most efficient approach to a given problem, but its a possible strategy. The hard part on the path to becoming a command line wizard, or any kind of wizard, is thinking about how to apply existing tools to your particular problems. You could memorize McIlroys script and be prepared next time you need to report word frequencies, but applying thespirit of his script to your particular problems takes work. Reading one-liners that other people have developed for their work may be inspiring, or intimidating, but theyre no substitute for thinking hard about your particular work. Repetition You get faster at anything with repetition. Maybe you dont solve any particular kind of problem often enough to be fluent at solving it. If someone can solve a problem by quickly typing a one-liner in a shell, maybe they are clever, or maybe their job is repetitive. Or maybe both: maybe theyve found a way to make semi-repetitive tasks repetitive enough to automate. One way to become more productive is to split semi-repetitive tasks into more creative and more repetitive parts. More command line posts Hipster computing Improving on the Unix shell [1] The odd-looking line break is a quoted newline. [2] Everetts speech contained 13,607 words while Lincolns Gettysburg Address contained 272, a ratio of almost exactly 50 to 1. [3] See Hillel Waynes post Donald Knuth was Framed. Heres an excerpt: Most of the eight pages arent because Knuth is doing LP [literate programming], but because hes Donald Knuth: One page is him setting up the problem (what do we mean by word? What if multiple words share the same frequency?) and one page is just the index. Another page is just about working around specific Pascal issues no modern language has, like how do we read in an integer and how do we identify letters when Pascals character set is poorly defined. Then theres almost four pages of handrolling a hash trie. The eight pages refers to the length of the original publication. I described the paper as 17 pages because that the length in the book where I found it. ",
        "_version_":1718938152540831744},
      {
        "story_id":21196177,
        "story_author":"vinnyglennon",
        "story_descendants":1,
        "story_score":13,
        "story_time":"2019-10-08T19:29:58Z",
        "story_title":"I wrote a command-line Ruby program to manage EC2 instances for me",
        "search":["I wrote a command-line Ruby program to manage EC2 instances for me",
          "Normal",
          "https://www.codewithjason.com/wrote-command-line-ruby-program-manage-ec2-instances/",
          "Why I did this Heroku is great, but not in 100% of cases When I want to quickly deploy a Rails application, my go-to choice is Heroku. Im a big fan of the idea that I can just run heroku create and have a production application online in just a matter of seconds. Unfortunately, Heroku isnt always a desirable option. If Im just messing around, I dont usually want to pay for Heroku features, but I also dont always want my dynos to fall asleep after 30 minutes like on the free tier. (Im aware that there are ways around this but I dont necessarily want to deal with the hassle of all that.) Also, sometimes I want finer control than what Heroku provides. I want to be closer to the metal with the ability to directly manage my EC2 instances, RDS instances, and other AWS services. Sometimes I desire this for cost reasons. Sometimes I just want to learn what I think is the valuable developer skill of knowing how to manage AWS infrastructure. Unfortunately, using AWS by itself isnt very easy. Setting up Rails on bare EC2 is a time-consuming and brain-consuming hassle Getting a Rails app standing up on AWS is pretty hard and time-consuming. Im actually not even going to get into Rails-related stuff in this post because even the small task of getting an EC2 instance up and runningwithout no additional software installed on that instanceis a lot harder than I think it should be, and theres a lot to discuss and improve just inside that step. Just to briefly illustrate what a pain in the ass it is to get an EC2 instance launched and to SSH into it, here are the steps. The steps that follow are the command-line steps. I find the AWS GUI console steps roughly equally painful. 1. Use the AWS CLI create-key-pair command to create a key pair. This step is necessary for later when I want to SSH into my instance. 2. Think of a name for the key pair and save it somewhere. Thinking of a name might seem like a trivially small hurdle, but every tiny bit of mental friction adds up. I dont want to have to think of a name, and I dont want to have to think about where to put the file (even if that means just remembering that I want to put the key in ~/.ssh, which is the most likely case. 3. Use the run-instances command, using an AMI ID (AMI == Amazon Machine Image) and passing in my key name. Now I have to go look up the run-instances (because I sure as hell dont remember it) and, look up my AMI ID, and remember what my key name is. (If you dont know what an AMI ID is, thats what determines whether the instance will be Ubuntu, Amazon Linux, Windows, etc.) 4. Use the describe-instances command to find out the public DNS name of the instance I just launched. This means I either have to search the JSON response of describe-instances for the PublicDnsName entry or apply a filter. Just like with every AWS CLI command, Id have to go look up the exact syntax for this. 5. Run the ssh command, passing in my instances DNS and the path to my key. This step is probably the easiest, although it took me a long time to commit the exact ssh -i syntax to memory. For the record, the command is ssh -i ~/.ssh/my_key.pem ubuntu@mypublicdns.com. Its a small pain in the ass to have to look up the public DNS for my instance again and remember whether my EC2 user is going to be ubuntu or ec2-user (it depends on what AMI I used). My goals for my AWS command-line tool All this fuckery was a big hassle so I decided to write my own command-line tool to manage EC2 instances. I call the tool Exosuit. You can actually try it out yourself by following these instructions. There were four specific capabilities I wanted Exosuit to have. Launch an instance By running bin/exo launch, it should launch an EC2 instance for me. It should assume I want Ubuntu. It should let me know when the instance is ready, and what its instance ID and public DNS are. SSH into an instance I should be able to run bin/exo ssh, get prompted for which instance I want to SSH into, and then get SSHd into that instance. List all running instances I should be able to run bin/exo instances to see all my running instances. It should show the instance ID and public DNS for each. Terminate instances I should be able to run bin/exo terminate which will show me all my instance IDs and allow me to select one or more of them for termination. How I did it Side note: when I first wrote this, I forgot that the AWS SDK for Ruby existed, so I reinvented some wheels. Whoops. After I wrote this I refactored the project to use AWS SDK instead of shell out to AWS CLI. For brevity Ill focus on the bin/exo launch command. Using the AW CLI run-instances command The AWS CLI command for launching an instance looks like this: aws ec2 run-instances \\ --count 1 \\ --image-id ami-05c1fa8df71875112 \\ --instance-type t2.micro \\ --key-name normal-quiet-carrot \\ --profile personal Hopefully most of these flags are self-explanatory. You might wonder where the key name of normal-quiet-carrot came from. When the bin/exo launch command is run, Exosuit asks Is there a file defined at .exosuit/config.yml that contains a key pair name and path? If not, create that file, create a new key pair with a random phrase for a name, and save the name and path to that file. Heres what my .exosuit/config.yml looks like: --- aws_profile_name: personal key_pair: name: normal-quiet-carrot path: \"~/.ssh/normal-quiet-carrot.pem\" The aws_profile_name is something that I imagine most users arent likely to need. I personally happen to have multiple AWS accounts, so its necessary for me to send a --profile flag when using AWS CLI commands so AWS knows which account of mine to use. If a profile isnt specified in .exosuit/config.yml, Exosuit will just leave the --profile flag off and everything will still work fine. Abstracting the run-instances command Once I had coded Exosuit to construct a few different AWS CLI commands (e.g. run-instances, terminate-instances), I noticed that things were getting a little repetitive. Most troubling, I had to always remember to include the --profile flag (just as I would if I were typing all this on the command line manually), and I didnt always remember to do so. In those cases my command would get sent to the wrong account. Thats bad. So I created an abstraction called AWSCommand. Heres what a usage of it looks like: command = AWSCommand.new( :run_instances, count: 1, image_id: IMAGE_ID, instance_type: INSTANCE_TYPE, key_name: key_pair.name ) JSON.parse(command.run) You can probably see the resemblance it bears to the bare run-instances usage. Note the conspicuous absence of the profile flag, which is now automatically included every single time. Listening for launch success One of my least favorite things about manually launching EC2 instances is having to check periodically to see when theyve started running. So I wanted Exosuit to tell me when my EC2 instance was running. I achieved this by writing a loop that hits AWS once per second, checking the state of my new instance each time. module Exosuit def self.launch_instance response = Instance.launch(self.key_pair) instance_id = response['Instances'][0]['InstanceId'] print \"Launching instance #{instance_id}...\" while true sleep(1) print '.' instance = Instance.find(instance_id) if instance && instance.running? puts break end end puts 'Instance is now running' puts \"Public DNS: #{instance.public_dns_name}\" end end You might wonder what Instance.find and instance.running? do. The Instance.find method will run the aws ec2 describe-instances command, parse the JSON response, then grab the relevant JSON data for whatever instance_id I passed to it. The return value is an instance of the Instance class. When an instance of Instance is instantiated, an instance variable gets set (pardon all the instances) with all the JSON data for that instance that was returned by the AWS CLI. The instance.running? method simply looks at that JSON data (which has since been converted to a Ruby hash) and checks to see what the value of ['State']['Name'] is. Heres an abbreviated version of the Instance class for reference. module Exosuit class Instance def initialize(info) @info = info end def state @info['State']['Name'] end def running? state == 'running' end end end (By the way, all the Exosuit code is available on GitHub if youd like to take a look.) Success notification As you can see from the code a couple snippets above, Exosuit lets me know once my instances has entered a running state. At this point I can run bin/exo ssh, bin/exo instances or bin/exo terminate to mess with my instance(s) as I please. Demo video Heres a small sample of Exosuit in action: Try it out yourself If youd like to try out Exosuit, just visit the Getting Started with Exosuit guide. If you think this idea is cool and useful, please let me know by opening a GitHub issue for a feature youd like to see, or tweeting at me, or simply starring the project on GitHub so I can gage interest. I hope you enjoyed this explanation and I look forward to sharing the next steps I take with this project. ",
          "Dude, thank you so much. This looks like exactly what I need for a project I'm on."],
        "story_type":"Normal",
        "url_raw":"https://www.codewithjason.com/wrote-command-line-ruby-program-manage-ec2-instances/",
        "comments.comment_id":[21196575],
        "comments.comment_author":["rpmisms"],
        "comments.comment_descendants":[0],
        "comments.comment_time":["2019-10-08T20:04:40Z"],
        "comments.comment_text":["Dude, thank you so much. This looks like exactly what I need for a project I'm on."],
        "id":"88e52251-23d6-4302-ae88-069a9b6c848b",
        "url_text":"Why I did this Heroku is great, but not in 100% of cases When I want to quickly deploy a Rails application, my go-to choice is Heroku. Im a big fan of the idea that I can just run heroku create and have a production application online in just a matter of seconds. Unfortunately, Heroku isnt always a desirable option. If Im just messing around, I dont usually want to pay for Heroku features, but I also dont always want my dynos to fall asleep after 30 minutes like on the free tier. (Im aware that there are ways around this but I dont necessarily want to deal with the hassle of all that.) Also, sometimes I want finer control than what Heroku provides. I want to be closer to the metal with the ability to directly manage my EC2 instances, RDS instances, and other AWS services. Sometimes I desire this for cost reasons. Sometimes I just want to learn what I think is the valuable developer skill of knowing how to manage AWS infrastructure. Unfortunately, using AWS by itself isnt very easy. Setting up Rails on bare EC2 is a time-consuming and brain-consuming hassle Getting a Rails app standing up on AWS is pretty hard and time-consuming. Im actually not even going to get into Rails-related stuff in this post because even the small task of getting an EC2 instance up and runningwithout no additional software installed on that instanceis a lot harder than I think it should be, and theres a lot to discuss and improve just inside that step. Just to briefly illustrate what a pain in the ass it is to get an EC2 instance launched and to SSH into it, here are the steps. The steps that follow are the command-line steps. I find the AWS GUI console steps roughly equally painful. 1. Use the AWS CLI create-key-pair command to create a key pair. This step is necessary for later when I want to SSH into my instance. 2. Think of a name for the key pair and save it somewhere. Thinking of a name might seem like a trivially small hurdle, but every tiny bit of mental friction adds up. I dont want to have to think of a name, and I dont want to have to think about where to put the file (even if that means just remembering that I want to put the key in ~/.ssh, which is the most likely case. 3. Use the run-instances command, using an AMI ID (AMI == Amazon Machine Image) and passing in my key name. Now I have to go look up the run-instances (because I sure as hell dont remember it) and, look up my AMI ID, and remember what my key name is. (If you dont know what an AMI ID is, thats what determines whether the instance will be Ubuntu, Amazon Linux, Windows, etc.) 4. Use the describe-instances command to find out the public DNS name of the instance I just launched. This means I either have to search the JSON response of describe-instances for the PublicDnsName entry or apply a filter. Just like with every AWS CLI command, Id have to go look up the exact syntax for this. 5. Run the ssh command, passing in my instances DNS and the path to my key. This step is probably the easiest, although it took me a long time to commit the exact ssh -i syntax to memory. For the record, the command is ssh -i ~/.ssh/my_key.pem ubuntu@mypublicdns.com. Its a small pain in the ass to have to look up the public DNS for my instance again and remember whether my EC2 user is going to be ubuntu or ec2-user (it depends on what AMI I used). My goals for my AWS command-line tool All this fuckery was a big hassle so I decided to write my own command-line tool to manage EC2 instances. I call the tool Exosuit. You can actually try it out yourself by following these instructions. There were four specific capabilities I wanted Exosuit to have. Launch an instance By running bin/exo launch, it should launch an EC2 instance for me. It should assume I want Ubuntu. It should let me know when the instance is ready, and what its instance ID and public DNS are. SSH into an instance I should be able to run bin/exo ssh, get prompted for which instance I want to SSH into, and then get SSHd into that instance. List all running instances I should be able to run bin/exo instances to see all my running instances. It should show the instance ID and public DNS for each. Terminate instances I should be able to run bin/exo terminate which will show me all my instance IDs and allow me to select one or more of them for termination. How I did it Side note: when I first wrote this, I forgot that the AWS SDK for Ruby existed, so I reinvented some wheels. Whoops. After I wrote this I refactored the project to use AWS SDK instead of shell out to AWS CLI. For brevity Ill focus on the bin/exo launch command. Using the AW CLI run-instances command The AWS CLI command for launching an instance looks like this: aws ec2 run-instances \\ --count 1 \\ --image-id ami-05c1fa8df71875112 \\ --instance-type t2.micro \\ --key-name normal-quiet-carrot \\ --profile personal Hopefully most of these flags are self-explanatory. You might wonder where the key name of normal-quiet-carrot came from. When the bin/exo launch command is run, Exosuit asks Is there a file defined at .exosuit/config.yml that contains a key pair name and path? If not, create that file, create a new key pair with a random phrase for a name, and save the name and path to that file. Heres what my .exosuit/config.yml looks like: --- aws_profile_name: personal key_pair: name: normal-quiet-carrot path: \"~/.ssh/normal-quiet-carrot.pem\" The aws_profile_name is something that I imagine most users arent likely to need. I personally happen to have multiple AWS accounts, so its necessary for me to send a --profile flag when using AWS CLI commands so AWS knows which account of mine to use. If a profile isnt specified in .exosuit/config.yml, Exosuit will just leave the --profile flag off and everything will still work fine. Abstracting the run-instances command Once I had coded Exosuit to construct a few different AWS CLI commands (e.g. run-instances, terminate-instances), I noticed that things were getting a little repetitive. Most troubling, I had to always remember to include the --profile flag (just as I would if I were typing all this on the command line manually), and I didnt always remember to do so. In those cases my command would get sent to the wrong account. Thats bad. So I created an abstraction called AWSCommand. Heres what a usage of it looks like: command = AWSCommand.new( :run_instances, count: 1, image_id: IMAGE_ID, instance_type: INSTANCE_TYPE, key_name: key_pair.name ) JSON.parse(command.run) You can probably see the resemblance it bears to the bare run-instances usage. Note the conspicuous absence of the profile flag, which is now automatically included every single time. Listening for launch success One of my least favorite things about manually launching EC2 instances is having to check periodically to see when theyve started running. So I wanted Exosuit to tell me when my EC2 instance was running. I achieved this by writing a loop that hits AWS once per second, checking the state of my new instance each time. module Exosuit def self.launch_instance response = Instance.launch(self.key_pair) instance_id = response['Instances'][0]['InstanceId'] print \"Launching instance #{instance_id}...\" while true sleep(1) print '.' instance = Instance.find(instance_id) if instance && instance.running? puts break end end puts 'Instance is now running' puts \"Public DNS: #{instance.public_dns_name}\" end end You might wonder what Instance.find and instance.running? do. The Instance.find method will run the aws ec2 describe-instances command, parse the JSON response, then grab the relevant JSON data for whatever instance_id I passed to it. The return value is an instance of the Instance class. When an instance of Instance is instantiated, an instance variable gets set (pardon all the instances) with all the JSON data for that instance that was returned by the AWS CLI. The instance.running? method simply looks at that JSON data (which has since been converted to a Ruby hash) and checks to see what the value of ['State']['Name'] is. Heres an abbreviated version of the Instance class for reference. module Exosuit class Instance def initialize(info) @info = info end def state @info['State']['Name'] end def running? state == 'running' end end end (By the way, all the Exosuit code is available on GitHub if youd like to take a look.) Success notification As you can see from the code a couple snippets above, Exosuit lets me know once my instances has entered a running state. At this point I can run bin/exo ssh, bin/exo instances or bin/exo terminate to mess with my instance(s) as I please. Demo video Heres a small sample of Exosuit in action: Try it out yourself If youd like to try out Exosuit, just visit the Getting Started with Exosuit guide. If you think this idea is cool and useful, please let me know by opening a GitHub issue for a feature youd like to see, or tweeting at me, or simply starring the project on GitHub so I can gage interest. I hope you enjoyed this explanation and I look forward to sharing the next steps I take with this project. ",
        "_version_":1718938228436762624},
      {
        "story_id":21900715,
        "story_author":"2038AD",
        "story_descendants":70,
        "story_score":129,
        "story_time":"2019-12-28T18:36:58Z",
        "story_title":"Unintuitive JSON Parsing",
        "search":["Unintuitive JSON Parsing",
          "Normal",
          "https://nullprogram.com/blog/2019/12/28/",
          "December 28, 2019 nullprogram.com/blog/2019/12/28/ This article was discussed on Hacker News and on reddit. Despite the goal of JSON being a subset of JavaScript which it failed to achieve (update: this was fixed) parsing JSON is quite unlike parsing a programming language. For invalid inputs, the specific cause of error is often counter-intuitive. Normally this doesnt matter, but I recently ran into a case where it does. Consider this invalid input to a JSON parser: To a human this might be interpreted as an array containing a number. Either the leading zero is ignored, or it indicates octal, as it does in many languages, including JavaScript. In either case the number in the array would be 1. However, JSON does not support leading zeros, neither ignoring them nor supporting octal notation. Heres the railroad diagram for numbers from the JSON specficaiton: Or in regular expression form: -?(0|[1-9][0-9]*)(\\.[0-9]+)?([eE][+-]?[0-9]+)? If a token starts with 0 then it can only be followed by ., e, or E. It cannot be followed by a digit. So, the natural human response to mentally parsing [01] is: This input is invalid because it contains a number with a leading zero, and leading zeros are not accepted. But this is not actually why parsing fails! A simple model for the parser is as consuming tokens from a lexer. The lexers job is to read individual code points (characters) from the input and group them into tokens. The possible tokens are string, number, left brace, right brace, left bracket, right bracket, comma, true, false, and null. The lexer skips over insignificant whitespace, and it doesnt care about structure, like matching braces and brackets. Thats the parsers job. In some instances the lexer can fail to parse a token. For example, if while looking for a new token the lexer reads the character %, then the input must be invalid. No token starts with this character. So in some cases invalid input will be detected by the lexer. The parser consumes tokens from the lexer and, using some state, ensures the sequence of tokens is valid. For example, arrays must be a well formed sequence of left bracket, value, comma, value, comma, etc., right bracket. One way to reject input with trailing garbage, is for the lexer to also produce an EOF (end of file/input) token when there are no more tokens, and the parser could specifically check for that token before accepting the input as valid. Getting back to the input [01], a JSON parser receives a left bracket token, then updates its bookkeeping to track that its parsing an array. When looking for the next token, the lexer sees the character 0 followed by 1. According to the railroad diagram, this is a number token (starts with 0), but 1 cannot be part of this token, so it produces a number token with the contents 0. Everything is still fine. Next the lexer sees 1 followed by ]. Since ] cannot be part of a number, it produces another number token with the contents 1. The parser receives this token but, since its parsing an array, it expects either a comma token or a right bracket. Since this is neither, the parser fails with an error about an unexpected number. The parser will not complain about leading zeros because JSON has no concept of leading zeros. Human intuition is right, but for the wrong reasons. Try this for yourself in your favorite JSON parser. Or even just pop up the JavaScript console in your browser and try it out: Firefox reports: SyntaxError: JSON.parse: expected , or ] after array element Chromium reports: SyntaxError: Unexpected number in JSON Edge reports (note it says number not digit): Error: Invalid number at position:3 In all cases the parsers accepted a zero as the first array element, then rejected the input after the second number token for being a bad sequence of tokens. In other words, this is a parser error rather than a lexer error, as a human might intuit. My JSON parser comes with a testing tool that shows the token stream up until the parser rejects the input, useful for understanding these situations: $ echo '[01]' | tests/stream struct expect seq[] = { {JSON_ARRAY}, {JSON_NUMBER, \"0\"}, {JSON_ERROR}, }; Theres an argument to be made here that perhaps the human readable error message should mention leading zeros, since thats likely the cause of the invalid input. That is, a human probably thought JSON allowed leading zeros, and so the clearer message would tell the human that JSON does not allow leading zeros. This is the more art than science part of parsing. Its the same story with this invalid input: From this input, the lexer unambiguously produces left bracket, true, false, right bracket. Its still up to the parser to reject this input. The only reason we never see truefalse in valid JSON is that the overall structure never allows these tokens to be adjacent, not because theyd be ambiguous. Programming languages have identifiers, and in a programming language this would parse as the identifier truefalse rather than true followed by false. From this point of view, JSON seems quite strange. Just as before, Firefox reports: SyntaxError: JSON.parse: expected , or ] after array element Chromium reports the same error as it does for [true false]: SyntaxError: Unexpected token f in JSON Edges message is probably a minor bug in their JSON parser: Error: Expected ] at position:10 Position 10 is the last character in false. The lexer consumed false from the input, produced a false token, then the parser rejected the input. When it reported the error, it chose the end of the invalid token as the error position rather than the start, despite the fact that the only two valid tokens (comma, right bracket) are both a single character. It should also say Expected ] or , (as Firefox does) rather than just ]. Concatenated JSON Thats all pretty academic. Except for producing nice error messages, nobody really cares so much why the input was rejected. The mismatch between intuition and reality isnt important. However, it does come up with concatenated JSON. Some parsers, including mine, will optionally consume multiple JSON values, one after another, from the same input. Heres an example from one of my favorite command line tools, jq: echo '{\"x\":0,\"y\":1}{\"x\":2,\"y\":3}{\"x\":4,\"y\":5}' | jq '.x + .y' 1 5 9 The input contains three unambiguously-concatenated JSON objects, so the parser produces three distinct objects. Now consider this input, this time outside of the context of an array: Is this invalid, one number, or two numbers? According to the lexer and parser model described above, this is valid and unambiguously two concatenated numbers. Heres what my parser says: $ echo '01' | tests/stream struct expect seq[] = { {JSON_NUMBER, \"0\"}, {JSON_DONE}, {JSON_NUMBER, \"1\"}, {JSON_DONE}, {JSON_ERROR}, }; Note: The JSON_DONE token indicates acceptance, and the JSON_ERROR token is an EOF indicator, not a hard error. Since jq allows leading zeros in its JSON input, its ambiguous and parses this as the number 1, so asking its opinion on this input isnt so interesting. I surveyed some other JSON parsers that accept concatenated JSON: Jackson: Reject as leading zero. Noggit: Reject as leading zero. yajl: Accept as two numbers. For my parser its the same story for truefalse: echo 'truefalse' | tests/stream struct expect seq[] = { {JSON_TRUE, \"true\"}, {JSON_DONE}, {JSON_FALSE, \"false\"}, {JSON_DONE}, {JSON_ERROR}, }; Neither rejecting nor accepting this input is wrong, per se. Concatenated JSON is outside of the scope of JSON itself, and concatenating arbitrary JSON objects without a whitespace delimiter can lead to weird and ill-formed input. This is all a great argument in favor or Newline Delimited JSON, and its two simple rules: Line separator is '\\n' Each line is a valid JSON value This solves the concatenation issue, and, even more, it works well with parsers not supporting concatenation: Split the input on newlines and pass each line to your JSON parser. ",
          "> The parser will not complain about leading zeros because JSON has no concept of leading zeros.<p>Of course there is no logical reason why the parser shouldn't have this concept just because the spec doesn't require. IMO, beyond basic correctness, user friendly error messages are the main differentiator between excellent parsers and crappy parsers.",
          "In cases like this, the parser and lexer can often produce a better error message if they are written to accept a more lax input and then check it for errors.<p>For example, instead of faithfully implementing the grammar from the specification, allow numbers with leading zeroes and then produce an error for them.<p>Another situation where this comes up is parsing language keywords. Instead of writing a separate lexer rule for every keyword, write a single rule for keyword-or-identifier, and then use a hash table lookup inside of that to determine if it is a keyword or identifier."],
        "story_type":"Normal",
        "url_raw":"https://nullprogram.com/blog/2019/12/28/",
        "comments.comment_id":[21901262,
          21901416],
        "comments.comment_author":["nicoburns",
          "ufo"],
        "comments.comment_descendants":[6,
          2],
        "comments.comment_time":["2019-12-28T19:49:51Z",
          "2019-12-28T20:10:12Z"],
        "comments.comment_text":["> The parser will not complain about leading zeros because JSON has no concept of leading zeros.<p>Of course there is no logical reason why the parser shouldn't have this concept just because the spec doesn't require. IMO, beyond basic correctness, user friendly error messages are the main differentiator between excellent parsers and crappy parsers.",
          "In cases like this, the parser and lexer can often produce a better error message if they are written to accept a more lax input and then check it for errors.<p>For example, instead of faithfully implementing the grammar from the specification, allow numbers with leading zeroes and then produce an error for them.<p>Another situation where this comes up is parsing language keywords. Instead of writing a separate lexer rule for every keyword, write a single rule for keyword-or-identifier, and then use a hash table lookup inside of that to determine if it is a keyword or identifier."],
        "id":"5fe89d2d-3f90-4ab9-b20a-2397acdea3d8",
        "url_text":"December 28, 2019 nullprogram.com/blog/2019/12/28/ This article was discussed on Hacker News and on reddit. Despite the goal of JSON being a subset of JavaScript which it failed to achieve (update: this was fixed) parsing JSON is quite unlike parsing a programming language. For invalid inputs, the specific cause of error is often counter-intuitive. Normally this doesnt matter, but I recently ran into a case where it does. Consider this invalid input to a JSON parser: To a human this might be interpreted as an array containing a number. Either the leading zero is ignored, or it indicates octal, as it does in many languages, including JavaScript. In either case the number in the array would be 1. However, JSON does not support leading zeros, neither ignoring them nor supporting octal notation. Heres the railroad diagram for numbers from the JSON specficaiton: Or in regular expression form: -?(0|[1-9][0-9]*)(\\.[0-9]+)?([eE][+-]?[0-9]+)? If a token starts with 0 then it can only be followed by ., e, or E. It cannot be followed by a digit. So, the natural human response to mentally parsing [01] is: This input is invalid because it contains a number with a leading zero, and leading zeros are not accepted. But this is not actually why parsing fails! A simple model for the parser is as consuming tokens from a lexer. The lexers job is to read individual code points (characters) from the input and group them into tokens. The possible tokens are string, number, left brace, right brace, left bracket, right bracket, comma, true, false, and null. The lexer skips over insignificant whitespace, and it doesnt care about structure, like matching braces and brackets. Thats the parsers job. In some instances the lexer can fail to parse a token. For example, if while looking for a new token the lexer reads the character %, then the input must be invalid. No token starts with this character. So in some cases invalid input will be detected by the lexer. The parser consumes tokens from the lexer and, using some state, ensures the sequence of tokens is valid. For example, arrays must be a well formed sequence of left bracket, value, comma, value, comma, etc., right bracket. One way to reject input with trailing garbage, is for the lexer to also produce an EOF (end of file/input) token when there are no more tokens, and the parser could specifically check for that token before accepting the input as valid. Getting back to the input [01], a JSON parser receives a left bracket token, then updates its bookkeeping to track that its parsing an array. When looking for the next token, the lexer sees the character 0 followed by 1. According to the railroad diagram, this is a number token (starts with 0), but 1 cannot be part of this token, so it produces a number token with the contents 0. Everything is still fine. Next the lexer sees 1 followed by ]. Since ] cannot be part of a number, it produces another number token with the contents 1. The parser receives this token but, since its parsing an array, it expects either a comma token or a right bracket. Since this is neither, the parser fails with an error about an unexpected number. The parser will not complain about leading zeros because JSON has no concept of leading zeros. Human intuition is right, but for the wrong reasons. Try this for yourself in your favorite JSON parser. Or even just pop up the JavaScript console in your browser and try it out: Firefox reports: SyntaxError: JSON.parse: expected , or ] after array element Chromium reports: SyntaxError: Unexpected number in JSON Edge reports (note it says number not digit): Error: Invalid number at position:3 In all cases the parsers accepted a zero as the first array element, then rejected the input after the second number token for being a bad sequence of tokens. In other words, this is a parser error rather than a lexer error, as a human might intuit. My JSON parser comes with a testing tool that shows the token stream up until the parser rejects the input, useful for understanding these situations: $ echo '[01]' | tests/stream struct expect seq[] = { {JSON_ARRAY}, {JSON_NUMBER, \"0\"}, {JSON_ERROR}, }; Theres an argument to be made here that perhaps the human readable error message should mention leading zeros, since thats likely the cause of the invalid input. That is, a human probably thought JSON allowed leading zeros, and so the clearer message would tell the human that JSON does not allow leading zeros. This is the more art than science part of parsing. Its the same story with this invalid input: From this input, the lexer unambiguously produces left bracket, true, false, right bracket. Its still up to the parser to reject this input. The only reason we never see truefalse in valid JSON is that the overall structure never allows these tokens to be adjacent, not because theyd be ambiguous. Programming languages have identifiers, and in a programming language this would parse as the identifier truefalse rather than true followed by false. From this point of view, JSON seems quite strange. Just as before, Firefox reports: SyntaxError: JSON.parse: expected , or ] after array element Chromium reports the same error as it does for [true false]: SyntaxError: Unexpected token f in JSON Edges message is probably a minor bug in their JSON parser: Error: Expected ] at position:10 Position 10 is the last character in false. The lexer consumed false from the input, produced a false token, then the parser rejected the input. When it reported the error, it chose the end of the invalid token as the error position rather than the start, despite the fact that the only two valid tokens (comma, right bracket) are both a single character. It should also say Expected ] or , (as Firefox does) rather than just ]. Concatenated JSON Thats all pretty academic. Except for producing nice error messages, nobody really cares so much why the input was rejected. The mismatch between intuition and reality isnt important. However, it does come up with concatenated JSON. Some parsers, including mine, will optionally consume multiple JSON values, one after another, from the same input. Heres an example from one of my favorite command line tools, jq: echo '{\"x\":0,\"y\":1}{\"x\":2,\"y\":3}{\"x\":4,\"y\":5}' | jq '.x + .y' 1 5 9 The input contains three unambiguously-concatenated JSON objects, so the parser produces three distinct objects. Now consider this input, this time outside of the context of an array: Is this invalid, one number, or two numbers? According to the lexer and parser model described above, this is valid and unambiguously two concatenated numbers. Heres what my parser says: $ echo '01' | tests/stream struct expect seq[] = { {JSON_NUMBER, \"0\"}, {JSON_DONE}, {JSON_NUMBER, \"1\"}, {JSON_DONE}, {JSON_ERROR}, }; Note: The JSON_DONE token indicates acceptance, and the JSON_ERROR token is an EOF indicator, not a hard error. Since jq allows leading zeros in its JSON input, its ambiguous and parses this as the number 1, so asking its opinion on this input isnt so interesting. I surveyed some other JSON parsers that accept concatenated JSON: Jackson: Reject as leading zero. Noggit: Reject as leading zero. yajl: Accept as two numbers. For my parser its the same story for truefalse: echo 'truefalse' | tests/stream struct expect seq[] = { {JSON_TRUE, \"true\"}, {JSON_DONE}, {JSON_FALSE, \"false\"}, {JSON_DONE}, {JSON_ERROR}, }; Neither rejecting nor accepting this input is wrong, per se. Concatenated JSON is outside of the scope of JSON itself, and concatenating arbitrary JSON objects without a whitespace delimiter can lead to weird and ill-formed input. This is all a great argument in favor or Newline Delimited JSON, and its two simple rules: Line separator is '\\n' Each line is a valid JSON value This solves the concatenation issue, and, even more, it works well with parsers not supporting concatenation: Split the input on newlines and pass each line to your JSON parser. ",
        "_version_":1718938256012214272},
      {
        "story_id":21105895,
        "story_author":"gballan",
        "story_descendants":121,
        "story_score":189,
        "story_time":"2019-09-29T08:35:52Z",
        "story_title":"JSON for Modern C++",
        "search":["JSON for Modern C++",
          "Normal",
          "https://github.com/nlohmann/json",
          "Design goals Sponsors Support (documentation, FAQ, discussions, API, bug issues) Examples JSON as first-class data type Serialization / Deserialization STL-like access Conversion from STL containers JSON Pointer and JSON Patch JSON Merge Patch Implicit conversions Conversions to/from arbitrary types Specializing enum conversion Binary formats (BSON, CBOR, MessagePack, and UBJSON) Supported compilers Integration CMake Package Managers Pkg-config License Contact Thanks Used third-party tools Projects using JSON for Modern C++ Notes Execute unit tests Design goals There are myriads of JSON libraries out there, and each may even have its reason to exist. Our class had these design goals: Intuitive syntax. In languages such as Python, JSON feels like a first class data type. We used all the operator magic of modern C++ to achieve the same feeling in your code. Check out the examples below and you'll know what I mean. Trivial integration. Our whole code consists of a single header file json.hpp. That's it. No library, no subproject, no dependencies, no complex build system. The class is written in vanilla C++11. All in all, everything should require no adjustment of your compiler flags or project settings. Serious testing. Our class is heavily unit-tested and covers 100% of the code, including all exceptional behavior. Furthermore, we checked with Valgrind and the Clang Sanitizers that there are no memory leaks. Google OSS-Fuzz additionally runs fuzz tests against all parsers 24/7, effectively executing billions of tests so far. To maintain high quality, the project is following the Core Infrastructure Initiative (CII) best practices. Other aspects were not so important to us: Memory efficiency. Each JSON object has an overhead of one pointer (the maximal size of a union) and one enumeration element (1 byte). The default generalization uses the following C++ data types: std::string for strings, int64_t, uint64_t or double for numbers, std::map for objects, std::vector for arrays, and bool for Booleans. However, you can template the generalized class basic_json to your needs. Speed. There are certainly faster JSON libraries out there. However, if your goal is to speed up your development by adding JSON support with a single header, then this library is the way to go. If you know how to use a std::vector or std::map, you are already set. See the contribution guidelines for more information. Sponsors You can sponsor this library at GitHub Sponsors. Named Sponsors Michael Hartmann Stefan Hagen Steve Sperandeo Robert Jefe Lindstdt Steve Wagner Thanks everyone! Support If you have a question, please check if it is already answered in the FAQ or the Q&A section. If not, please ask a new question there. If you want to learn more about how to use the library, check out the rest of the README, have a look at code examples, or browse through the help pages. If you want to understand the API better, check out the API Reference or the Doxygen documentation. If you found a bug, please check the FAQ if it is a known issue or the result of a design decision. Please also have a look at the issue list before you create a new issue. Please provide as many information as possible to help us understand and reproduce your issue. There is also a docset for the documentation browsers Dash, Velocity, and Zeal that contains the full documentation as offline resource. Examples Beside the examples below, you may want to check the documentation where each function contains a separate code example (e.g., check out emplace()). All example files can be compiled and executed on their own (e.g., file emplace.cpp). JSON as first-class data type Here are some examples to give you an idea how to use the class. Assume you want to create the JSON object { \"pi\": 3.141, \"happy\": true, \"name\": \"Niels\", \"nothing\": null, \"answer\": { \"everything\": 42 }, \"list\": [1, 0, 2], \"object\": { \"currency\": \"USD\", \"value\": 42.99 } } With this library, you could write: // create an empty structure (null) json j; // add a number that is stored as double (note the implicit conversion of j to an object) j[\"pi\"] = 3.141; // add a Boolean that is stored as bool j[\"happy\"] = true; // add a string that is stored as std::string j[\"name\"] = \"Niels\"; // add another null object by passing nullptr j[\"nothing\"] = nullptr; // add an object inside the object j[\"answer\"][\"everything\"] = 42; // add an array that is stored as std::vector (using an initializer list) j[\"list\"] = { 1, 0, 2 }; // add another object (using an initializer list of pairs) j[\"object\"] = { {\"currency\", \"USD\"}, {\"value\", 42.99} }; // instead, you could also write (which looks very similar to the JSON above) json j2 = { {\"pi\", 3.141}, {\"happy\", true}, {\"name\", \"Niels\"}, {\"nothing\", nullptr}, {\"answer\", { {\"everything\", 42} }}, {\"list\", {1, 0, 2}}, {\"object\", { {\"currency\", \"USD\"}, {\"value\", 42.99} }} }; Note that in all these cases, you never need to \"tell\" the compiler which JSON value type you want to use. If you want to be explicit or express some edge cases, the functions json::array() and json::object() will help: // a way to express the empty array [] json empty_array_explicit = json::array(); // ways to express the empty object {} json empty_object_implicit = json({}); json empty_object_explicit = json::object(); // a way to express an _array_ of key/value pairs [[\"currency\", \"USD\"], [\"value\", 42.99]] json array_not_object = json::array({ {\"currency\", \"USD\"}, {\"value\", 42.99} }); Serialization / Deserialization To/from strings You can create a JSON value (deserialization) by appending _json to a string literal: // create object from string literal json j = \"{ \\\"happy\\\": true, \\\"pi\\\": 3.141 }\"_json; // or even nicer with a raw string literal auto j2 = R\"( { \"happy\": true, \"pi\": 3.141 } )\"_json; Note that without appending the _json suffix, the passed string literal is not parsed, but just used as JSON string value. That is, json j = \"{ \\\"happy\\\": true, \\\"pi\\\": 3.141 }\" would just store the string \"{ \"happy\": true, \"pi\": 3.141 }\" rather than parsing the actual object. The above example can also be expressed explicitly using json::parse(): // parse explicitly auto j3 = json::parse(R\"({\"happy\": true, \"pi\": 3.141})\"); You can also get a string representation of a JSON value (serialize): // explicit conversion to string std::string s = j.dump(); // {\"happy\":true,\"pi\":3.141} // serialization with pretty printing // pass in the amount of spaces to indent std::cout << j.dump(4) << std::endl; // { // \"happy\": true, // \"pi\": 3.141 // } Note the difference between serialization and assignment: // store a string in a JSON value json j_string = \"this is a string\"; // retrieve the string value auto cpp_string = j_string.get<std::string>(); // retrieve the string value (alternative when an variable already exists) std::string cpp_string2; j_string.get_to(cpp_string2); // retrieve the serialized value (explicit JSON serialization) std::string serialized_string = j_string.dump(); // output of original string std::cout << cpp_string << \" == \" << cpp_string2 << \" == \" << j_string.get<std::string>() << '\\n'; // output of serialized value std::cout << j_string << \" == \" << serialized_string << std::endl; .dump() returns the originally stored string value. Note the library only supports UTF-8. When you store strings with different encodings in the library, calling dump() may throw an exception unless json::error_handler_t::replace or json::error_handler_t::ignore are used as error handlers. To/from streams (e.g. files, string streams) You can also use streams to serialize and deserialize: // deserialize from standard input json j; std::cin >> j; // serialize to standard output std::cout << j; // the setw manipulator was overloaded to set the indentation for pretty printing std::cout << std::setw(4) << j << std::endl; These operators work for any subclasses of std::istream or std::ostream. Here is the same example with files: // read a JSON file std::ifstream i(\"file.json\"); json j; i >> j; // write prettified JSON to another file std::ofstream o(\"pretty.json\"); o << std::setw(4) << j << std::endl; Please note that setting the exception bit for failbit is inappropriate for this use case. It will result in program termination due to the noexcept specifier in use. Read from iterator range You can also parse JSON from an iterator range; that is, from any container accessible by iterators whose value_type is an integral type of 1, 2 or 4 bytes, which will be interpreted as UTF-8, UTF-16 and UTF-32 respectively. For instance, a std::vector<std::uint8_t>, or a std::list<std::uint16_t>: std::vector<std::uint8_t> v = {'t', 'r', 'u', 'e'}; json j = json::parse(v.begin(), v.end()); You may leave the iterators for the range [begin, end): std::vector<std::uint8_t> v = {'t', 'r', 'u', 'e'}; json j = json::parse(v); Custom data source Since the parse function accepts arbitrary iterator ranges, you can provide your own data sources by implementing the LegacyInputIterator concept. struct MyContainer { void advance(); const char& get_current(); }; struct MyIterator { using difference_type = std::ptrdiff_t; using value_type = char; using pointer = const char*; using reference = const char&; using iterator_category = std::input_iterator_tag; MyIterator& operator++() { MyContainer.advance(); return *this; } bool operator!=(const MyIterator& rhs) const { return rhs.target != target; } reference operator*() const { return target.get_current(); } MyContainer* target = nullptr; }; MyIterator begin(MyContainer& tgt) { return MyIterator{&tgt}; } MyIterator end(const MyContainer&) { return {}; } void foo() { MyContainer c; json j = json::parse(c); } SAX interface The library uses a SAX-like interface with the following functions: // called when null is parsed bool null(); // called when a boolean is parsed; value is passed bool boolean(bool val); // called when a signed or unsigned integer number is parsed; value is passed bool number_integer(number_integer_t val); bool number_unsigned(number_unsigned_t val); // called when a floating-point number is parsed; value and original string is passed bool number_float(number_float_t val, const string_t& s); // called when a string is parsed; value is passed and can be safely moved away bool string(string_t& val); // called when a binary value is parsed; value is passed and can be safely moved away bool binary(binary_t& val); // called when an object or array begins or ends, resp. The number of elements is passed (or -1 if not known) bool start_object(std::size_t elements); bool end_object(); bool start_array(std::size_t elements); bool end_array(); // called when an object key is parsed; value is passed and can be safely moved away bool key(string_t& val); // called when a parse error occurs; byte position, the last token, and an exception is passed bool parse_error(std::size_t position, const std::string& last_token, const detail::exception& ex); The return value of each function determines whether parsing should proceed. To implement your own SAX handler, proceed as follows: Implement the SAX interface in a class. You can use class nlohmann::json_sax<json> as base class, but you can also use any class where the functions described above are implemented and public. Create an object of your SAX interface class, e.g. my_sax. Call bool json::sax_parse(input, &my_sax); where the first parameter can be any input like a string or an input stream and the second parameter is a pointer to your SAX interface. Note the sax_parse function only returns a bool indicating the result of the last executed SAX event. It does not return a json value - it is up to you to decide what to do with the SAX events. Furthermore, no exceptions are thrown in case of a parse error - it is up to you what to do with the exception object passed to your parse_error implementation. Internally, the SAX interface is used for the DOM parser (class json_sax_dom_parser) as well as the acceptor (json_sax_acceptor), see file json_sax.hpp. STL-like access We designed the JSON class to behave just like an STL container. In fact, it satisfies the ReversibleContainer requirement. // create an array using push_back json j; j.push_back(\"foo\"); j.push_back(1); j.push_back(true); // also use emplace_back j.emplace_back(1.78); // iterate the array for (json::iterator it = j.begin(); it != j.end(); ++it) { std::cout << *it << '\\n'; } // range-based for for (auto& element : j) { std::cout << element << '\\n'; } // getter/setter const auto tmp = j[0].get<std::string>(); j[1] = 42; bool foo = j.at(2); // comparison j == R\"([\"foo\", 1, true, 1.78])\"_json; // true // other stuff j.size(); // 4 entries j.empty(); // false j.type(); // json::value_t::array j.clear(); // the array is empty again // convenience type checkers j.is_null(); j.is_boolean(); j.is_number(); j.is_object(); j.is_array(); j.is_string(); // create an object json o; o[\"foo\"] = 23; o[\"bar\"] = false; o[\"baz\"] = 3.141; // also use emplace o.emplace(\"weather\", \"sunny\"); // special iterator member functions for objects for (json::iterator it = o.begin(); it != o.end(); ++it) { std::cout << it.key() << \" : \" << it.value() << \"\\n\"; } // the same code as range for for (auto& el : o.items()) { std::cout << el.key() << \" : \" << el.value() << \"\\n\"; } // even easier with structured bindings (C++17) for (auto& [key, value] : o.items()) { std::cout << key << \" : \" << value << \"\\n\"; } // find an entry if (o.contains(\"foo\")) { // there is an entry with key \"foo\" } // or via find and an iterator if (o.find(\"foo\") != o.end()) { // there is an entry with key \"foo\" } // or simpler using count() int foo_present = o.count(\"foo\"); // 1 int fob_present = o.count(\"fob\"); // 0 // delete an entry o.erase(\"foo\"); Conversion from STL containers Any sequence container (std::array, std::vector, std::deque, std::forward_list, std::list) whose values can be used to construct JSON values (e.g., integers, floating point numbers, Booleans, string types, or again STL containers described in this section) can be used to create a JSON array. The same holds for similar associative containers (std::set, std::multiset, std::unordered_set, std::unordered_multiset), but in these cases the order of the elements of the array depends on how the elements are ordered in the respective STL container. std::vector<int> c_vector {1, 2, 3, 4}; json j_vec(c_vector); // [1, 2, 3, 4] std::deque<double> c_deque {1.2, 2.3, 3.4, 5.6}; json j_deque(c_deque); // [1.2, 2.3, 3.4, 5.6] std::list<bool> c_list {true, true, false, true}; json j_list(c_list); // [true, true, false, true] std::forward_list<int64_t> c_flist {12345678909876, 23456789098765, 34567890987654, 45678909876543}; json j_flist(c_flist); // [12345678909876, 23456789098765, 34567890987654, 45678909876543] std::array<unsigned long, 4> c_array {{1, 2, 3, 4}}; json j_array(c_array); // [1, 2, 3, 4] std::set<std::string> c_set {\"one\", \"two\", \"three\", \"four\", \"one\"}; json j_set(c_set); // only one entry for \"one\" is used // [\"four\", \"one\", \"three\", \"two\"] std::unordered_set<std::string> c_uset {\"one\", \"two\", \"three\", \"four\", \"one\"}; json j_uset(c_uset); // only one entry for \"one\" is used // maybe [\"two\", \"three\", \"four\", \"one\"] std::multiset<std::string> c_mset {\"one\", \"two\", \"one\", \"four\"}; json j_mset(c_mset); // both entries for \"one\" are used // maybe [\"one\", \"two\", \"one\", \"four\"] std::unordered_multiset<std::string> c_umset {\"one\", \"two\", \"one\", \"four\"}; json j_umset(c_umset); // both entries for \"one\" are used // maybe [\"one\", \"two\", \"one\", \"four\"] Likewise, any associative key-value containers (std::map, std::multimap, std::unordered_map, std::unordered_multimap) whose keys can construct an std::string and whose values can be used to construct JSON values (see examples above) can be used to create a JSON object. Note that in case of multimaps only one key is used in the JSON object and the value depends on the internal order of the STL container. std::map<std::string, int> c_map { {\"one\", 1}, {\"two\", 2}, {\"three\", 3} }; json j_map(c_map); // {\"one\": 1, \"three\": 3, \"two\": 2 } std::unordered_map<const char*, double> c_umap { {\"one\", 1.2}, {\"two\", 2.3}, {\"three\", 3.4} }; json j_umap(c_umap); // {\"one\": 1.2, \"two\": 2.3, \"three\": 3.4} std::multimap<std::string, bool> c_mmap { {\"one\", true}, {\"two\", true}, {\"three\", false}, {\"three\", true} }; json j_mmap(c_mmap); // only one entry for key \"three\" is used // maybe {\"one\": true, \"two\": true, \"three\": true} std::unordered_multimap<std::string, bool> c_ummap { {\"one\", true}, {\"two\", true}, {\"three\", false}, {\"three\", true} }; json j_ummap(c_ummap); // only one entry for key \"three\" is used // maybe {\"one\": true, \"two\": true, \"three\": true} JSON Pointer and JSON Patch The library supports JSON Pointer (RFC 6901) as alternative means to address structured values. On top of this, JSON Patch (RFC 6902) allows to describe differences between two JSON values - effectively allowing patch and diff operations known from Unix. // a JSON value json j_original = R\"({ \"baz\": [\"one\", \"two\", \"three\"], \"foo\": \"bar\" })\"_json; // access members with a JSON pointer (RFC 6901) j_original[\"/baz/1\"_json_pointer]; // \"two\" // a JSON patch (RFC 6902) json j_patch = R\"([ { \"op\": \"replace\", \"path\": \"/baz\", \"value\": \"boo\" }, { \"op\": \"add\", \"path\": \"/hello\", \"value\": [\"world\"] }, { \"op\": \"remove\", \"path\": \"/foo\"} ])\"_json; // apply the patch json j_result = j_original.patch(j_patch); // { // \"baz\": \"boo\", // \"hello\": [\"world\"] // } // calculate a JSON patch from two JSON values json::diff(j_result, j_original); // [ // { \"op\":\" replace\", \"path\": \"/baz\", \"value\": [\"one\", \"two\", \"three\"] }, // { \"op\": \"remove\",\"path\": \"/hello\" }, // { \"op\": \"add\", \"path\": \"/foo\", \"value\": \"bar\" } // ] JSON Merge Patch The library supports JSON Merge Patch (RFC 7386) as a patch format. Instead of using JSON Pointer (see above) to specify values to be manipulated, it describes the changes using a syntax that closely mimics the document being modified. // a JSON value json j_document = R\"({ \"a\": \"b\", \"c\": { \"d\": \"e\", \"f\": \"g\" } })\"_json; // a patch json j_patch = R\"({ \"a\":\"z\", \"c\": { \"f\": null } })\"_json; // apply the patch j_document.merge_patch(j_patch); // { // \"a\": \"z\", // \"c\": { // \"d\": \"e\" // } // } Implicit conversions Supported types can be implicitly converted to JSON values. It is recommended to NOT USE implicit conversions FROM a JSON value. You can find more details about this recommendation here. You can switch off implicit conversions by defining JSON_USE_IMPLICIT_CONVERSIONS to 0 before including the json.hpp header. When using CMake, you can also achieve this by setting the option JSON_ImplicitConversions to OFF. // strings std::string s1 = \"Hello, world!\"; json js = s1; auto s2 = js.get<std::string>(); // NOT RECOMMENDED std::string s3 = js; std::string s4; s4 = js; // Booleans bool b1 = true; json jb = b1; auto b2 = jb.get<bool>(); // NOT RECOMMENDED bool b3 = jb; bool b4; b4 = jb; // numbers int i = 42; json jn = i; auto f = jn.get<double>(); // NOT RECOMMENDED double f2 = jb; double f3; f3 = jb; // etc. Note that char types are not automatically converted to JSON strings, but to integer numbers. A conversion to a string must be specified explicitly: char ch = 'A'; // ASCII value 65 json j_default = ch; // stores integer number 65 json j_string = std::string(1, ch); // stores string \"A\" Arbitrary types conversions Every type can be serialized in JSON, not just STL containers and scalar types. Usually, you would do something along those lines: namespace ns { // a simple struct to model a person struct person { std::string name; std::string address; int age; }; } ns::person p = {\"Ned Flanders\", \"744 Evergreen Terrace\", 60}; // convert to JSON: copy each value into the JSON object json j; j[\"name\"] = p.name; j[\"address\"] = p.address; j[\"age\"] = p.age; // ... // convert from JSON: copy each value from the JSON object ns::person p { j[\"name\"].get<std::string>(), j[\"address\"].get<std::string>(), j[\"age\"].get<int>() }; It works, but that's quite a lot of boilerplate... Fortunately, there's a better way: // create a person ns::person p {\"Ned Flanders\", \"744 Evergreen Terrace\", 60}; // conversion: person -> json json j = p; std::cout << j << std::endl; // {\"address\":\"744 Evergreen Terrace\",\"age\":60,\"name\":\"Ned Flanders\"} // conversion: json -> person auto p2 = j.get<ns::person>(); // that's it assert(p == p2); Basic usage To make this work with one of your types, you only need to provide two functions: using json = nlohmann::json; namespace ns { void to_json(json& j, const person& p) { j = json{{\"name\", p.name}, {\"address\", p.address}, {\"age\", p.age}}; } void from_json(const json& j, person& p) { j.at(\"name\").get_to(p.name); j.at(\"address\").get_to(p.address); j.at(\"age\").get_to(p.age); } } // namespace ns That's all! When calling the json constructor with your type, your custom to_json method will be automatically called. Likewise, when calling get<your_type>() or get_to(your_type&), the from_json method will be called. Some important things: Those methods MUST be in your type's namespace (which can be the global namespace), or the library will not be able to locate them (in this example, they are in namespace ns, where person is defined). Those methods MUST be available (e.g., proper headers must be included) everywhere you use these conversions. Look at issue 1108 for errors that may occur otherwise. When using get<your_type>(), your_type MUST be DefaultConstructible. (There is a way to bypass this requirement described later.) In function from_json, use function at() to access the object values rather than operator[]. In case a key does not exist, at throws an exception that you can handle, whereas operator[] exhibits undefined behavior. You do not need to add serializers or deserializers for STL types like std::vector: the library already implements these. Simplify your life with macros If you just want to serialize/deserialize some structs, the to_json/from_json functions can be a lot of boilerplate. There are two macros to make your life easier as long as you (1) want to use a JSON object as serialization and (2) want to use the member variable names as object keys in that object: NLOHMANN_DEFINE_TYPE_NON_INTRUSIVE(name, member1, member2, ...) is to be defined inside of the namespace of the class/struct to create code for. NLOHMANN_DEFINE_TYPE_INTRUSIVE(name, member1, member2, ...) is to be defined inside of the class/struct to create code for. This macro can also access private members. In both macros, the first parameter is the name of the class/struct, and all remaining parameters name the members. Examples The to_json/from_json functions for the person struct above can be created with: namespace ns { NLOHMANN_DEFINE_TYPE_NON_INTRUSIVE(person, name, address, age) } Here is an example with private members, where NLOHMANN_DEFINE_TYPE_INTRUSIVE is needed: namespace ns { class address { private: std::string street; int housenumber; int postcode; public: NLOHMANN_DEFINE_TYPE_INTRUSIVE(address, street, housenumber, postcode) }; } How do I convert third-party types? This requires a bit more advanced technique. But first, let's see how this conversion mechanism works: The library uses JSON Serializers to convert types to json. The default serializer for nlohmann::json is nlohmann::adl_serializer (ADL means Argument-Dependent Lookup). It is implemented like this (simplified): template <typename T> struct adl_serializer { static void to_json(json& j, const T& value) { // calls the \"to_json\" method in T's namespace } static void from_json(const json& j, T& value) { // same thing, but with the \"from_json\" method } }; This serializer works fine when you have control over the type's namespace. However, what about boost::optional or std::filesystem::path (C++17)? Hijacking the boost namespace is pretty bad, and it's illegal to add something other than template specializations to std... To solve this, you need to add a specialization of adl_serializer to the nlohmann namespace, here's an example: // partial specialization (full specialization works too) namespace nlohmann { template <typename T> struct adl_serializer<boost::optional<T>> { static void to_json(json& j, const boost::optional<T>& opt) { if (opt == boost::none) { j = nullptr; } else { j = *opt; // this will call adl_serializer<T>::to_json which will // find the free function to_json in T's namespace! } } static void from_json(const json& j, boost::optional<T>& opt) { if (j.is_null()) { opt = boost::none; } else { opt = j.get<T>(); // same as above, but with // adl_serializer<T>::from_json } } }; } How can I use get() for non-default constructible/non-copyable types? There is a way, if your type is MoveConstructible. You will need to specialize the adl_serializer as well, but with a special from_json overload: struct move_only_type { move_only_type() = delete; move_only_type(int ii): i(ii) {} move_only_type(const move_only_type&) = delete; move_only_type(move_only_type&&) = default; int i; }; namespace nlohmann { template <> struct adl_serializer<move_only_type> { // note: the return type is no longer 'void', and the method only takes // one argument static move_only_type from_json(const json& j) { return {j.get<int>()}; } // Here's the catch! You must provide a to_json method! Otherwise you // will not be able to convert move_only_type to json, since you fully // specialized adl_serializer on that type static void to_json(json& j, move_only_type t) { j = t.i; } }; } Can I write my own serializer? (Advanced use) Yes. You might want to take a look at unit-udt.cpp in the test suite, to see a few examples. If you write your own serializer, you'll need to do a few things: use a different basic_json alias than nlohmann::json (the last template parameter of basic_json is the JSONSerializer) use your basic_json alias (or a template parameter) in all your to_json/from_json methods use nlohmann::to_json and nlohmann::from_json when you need ADL Here is an example, without simplifications, that only accepts types with a size <= 32, and uses ADL. // You should use void as a second template argument // if you don't need compile-time checks on T template<typename T, typename SFINAE = typename std::enable_if<sizeof(T) <= 32>::type> struct less_than_32_serializer { template <typename BasicJsonType> static void to_json(BasicJsonType& j, T value) { // we want to use ADL, and call the correct to_json overload using nlohmann::to_json; // this method is called by adl_serializer, // this is where the magic happens to_json(j, value); } template <typename BasicJsonType> static void from_json(const BasicJsonType& j, T& value) { // same thing here using nlohmann::from_json; from_json(j, value); } }; Be very careful when reimplementing your serializer, you can stack overflow if you don't pay attention: template <typename T, void> struct bad_serializer { template <typename BasicJsonType> static void to_json(BasicJsonType& j, const T& value) { // this calls BasicJsonType::json_serializer<T>::to_json(j, value); // if BasicJsonType::json_serializer == bad_serializer ... oops! j = value; } template <typename BasicJsonType> static void to_json(const BasicJsonType& j, T& value) { // this calls BasicJsonType::json_serializer<T>::from_json(j, value); // if BasicJsonType::json_serializer == bad_serializer ... oops! value = j.template get<T>(); // oops! } }; Specializing enum conversion By default, enum values are serialized to JSON as integers. In some cases this could result in undesired behavior. If an enum is modified or re-ordered after data has been serialized to JSON, the later de-serialized JSON data may be undefined or a different enum value than was originally intended. It is possible to more precisely specify how a given enum is mapped to and from JSON as shown below: // example enum type declaration enum TaskState { TS_STOPPED, TS_RUNNING, TS_COMPLETED, TS_INVALID=-1, }; // map TaskState values to JSON as strings NLOHMANN_JSON_SERIALIZE_ENUM( TaskState, { {TS_INVALID, nullptr}, {TS_STOPPED, \"stopped\"}, {TS_RUNNING, \"running\"}, {TS_COMPLETED, \"completed\"}, }) The NLOHMANN_JSON_SERIALIZE_ENUM() macro declares a set of to_json() / from_json() functions for type TaskState while avoiding repetition and boilerplate serialization code. Usage: // enum to JSON as string json j = TS_STOPPED; assert(j == \"stopped\"); // json string to enum json j3 = \"running\"; assert(j3.get<TaskState>() == TS_RUNNING); // undefined json value to enum (where the first map entry above is the default) json jPi = 3.14; assert(jPi.get<TaskState>() == TS_INVALID ); Just as in Arbitrary Type Conversions above, NLOHMANN_JSON_SERIALIZE_ENUM() MUST be declared in your enum type's namespace (which can be the global namespace), or the library will not be able to locate it and it will default to integer serialization. It MUST be available (e.g., proper headers must be included) everywhere you use the conversions. Other Important points: When using get<ENUM_TYPE>(), undefined JSON values will default to the first pair specified in your map. Select this default pair carefully. If an enum or JSON value is specified more than once in your map, the first matching occurrence from the top of the map will be returned when converting to or from JSON. Binary formats (BSON, CBOR, MessagePack, and UBJSON) Though JSON is a ubiquitous data format, it is not a very compact format suitable for data exchange, for instance over a network. Hence, the library supportsBSON (Binary JSON), CBOR (Concise Binary Object Representation), MessagePack, and UBJSON (Universal Binary JSON Specification) to efficiently encode JSON values to byte vectors and to decode such vectors. // create a JSON value json j = R\"({\"compact\": true, \"schema\": 0})\"_json; // serialize to BSON std::vector<std::uint8_t> v_bson = json::to_bson(j); // 0x1B, 0x00, 0x00, 0x00, 0x08, 0x63, 0x6F, 0x6D, 0x70, 0x61, 0x63, 0x74, 0x00, 0x01, 0x10, 0x73, 0x63, 0x68, 0x65, 0x6D, 0x61, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00 // roundtrip json j_from_bson = json::from_bson(v_bson); // serialize to CBOR std::vector<std::uint8_t> v_cbor = json::to_cbor(j); // 0xA2, 0x67, 0x63, 0x6F, 0x6D, 0x70, 0x61, 0x63, 0x74, 0xF5, 0x66, 0x73, 0x63, 0x68, 0x65, 0x6D, 0x61, 0x00 // roundtrip json j_from_cbor = json::from_cbor(v_cbor); // serialize to MessagePack std::vector<std::uint8_t> v_msgpack = json::to_msgpack(j); // 0x82, 0xA7, 0x63, 0x6F, 0x6D, 0x70, 0x61, 0x63, 0x74, 0xC3, 0xA6, 0x73, 0x63, 0x68, 0x65, 0x6D, 0x61, 0x00 // roundtrip json j_from_msgpack = json::from_msgpack(v_msgpack); // serialize to UBJSON std::vector<std::uint8_t> v_ubjson = json::to_ubjson(j); // 0x7B, 0x69, 0x07, 0x63, 0x6F, 0x6D, 0x70, 0x61, 0x63, 0x74, 0x54, 0x69, 0x06, 0x73, 0x63, 0x68, 0x65, 0x6D, 0x61, 0x69, 0x00, 0x7D // roundtrip json j_from_ubjson = json::from_ubjson(v_ubjson); The library also supports binary types from BSON, CBOR (byte strings), and MessagePack (bin, ext, fixext). They are stored by default as std::vector<std::uint8_t> to be processed outside of the library. // CBOR byte string with payload 0xCAFE std::vector<std::uint8_t> v = {0x42, 0xCA, 0xFE}; // read value json j = json::from_cbor(v); // the JSON value has type binary j.is_binary(); // true // get reference to stored binary value auto& binary = j.get_binary(); // the binary value has no subtype (CBOR has no binary subtypes) binary.has_subtype(); // false // access std::vector<std::uint8_t> member functions binary.size(); // 2 binary[0]; // 0xCA binary[1]; // 0xFE // set subtype to 0x10 binary.set_subtype(0x10); // serialize to MessagePack auto cbor = json::to_msgpack(j); // 0xD5 (fixext2), 0x10, 0xCA, 0xFE Supported compilers Though it's 2021 already, the support for C++11 is still a bit sparse. Currently, the following compilers are known to work: GCC 4.8 - 11.0 (and possibly later) Clang 3.4 - 13.0 (and possibly later) Apple Clang 9.1 - 12.4 (and possibly later) Intel C++ Compiler 17.0.2 (and possibly later) Microsoft Visual C++ 2015 / Build Tools 14.0.25123.0 (and possibly later) Microsoft Visual C++ 2017 / Build Tools 15.5.180.51428 (and possibly later) Microsoft Visual C++ 2019 / Build Tools 16.3.1+1def00d3d (and possibly later) I would be happy to learn about other compilers/versions. Please note: GCC 4.8 has a bug 57824): multiline raw strings cannot be the arguments to macros. Don't use multiline raw strings directly in macros with this compiler. Android defaults to using very old compilers and C++ libraries. To fix this, add the following to your Application.mk. This will switch to the LLVM C++ library, the Clang compiler, and enable C++11 and other features disabled by default. APP_STL := c++_shared NDK_TOOLCHAIN_VERSION := clang3.6 APP_CPPFLAGS += -frtti -fexceptions The code compiles successfully with Android NDK, Revision 9 - 11 (and possibly later) and CrystaX's Android NDK version 10. For GCC running on MinGW or Android SDK, the error 'to_string' is not a member of 'std' (or similarly, for strtod or strtof) may occur. Note this is not an issue with the code, but rather with the compiler itself. On Android, see above to build with a newer environment. For MinGW, please refer to this site and this discussion for information on how to fix this bug. For Android NDK using APP_STL := gnustl_static, please refer to this discussion. Unsupported versions of GCC and Clang are rejected by #error directives. This can be switched off by defining JSON_SKIP_UNSUPPORTED_COMPILER_CHECK. Note that you can expect no support in this case. The following compilers are currently used in continuous integration at Travis, AppVeyor, Drone CI, and GitHub Actions: Compiler Operating System CI Provider Apple Clang 10.0.1 (clang-1001.0.46.4); Xcode 10.2.1 macOS 10.14.4 Travis Apple Clang 10.0.1 (clang-1001.0.46.4); Xcode 10.3 macOS 10.15.7 GitHub Actions Apple Clang 11.0.0 (clang-1100.0.33.12); Xcode 11.2.1 macOS 10.15.7 GitHub Actions Apple Clang 11.0.0 (clang-1100.0.33.17); Xcode 11.3.1 macOS 10.15.7 GitHub Actions Apple Clang 11.0.3 (clang-1103.0.32.59); Xcode 11.4.1 macOS 10.15.7 GitHub Actions Apple Clang 11.0.3 (clang-1103.0.32.62); Xcode 11.5 macOS 10.15.7 GitHub Actions Apple Clang 11.0.3 (clang-1103.0.32.62); Xcode 11.6 macOS 10.15.7 GitHub Actions Apple Clang 11.0.3 (clang-1103.0.32.62); Xcode 11.7 macOS 10.15.7 GitHub Actions Apple Clang 12.0.0 (clang-1200.0.32.2); Xcode 12 macOS 10.15.7 GitHub Actions Apple Clang 12.0.0 (clang-1200.0.32.21); Xcode 12.1 macOS 10.15.7 GitHub Actions Apple Clang 12.0.0 (clang-1200.0.32.21); Xcode 12.1.1 macOS 10.15.7 GitHub Actions Apple Clang 12.0.0 (clang-1200.0.32.27); Xcode 12.2 macOS 10.15.7 GitHub Actions Apple Clang 12.0.0 (clang-1200.0.32.28); Xcode 12.3 macOS 10.15.7 GitHub Actions Apple Clang 12.0.0 (clang-1200.0.32.29); Xcode 12.4 macOS 10.15.7 GitHub Actions GCC 4.8.5 (Ubuntu 4.8.5-4ubuntu2) Ubuntu 20.04.3 LTS GitHub Actions GCC 4.9.3 (Ubuntu 4.9.3-13ubuntu2) Ubuntu 20.04.3 LTS GitHub Actions GCC 5.4.0 (Ubuntu 5.4.0-6ubuntu1~16.04.12) Ubuntu 20.04.3 LTS GitHub Actions GCC 6.4.0 (Ubuntu 6.4.0-17ubuntu1) Ubuntu 20.04.3 LTS GitHub Actions GCC 6.5.0 (Ubuntu 6.5.0-2ubuntu1~14.04.1) Ubuntu 14.04.5 LTS Travis GCC 7.5.0 (Ubuntu 7.5.0-6ubuntu2) Ubuntu 20.04.3 LTS GitHub Actions GCC 8.1.0 (x86_64-posix-seh-rev0, Built by MinGW-W64 project) Windows-10.0.17763 GitHub Actions GCC 8.1.0 (i686-posix-dwarf-rev0, Built by MinGW-W64 project) Windows-10.0.17763 GitHub Actions GCC 8.4.0 (Ubuntu 8.4.0-3ubuntu2) Ubuntu 20.04.3 LTS GitHub Actions GCC 9.3.0 (Ubuntu 9.3.0-17ubuntu1~20.04) Ubuntu 20.04.3 LTS GitHub Actions GCC 10.2.0 (Ubuntu 10.2.0-5ubuntu1~20.04) Ubuntu 20.04.3 LTS GitHub Actions GCC 11.0.1 20210321 (experimental) Ubuntu 20.04.3 LTS GitHub Actions GCC 11.1.0 Ubuntu (aarch64) Drone CI Clang 3.5.2 (3.5.2-3ubuntu1) Ubuntu 20.04.3 LTS GitHub Actions Clang 3.6.2 (3.6.2-3ubuntu2) Ubuntu 20.04.3 LTS GitHub Actions Clang 3.7.1 (3.7.1-2ubuntu2) Ubuntu 20.04.3 LTS GitHub Actions Clang 3.8.0 (3.8.0-2ubuntu4) Ubuntu 20.04.3 LTS GitHub Actions Clang 3.9.1 (3.9.1-4ubuntu3~16.04.2) Ubuntu 20.04.3 LTS GitHub Actions Clang 4.0.0 (4.0.0-1ubuntu1~16.04.2) Ubuntu 20.04.3 LTS GitHub Actions Clang 5.0.0 (5.0.0-3~16.04.1) Ubuntu 20.04.3 LTS GitHub Actions Clang 6.0.1 (6.0.1-14) Ubuntu 20.04.3 LTS GitHub Actions Clang 7.0.1 (7.0.1-12) Ubuntu 20.04.3 LTS GitHub Actions Clang 8.0.1 (8.0.1-9) Ubuntu 20.04.3 LTS GitHub Actions Clang 9.0.1 (9.0.1-12) Ubuntu 20.04.3 LTS GitHub Actions Clang 10.0.0 (10.0.0-4ubuntu1) Ubuntu 20.04.3 LTS GitHub Actions Clang 10.0.0 with GNU-like command-line Windows-10.0.17763 GitHub Actions Clang 11.0.0 with GNU-like command-line Windows-10.0.17763 GitHub Actions Clang 11.0.0 with MSVC-like command-line Windows-10.0.17763 GitHub Actions Clang 11.0.0 (11.0.0-2~ubuntu20.04.1) Ubuntu 20.04.3 LTS GitHub Actions Clang 12.0.0 (12.0.0-3ubuntu1~20.04.3) Ubuntu 20.04.3 LTS GitHub Actions Clang 13.0.1 (13.0.1-++20211015123032+cf15ccdeb6d5-1exp120211015003613.5 Ubuntu 20.04.3 LTS GitHub Actions Clang 14.0.0 (14.0.0-++20211015062452+81e9c90686f7-1exp120211015063048.20 Ubuntu 20.04.3 LTS GitHub Actions Visual Studio 14 2015 MSVC 19.0.24241.7 (Build Engine version 14.0.25420.1) Windows-6.3.9600 AppVeyor Visual Studio 15 2017 MSVC 19.16.27035.0 (Build Engine version 15.9.21+g9802d43bc3 for .NET Framework) Windows-10.0.14393 AppVeyor Visual Studio 15 2017 MSVC 19.16.27045.0 (Build Engine version 15.9.21+g9802d43bc3 for .NET Framework) Windows-10.0.14393 GitHub Actions Visual Studio 16 2019 MSVC 19.28.29912.0 (Build Engine version 16.9.0+57a23d249 for .NET Framework) Windows-10.0.17763 GitHub Actions Visual Studio 16 2019 MSVC 19.28.29912.0 (Build Engine version 16.9.0+57a23d249 for .NET Framework) Windows-10.0.17763 AppVeyor Integration json.hpp is the single required file in single_include/nlohmann or released here. You need to add #include <nlohmann/json.hpp> // for convenience using json = nlohmann::json; to the files you want to process JSON and set the necessary switches to enable C++11 (e.g., -std=c++11 for GCC and Clang). You can further use file include/nlohmann/json_fwd.hpp for forward-declarations. The installation of json_fwd.hpp (as part of cmake's install step), can be achieved by setting -DJSON_MultipleHeaders=ON. CMake You can also use the nlohmann_json::nlohmann_json interface target in CMake. This target populates the appropriate usage requirements for INTERFACE_INCLUDE_DIRECTORIES to point to the appropriate include directories and INTERFACE_COMPILE_FEATURES for the necessary C++11 flags. External To use this library from a CMake project, you can locate it directly with find_package() and use the namespaced imported target from the generated package configuration: # CMakeLists.txt find_package(nlohmann_json 3.2.0 REQUIRED) ... add_library(foo ...) ... target_link_libraries(foo PRIVATE nlohmann_json::nlohmann_json) The package configuration file, nlohmann_jsonConfig.cmake, can be used either from an install tree or directly out of the build tree. Embedded To embed the library directly into an existing CMake project, place the entire source tree in a subdirectory and call add_subdirectory() in your CMakeLists.txt file: # Typically you don't care so much for a third party library's tests to be # run from your own project's code. set(JSON_BuildTests OFF CACHE INTERNAL \"\") # If you only include this third party in PRIVATE source files, you do not # need to install it when your main project gets installed. # set(JSON_Install OFF CACHE INTERNAL \"\") # Don't use include(nlohmann_json/CMakeLists.txt) since that carries with it # unintended consequences that will break the build. It's generally # discouraged (although not necessarily well documented as such) to use # include(...) for pulling in other CMake projects anyways. add_subdirectory(nlohmann_json) ... add_library(foo ...) ... target_link_libraries(foo PRIVATE nlohmann_json::nlohmann_json) Embedded (FetchContent) Since CMake v3.11, FetchContent can be used to automatically download the repository as a dependency at configure time. Example: include(FetchContent) FetchContent_Declare(json GIT_REPOSITORY https://github.com/nlohmann/json.git GIT_TAG v3.7.3) FetchContent_GetProperties(json) if(NOT json_POPULATED) FetchContent_Populate(json) add_subdirectory(${json_SOURCE_DIR} ${json_BINARY_DIR} EXCLUDE_FROM_ALL) endif() target_link_libraries(foo PRIVATE nlohmann_json::nlohmann_json) Note: The repository https://github.com/nlohmann/json download size is huge. It contains all the dataset used for the benchmarks. You might want to depend on a smaller repository. For instance, you might want to replace the URL above by https://github.com/ArthurSonzogni/nlohmann_json_cmake_fetchcontent Supporting Both To allow your project to support either an externally supplied or an embedded JSON library, you can use a pattern akin to the following: # Top level CMakeLists.txt project(FOO) ... option(FOO_USE_EXTERNAL_JSON \"Use an external JSON library\" OFF) ... add_subdirectory(thirdparty) ... add_library(foo ...) ... # Note that the namespaced target will always be available regardless of the # import method target_link_libraries(foo PRIVATE nlohmann_json::nlohmann_json) # thirdparty/CMakeLists.txt ... if(FOO_USE_EXTERNAL_JSON) find_package(nlohmann_json 3.2.0 REQUIRED) else() set(JSON_BuildTests OFF CACHE INTERNAL \"\") add_subdirectory(nlohmann_json) endif() ... thirdparty/nlohmann_json is then a complete copy of this source tree. Package Managers If you are using OS X and Homebrew, just type brew install nlohmann-json and you're set. If you want the bleeding edge rather than the latest release, use brew install nlohmann-json --HEAD. See nlohmann-json for more information. If you are using the Meson Build System, add this source tree as a meson subproject. You may also use the include.zip published in this project's Releases to reduce the size of the vendored source tree. Alternatively, you can get a wrap file by downloading it from Meson WrapDB, or simply use meson wrap install nlohmann_json. Please see the meson project for any issues regarding the packaging. The provided meson.build can also be used as an alternative to cmake for installing nlohmann_json system-wide in which case a pkg-config file is installed. To use it, simply have your build system require the nlohmann_json pkg-config dependency. In Meson, it is preferred to use the dependency() object with a subproject fallback, rather than using the subproject directly. If you are using Conan to manage your dependencies, merely add nlohmann_json/x.y.z to your conanfile's requires, where x.y.z is the release version you want to use. Please file issues here if you experience problems with the packages. If you are using Spack to manage your dependencies, you can use the nlohmann-json package. Please see the spack project for any issues regarding the packaging. If you are using hunter on your project for external dependencies, then you can use the nlohmann_json package. Please see the hunter project for any issues regarding the packaging. If you are using Buckaroo, you can install this library's module with buckaroo add github.com/buckaroo-pm/nlohmann-json. Please file issues here. There is a demo repo here. If you are using vcpkg on your project for external dependencies, then you can install the nlohmann-json package with vcpkg install nlohmann-json and follow the then displayed descriptions. Please see the vcpkg project for any issues regarding the packaging. If you are using cget, you can install the latest development version with cget install nlohmann/json. A specific version can be installed with cget install nlohmann/json@v3.1.0. Also, the multiple header version can be installed by adding the -DJSON_MultipleHeaders=ON flag (i.e., cget install nlohmann/json -DJSON_MultipleHeaders=ON). If you are using CocoaPods, you can use the library by adding pod \"nlohmann_json\", '~>3.1.2' to your podfile (see an example). Please file issues here. If you are using NuGet, you can use the package nlohmann.json. Please check this extensive description on how to use the package. Please files issues here. If you are using conda, you can use the package nlohmann_json from conda-forge executing conda install -c conda-forge nlohmann_json. Please file issues here. If you are using MSYS2, you can use the mingw-w64-nlohmann-json package, just type pacman -S mingw-w64-i686-nlohmann-json or pacman -S mingw-w64-x86_64-nlohmann-json for installation. Please file issues here if you experience problems with the packages. If you are using MacPorts, execute sudo port install nlohmann-json to install the nlohmann-json package. If you are using build2, you can use the nlohmann-json package from the public repository https://cppget.org or directly from the package's sources repository. In your project's manifest file, just add depends: nlohmann-json (probably with some version constraints). If you are not familiar with using dependencies in build2, please read this introduction. Please file issues here if you experience problems with the packages. If you are using wsjcpp, you can use the command wsjcpp install \"https://github.com/nlohmann/json:develop\" to get the latest version. Note you can change the branch \":develop\" to an existing tag or another branch. If you are using CPM.cmake, you can check this example. After adding CPM script to your project, implement the following snippet to your CMake: CPMAddPackage( NAME nlohmann_json GITHUB_REPOSITORY nlohmann/json VERSION 3.9.1) Pkg-config If you are using bare Makefiles, you can use pkg-config to generate the include flags that point to where the library is installed: pkg-config nlohmann_json --cflags Users of the Meson build system will also be able to use a system wide library, which will be found by pkg-config: json = dependency('nlohmann_json', required: true) License The class is licensed under the MIT License: Copyright 2013-2021 Niels Lohmann Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the Software), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED AS IS, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. The class contains the UTF-8 Decoder from Bjoern Hoehrmann which is licensed under the MIT License (see above). Copyright 2008-2009 Bjrn Hoehrmann bjoern@hoehrmann.de The class contains a slightly modified version of the Grisu2 algorithm from Florian Loitsch which is licensed under the MIT License (see above). Copyright 2009 Florian Loitsch The class contains a copy of Hedley from Evan Nemerson which is licensed as CC0-1.0. The class contains parts of Google Abseil which is licensed under the Apache 2.0 License. Contact If you have questions regarding the library, I would like to invite you to open an issue at GitHub. Please describe your request, problem, or question as detailed as possible, and also mention the version of the library you are using as well as the version of your compiler and operating system. Opening an issue at GitHub allows other users and contributors to this library to collaborate. For instance, I have little experience with MSVC, and most issues in this regard have been solved by a growing community. If you have a look at the closed issues, you will see that we react quite timely in most cases. Only if your request would contain confidential information, please send me an email. For encrypted messages, please use this key. Security Commits by Niels Lohmann and releases are signed with this PGP Key. Thanks I deeply appreciate the help of the following people. Teemperor implemented CMake support and lcov integration, realized escape and Unicode handling in the string parser, and fixed the JSON serialization. elliotgoodrich fixed an issue with double deletion in the iterator classes. kirkshoop made the iterators of the class composable to other libraries. wancw fixed a bug that hindered the class to compile with Clang. Tomas blad found a bug in the iterator implementation. Joshua C. Randall fixed a bug in the floating-point serialization. Aaron Burghardt implemented code to parse streams incrementally. Furthermore, he greatly improved the parser class by allowing the definition of a filter function to discard undesired elements while parsing. Daniel Kopeek fixed a bug in the compilation with GCC 5.0. Florian Weber fixed a bug in and improved the performance of the comparison operators. Eric Cornelius pointed out a bug in the handling with NaN and infinity values. He also improved the performance of the string escaping. implemented a conversion from anonymous enums. kepkin patiently pushed forward the support for Microsoft Visual studio. gregmarr simplified the implementation of reverse iterators and helped with numerous hints and improvements. In particular, he pushed forward the implementation of user-defined types. Caio Luppi fixed a bug in the Unicode handling. dariomt fixed some typos in the examples. Daniel Frey cleaned up some pointers and implemented exception-safe memory allocation. Colin Hirsch took care of a small namespace issue. Huu Nguyen correct a variable name in the documentation. Silverweed overloaded parse() to accept an rvalue reference. dariomt fixed a subtlety in MSVC type support and implemented the get_ref() function to get a reference to stored values. ZahlGraf added a workaround that allows compilation using Android NDK. whackashoe replaced a function that was marked as unsafe by Visual Studio. 406345 fixed two small warnings. Glen Fernandes noted a potential portability problem in the has_mapped_type function. Corbin Hughes fixed some typos in the contribution guidelines. twelsby fixed the array subscript operator, an issue that failed the MSVC build, and floating-point parsing/dumping. He further added support for unsigned integer numbers and implemented better roundtrip support for parsed numbers. Volker Diels-Grabsch fixed a link in the README file. msm- added support for American Fuzzy Lop. Annihil fixed an example in the README file. Themercee noted a wrong URL in the README file. Lv Zheng fixed a namespace issue with int64_t and uint64_t. abc100m analyzed the issues with GCC 4.8 and proposed a partial solution. zewt added useful notes to the README file about Android. Rbert Mrki added a fix to use move iterators and improved the integration via CMake. Chris Kitching cleaned up the CMake files. Tom Needham fixed a subtle bug with MSVC 2015 which was also proposed by Michael K.. Mrio Feroldi fixed a small typo. duncanwerner found a really embarrassing performance regression in the 2.0.0 release. Damien fixed one of the last conversion warnings. Thomas Braun fixed a warning in a test case and adjusted MSVC calls in the CI. Tho DELRIEU patiently and constructively oversaw the long way toward iterator-range parsing. He also implemented the magic behind the serialization/deserialization of user-defined types and split the single header file into smaller chunks. Stefan fixed a minor issue in the documentation. Vasil Dimov fixed the documentation regarding conversions from std::multiset. ChristophJud overworked the CMake files to ease project inclusion. Vladimir Petrigo made a SFINAE hack more readable and added Visual Studio 17 to the build matrix. Denis Andrejew fixed a grammar issue in the README file. Pierre-Antoine Lacaze found a subtle bug in the dump() function. TurpentineDistillery pointed to std::locale::classic() to avoid too much locale joggling, found some nice performance improvements in the parser, improved the benchmarking code, and realized locale-independent number parsing and printing. cgzones had an idea how to fix the Coverity scan. Jared Grubb silenced a nasty documentation warning. Yixin Zhang fixed an integer overflow check. Bosswestfalen merged two iterator classes into a smaller one. Daniel599 helped to get Travis execute the tests with Clang's sanitizers. Jonathan Lee fixed an example in the README file. gnzlbg supported the implementation of user-defined types. Alexej Harm helped to get the user-defined types working with Visual Studio. Jared Grubb supported the implementation of user-defined types. EnricoBilla noted a typo in an example. Martin Hoeovsk found a way for a 2x speedup for the compilation time of the test suite. ukhegg found proposed an improvement for the examples section. rswanson-ihi noted a typo in the README. Mihai Stan fixed a bug in the comparison with nullptrs. Tushar Maheshwari added cotire support to speed up the compilation. TedLyngmo noted a typo in the README, removed unnecessary bit arithmetic, and fixed some -Weffc++ warnings. Krzysztof Wo made exceptions more visible. ftillier fixed a compiler warning. tinloaf made sure all pushed warnings are properly popped. Fytch found a bug in the documentation. Jay Sistar implemented a Meson build description. Henry Lee fixed a warning in ICC and improved the iterator implementation. Vincent Thiery maintains a package for the Conan package manager. Steffen fixed a potential issue with MSVC and std::min. Mike Tzou fixed some typos. amrcode noted a misleading documentation about comparison of floats. Oleg Endo reduced the memory consumption by replacing <iostream> with <iosfwd>. dan-42 cleaned up the CMake files to simplify including/reusing of the library. Nikita Ofitserov allowed for moving values from initializer lists. Greg Hurrell fixed a typo. Dmitry Kukovinets fixed a typo. kbthomp1 fixed an issue related to the Intel OSX compiler. Markus Werle fixed a typo. WebProdPP fixed a subtle error in a precondition check. Alex noted an error in a code sample. Tom de Geus reported some warnings with ICC and helped fixing them. Perry Kundert simplified reading from input streams. Sonu Lohani fixed a small compilation error. Jamie Seward fixed all MSVC warnings. Nate Vargas added a Doxygen tag file. pvleuven helped fixing a warning in ICC. Pavel helped fixing some warnings in MSVC. Jamie Seward avoided unnecessary string copies in find() and count(). Mitja fixed some typos. Jorrit Wronski updated the Hunter package links. Matthias Mller added a .natvis for the MSVC debug view. bogemic fixed some C++17 deprecation warnings. Eren Okka fixed some MSVC warnings. abolz integrated the Grisu2 algorithm for proper floating-point formatting, allowing more roundtrip checks to succeed. Vadim Evard fixed a Markdown issue in the README. zerodefect fixed a compiler warning. Kert allowed to template the string type in the serialization and added the possibility to override the exceptional behavior. mark-99 helped fixing an ICC error. Patrik Huber fixed links in the README file. johnfb found a bug in the implementation of CBOR's indefinite length strings. Paul Fultz II added a note on the cget package manager. Wilson Lin made the integration section of the README more concise. RalfBielig detected and fixed a memory leak in the parser callback. agrianius allowed to dump JSON to an alternative string type. Kevin Tonon overworked the C++11 compiler checks in CMake. Axel Huebl simplified a CMake check and added support for the Spack package manager. Carlos O'Ryan fixed a typo. James Upjohn fixed a version number in the compilers section. Chuck Atkins adjusted the CMake files to the CMake packaging guidelines and provided documentation for the CMake integration. Jan Schppach fixed a typo. martin-mfg fixed a typo. Matthias Mller removed the dependency from std::stringstream. agrianius added code to use alternative string implementations. Daniel599 allowed to use more algorithms with the items() function. Julius Rakow fixed the Meson include directory and fixed the links to cppreference.com. Sonu Lohani fixed the compilation with MSVC 2015 in debug mode. grembo fixed the test suite and re-enabled several test cases. Hyeon Kim introduced the macro JSON_INTERNAL_CATCH to control the exception handling inside the library. thyu fixed a compiler warning. David Guthrie fixed a subtle compilation error with Clang 3.4.2. Dennis Fischer allowed to call find_package without installing the library. Hyeon Kim fixed an issue with a double macro definition. Ben Berman made some error messages more understandable. zakalibit fixed a compilation problem with the Intel C++ compiler. mandreyel fixed a compilation problem. Kostiantyn Ponomarenko added version and license information to the Meson build file. Henry Schreiner added support for GCC 4.8. knilch made sure the test suite does not stall when run in the wrong directory. Antonio Borondo fixed an MSVC 2017 warning. Dan Gendreau implemented the NLOHMANN_JSON_SERIALIZE_ENUM macro to quickly define a enum/JSON mapping. efp added line and column information to parse errors. julian-becker added BSON support. Pratik Chowdhury added support for structured bindings. David Avedissian added support for Clang 5.0.1 (PS4 version). Jonathan Dumaresq implemented an input adapter to read from FILE*. kjpus fixed a link in the documentation. Manvendra Singh fixed a typo in the documentation. ziggurat29 fixed an MSVC warning. Sylvain Corlay added code to avoid an issue with MSVC. mefyl fixed a bug when JSON was parsed from an input stream. Millian Poquet allowed to install the library via Meson. Michael Behrns-Miller found an issue with a missing namespace. Nasztanovics Ferenc fixed a compilation issue with libc 2.12. Andreas Schwab fixed the endian conversion. Mark-Dunning fixed a warning in MSVC. Gareth Sylvester-Bradley added operator/ for JSON Pointers. John-Mark noted a missing header. Vitaly Zaitsev fixed compilation with GCC 9.0. Laurent Stacul fixed compilation with GCC 9.0. Ivor Wanders helped reducing the CMake requirement to version 3.1. njlr updated the Buckaroo instructions. Lion fixed a compilation issue with GCC 7 on CentOS. Isaac Nickaein improved the integer serialization performance and implemented the contains() function. past-due suppressed an unfixable warning. Elvis Oric improved Meson support. Matj Plch fixed an example in the README. Mark Beckwith fixed a typo. scinart fixed bug in the serializer. Patrick Boettcher implemented push_back() and pop_back() for JSON Pointers. Bruno Oliveira added support for Conda. Michele Caini fixed links in the README. Hani documented how to install the library with NuGet. Mark Beckwith fixed a typo. yann-morin-1998 helped reducing the CMake requirement to version 3.1. Konstantin Podsvirov maintains a package for the MSYS2 software distro. remyabel added GNUInstallDirs to the CMake files. Taylor Howard fixed a unit test. Gabe Ron implemented the to_string method. Watal M. Iwasaki fixed a Clang warning. Viktor Kirilov switched the unit tests from Catch to doctest Juncheng E fixed a typo. tete17 fixed a bug in the contains function. Xav83 fixed some cppcheck warnings. 0xflotus fixed some typos. Christian Deneke added a const version of json_pointer::back. Julien Hamaide made the items() function work with custom string types. Evan Nemerson updated fixed a bug in Hedley and updated this library accordingly. Florian Pigorsch fixed a lot of typos. Camille Bgu fixed an issue in the conversion from std::pair and std::tuple to json. Anthony VH fixed a compile error in an enum deserialization. Yuriy Vountesmery noted a subtle bug in a preprocessor check. Chen fixed numerous issues in the library. Antony Kellermann added a CI step for GCC 10.1. Alex fixed an MSVC warning. Rainer proposed an improvement in the floating-point serialization in CBOR. Francois Chabot made performance improvements in the input adapters. Arthur Sonzogni documented how the library can be included via FetchContent. Rimas Miseviius fixed an error message. Alexander Myasnikov fixed some examples and a link in the README. Hubert Chathi made CMake's version config file architecture-independent. OmnipotentEntity implemented the binary values for CBOR, MessagePack, BSON, and UBJSON. ArtemSarmini fixed a compilation issue with GCC 10 and fixed a leak. Evgenii Sopov integrated the library to the wsjcpp package manager. Sergey Linev fixed a compiler warning. Miguel Magalhes fixed the year in the copyright. Gareth Sylvester-Bradley fixed a compilation issue with MSVC. Alexander weej Jones fixed an example in the README. Antoine Cur fixed some typos in the documentation. jothepro updated links to the Hunter package. Dave Lee fixed link in the README. Jol Lamotte added instruction for using Build2's package manager. Paul Jurczak fixed an example in the README. Sonu Lohani fixed a warning. Carlos Gomes Martinho updated the Conan package source. Konstantin Podsvirov fixed the MSYS2 package documentation. Tridacnid improved the CMake tests. Michael fixed MSVC warnings. Quentin Barbarat fixed an example in the documentation. XyFreak fixed a compiler warning. TotalCaesar659 fixed links in the README. Tanuj Garg improved the fuzzer coverage for UBSAN input. AODQ fixed a compiler warning. jwittbrodt made NLOHMANN_DEFINE_TYPE_NON_INTRUSIVE inline. pfeatherstone improved the upper bound of arguments of the NLOHMANN_DEFINE_TYPE_NON_INTRUSIVE/NLOHMANN_DEFINE_TYPE_INTRUSIVE macros. Jan Prochzka fixed a bug in the CBOR parser for binary and string values. T0b1-iOS fixed a bug in the new hash implementation. Matthew Bauer adjusted the CBOR writer to create tags for binary subtypes. gatopeich implemented an ordered map container for nlohmann::ordered_json. rico Nogueira Rolim added support for pkg-config. KonanM proposed an implementation for the NLOHMANN_DEFINE_TYPE_NON_INTRUSIVE/NLOHMANN_DEFINE_TYPE_INTRUSIVE macros. Guillaume Racicot implemented string_view support and allowed C++20 support. Alex Reinking improved CMake support for FetchContent. Hannes Domani provided a GDB pretty printer. Lars Wirzenius reviewed the README file. Jun Jie fixed a compiler path in the CMake scripts. Ronak Buch fixed typos in the documentation. Alexander Karzhenkov fixed a move constructor and the Travis builds. Leonardo Lima added CPM.Cmake support. Joseph Blackman fixed a warning. Yaroslav updated doctest and implemented unit tests. Martin Stump fixed a bug in the CMake files. Jaakko Moisio fixed a bug in the input adapters. bl-ue fixed some Markdown issues in the README file. William A. Wieselquist fixed an example from the README. abbaswasim fixed an example from the README. Remy Jette fixed a warning. Fraser fixed the documentation. Ben Beasley updated doctest. Doron Behar fixed pkg-config.pc. raduteo fixed a warning. David Pfahler added the possibility to compile the library without I/O support. Morten Fyhn Amundsen fixed a typo. jpl-mac allowed to treat the library as a system header in CMake. Jason Dsouza fixed the indentation of the CMake file. offa added a link to Conan Center to the documentation. TotalCaesar659 updated the links in the documentation to use HTTPS. Rafail Giavrimis fixed the Google Benchmark default branch. Louis Dionne fixed a conversion operator. justanotheranonymoususer made the examples in the README more consistent. Finkman suppressed some -Wfloat-equal warnings. Ferry Huberts fixed -Wswitch-enum warnings. Arseniy Terekhin made the GDB pretty-printer robust against unset variable names. Amir Masoud Abdol updated the Homebrew command as nlohmann/json is now in homebrew-core. Hallot fixed some -Wextra-semi-stmt warnings. Giovanni Cerretani fixed -Wunused warnings on JSON_DIAGNOSTICS. Bogdan Popescu hosts the docset for offline documentation viewers. Carl Smedstad fixed an assertion error when using JSON_DIAGNOSTICS. Thanks a lot for helping out! Please let me know if I forgot someone. Used third-party tools The library itself consists of a single header file licensed under the MIT license. However, it is built, tested, documented, and whatnot using a lot of third-party tools and services. Thanks a lot! amalgamate.py - Amalgamate C source and header files to create a single header file American fuzzy lop for fuzz testing AppVeyor for continuous integration on Windows Artistic Style for automatic source code indentation Clang for compilation with code sanitizers CMake for build automation Codacity for further code analysis Coveralls to measure code coverage Coverity Scan for static analysis cppcheck for static analysis doctest for the unit tests Doxygen to generate documentation git-update-ghpages to upload the documentation to gh-pages GitHub Changelog Generator to generate the ChangeLog Google Benchmark to implement the benchmarks Hedley to avoid re-inventing several compiler-agnostic feature macros lcov to process coverage information and create a HTML view libFuzzer to implement fuzz testing for OSS-Fuzz OSS-Fuzz for continuous fuzz testing of the library (project repository) Probot for automating maintainer tasks such as closing stale issues, requesting missing information, or detecting toxic comments. send_to_wandbox to send code examples to Wandbox Travis for continuous integration on Linux and macOS Valgrind to check for correct memory management Wandbox for online examples Projects using JSON for Modern C++ The library is currently used in Apple macOS Sierra and iOS 10. I am not sure what they are using the library for, but I am happy that it runs on so many devices. Notes Character encoding The library supports Unicode input as follows: Only UTF-8 encoded input is supported which is the default encoding for JSON according to RFC 8259. std::u16string and std::u32string can be parsed, assuming UTF-16 and UTF-32 encoding, respectively. These encodings are not supported when reading from files or other input containers. Other encodings such as Latin-1 or ISO 8859-1 are not supported and will yield parse or serialization errors. Unicode noncharacters will not be replaced by the library. Invalid surrogates (e.g., incomplete pairs such as \\uDEAD) will yield parse errors. The strings stored in the library are UTF-8 encoded. When using the default string type (std::string), note that its length/size functions return the number of stored bytes rather than the number of characters or glyphs. When you store strings with different encodings in the library, calling dump() may throw an exception unless json::error_handler_t::replace or json::error_handler_t::ignore are used as error handlers. To store wide strings (e.g., std::wstring), you need to convert them to a a UTF-8 encoded std::string before, see an example. Comments in JSON This library does not support comments by default. It does so for three reasons: Comments are not part of the JSON specification. You may argue that // or /* */ are allowed in JavaScript, but JSON is not JavaScript. This was not an oversight: Douglas Crockford wrote on this in May 2012: I removed comments from JSON because I saw people were using them to hold parsing directives, a practice which would have destroyed interoperability. I know that the lack of comments makes some people sad, but it shouldn't. Suppose you are using JSON to keep configuration files, which you would like to annotate. Go ahead and insert all the comments you like. Then pipe it through JSMin before handing it to your JSON parser. It is dangerous for interoperability if some libraries would add comment support while others don't. Please check The Harmful Consequences of the Robustness Principle on this. However, you can pass set parameter ignore_comments to true in the parse function to ignore // or /* */ comments. Comments will then be treated as whitespace. Order of object keys By default, the library does not preserve the insertion order of object elements. This is standards-compliant, as the JSON standard defines objects as \"an unordered collection of zero or more name/value pairs\". If you do want to preserve the insertion order, you can try the type nlohmann::ordered_json. Alternatively, you can use a more sophisticated ordered map like tsl::ordered_map (integration) or nlohmann::fifo_map (integration). Memory Release We checked with Valgrind and the Address Sanitizer (ASAN) that there are no memory leaks. If you find that a parsing program with this library does not release memory, please consider the following case and it maybe unrelated to this library. Your program is compiled with glibc. There is a tunable threshold that glibc uses to decide whether to actually return memory to the system or whether to cache it for later reuse. If in your program you make lots of small allocations and those small allocations are not a contiguous block and are presumably below the threshold, then they will not get returned to the OS. Here is a related issue #1924. Further notes The code contains numerous debug assertions which can be switched off by defining the preprocessor macro NDEBUG, see the documentation of assert. In particular, note operator[] implements unchecked access for const objects: If the given key is not present, the behavior is undefined (think of a dereferenced null pointer) and yields an assertion failure if assertions are switched on. If you are not sure whether an element in an object exists, use checked access with the at() function. Furthermore, you can define JSON_ASSERT(x) to replace calls to assert(x). As the exact type of a number is not defined in the JSON specification, this library tries to choose the best fitting C++ number type automatically. As a result, the type double may be used to store numbers which may yield floating-point exceptions in certain rare situations if floating-point exceptions have been unmasked in the calling code. These exceptions are not caused by the library and need to be fixed in the calling code, such as by re-masking the exceptions prior to calling library functions. The code can be compiled without C++ runtime type identification features; that is, you can use the -fno-rtti compiler flag. Exceptions are used widely within the library. They can, however, be switched off with either using the compiler flag -fno-exceptions or by defining the symbol JSON_NOEXCEPTION. In this case, exceptions are replaced by abort() calls. You can further control this behavior by defining JSON_THROW_USER (overriding throw), JSON_TRY_USER (overriding try), and JSON_CATCH_USER (overriding catch). Note that JSON_THROW_USER should leave the current scope (e.g., by throwing or aborting), as continuing after it may yield undefined behavior. Note the explanatory what() string of exceptions is not available for MSVC if exceptions are disabled, see #2824. Execute unit tests To compile and run the tests, you need to execute $ mkdir build $ cd build $ cmake .. -DJSON_BuildTests=On $ cmake --build . $ ctest --output-on-failure Note that during the ctest stage, several JSON test files are downloaded from an external repository. If policies forbid downloading artifacts during testing, you can download the files yourself and pass the directory with the test files via -DJSON_TestDataDirectory=path to CMake. Then, no Internet connectivity is required. See issue #2189 for more information. In case you have downloaded the library rather than checked out the code via Git, test cmake_fetch_content_configure will fail. Please execute ctest -LE git_required to skip these tests. See issue #2189 for more information. Some tests change the installed files and hence make the whole process not reproducible. Please execute ctest -LE not_reproducible to skip these tests. See issue #2324 for more information. Note you need to call cmake -LE \"not_reproducible|git_required\" to exclude both labels. See issue #2596 for more information. As Intel compilers use unsafe floating point optimization by default, the unit tests may fail. Use flag /fp:precise then. ",
          "How does this compare with RapidJSON, JSONCpp and JSON Spirit - other popular C++ JSON parser libraries?<p>Links:<p><a href=\"http://rapidjson.org/\" rel=\"nofollow\">http://rapidjson.org/</a>\n<a href=\"https://github.com/Tencent/rapidjson\" rel=\"nofollow\">https://github.com/Tencent/rapidjson</a><p><a href=\"https://github.com/open-source-parsers/jsoncpp\" rel=\"nofollow\">https://github.com/open-source-parsers/jsoncpp</a><p><a href=\"https://www.codeproject.com/Articles/20027/JSON-Spirit-A-C-JSON-Parser-Generator-Implemented\" rel=\"nofollow\">https://www.codeproject.com/Articles/20027/JSON-Spirit-A-C-J...</a>",
          "I don't understand the JSON obsession. JSON as any other file format should be a small detail in any application and require very little plumbing code.<p>In every application, any dependency to JSON should be minimized, contained and preferably eliminated."],
        "story_type":"Normal",
        "url_raw":"https://github.com/nlohmann/json",
        "comments.comment_id":[21106046,
          21106287],
        "comments.comment_author":["einpoklum",
          "kyberias"],
        "comments.comment_descendants":[3,
          12],
        "comments.comment_time":["2019-09-29T09:25:12Z",
          "2019-09-29T10:45:02Z"],
        "comments.comment_text":["How does this compare with RapidJSON, JSONCpp and JSON Spirit - other popular C++ JSON parser libraries?<p>Links:<p><a href=\"http://rapidjson.org/\" rel=\"nofollow\">http://rapidjson.org/</a>\n<a href=\"https://github.com/Tencent/rapidjson\" rel=\"nofollow\">https://github.com/Tencent/rapidjson</a><p><a href=\"https://github.com/open-source-parsers/jsoncpp\" rel=\"nofollow\">https://github.com/open-source-parsers/jsoncpp</a><p><a href=\"https://www.codeproject.com/Articles/20027/JSON-Spirit-A-C-JSON-Parser-Generator-Implemented\" rel=\"nofollow\">https://www.codeproject.com/Articles/20027/JSON-Spirit-A-C-J...</a>",
          "I don't understand the JSON obsession. JSON as any other file format should be a small detail in any application and require very little plumbing code.<p>In every application, any dependency to JSON should be minimized, contained and preferably eliminated."],
        "id":"87c6871b-143e-49cc-b736-5472ba47fb22",
        "url_text":"Design goals Sponsors Support (documentation, FAQ, discussions, API, bug issues) Examples JSON as first-class data type Serialization / Deserialization STL-like access Conversion from STL containers JSON Pointer and JSON Patch JSON Merge Patch Implicit conversions Conversions to/from arbitrary types Specializing enum conversion Binary formats (BSON, CBOR, MessagePack, and UBJSON) Supported compilers Integration CMake Package Managers Pkg-config License Contact Thanks Used third-party tools Projects using JSON for Modern C++ Notes Execute unit tests Design goals There are myriads of JSON libraries out there, and each may even have its reason to exist. Our class had these design goals: Intuitive syntax. In languages such as Python, JSON feels like a first class data type. We used all the operator magic of modern C++ to achieve the same feeling in your code. Check out the examples below and you'll know what I mean. Trivial integration. Our whole code consists of a single header file json.hpp. That's it. No library, no subproject, no dependencies, no complex build system. The class is written in vanilla C++11. All in all, everything should require no adjustment of your compiler flags or project settings. Serious testing. Our class is heavily unit-tested and covers 100% of the code, including all exceptional behavior. Furthermore, we checked with Valgrind and the Clang Sanitizers that there are no memory leaks. Google OSS-Fuzz additionally runs fuzz tests against all parsers 24/7, effectively executing billions of tests so far. To maintain high quality, the project is following the Core Infrastructure Initiative (CII) best practices. Other aspects were not so important to us: Memory efficiency. Each JSON object has an overhead of one pointer (the maximal size of a union) and one enumeration element (1 byte). The default generalization uses the following C++ data types: std::string for strings, int64_t, uint64_t or double for numbers, std::map for objects, std::vector for arrays, and bool for Booleans. However, you can template the generalized class basic_json to your needs. Speed. There are certainly faster JSON libraries out there. However, if your goal is to speed up your development by adding JSON support with a single header, then this library is the way to go. If you know how to use a std::vector or std::map, you are already set. See the contribution guidelines for more information. Sponsors You can sponsor this library at GitHub Sponsors. Named Sponsors Michael Hartmann Stefan Hagen Steve Sperandeo Robert Jefe Lindstdt Steve Wagner Thanks everyone! Support If you have a question, please check if it is already answered in the FAQ or the Q&A section. If not, please ask a new question there. If you want to learn more about how to use the library, check out the rest of the README, have a look at code examples, or browse through the help pages. If you want to understand the API better, check out the API Reference or the Doxygen documentation. If you found a bug, please check the FAQ if it is a known issue or the result of a design decision. Please also have a look at the issue list before you create a new issue. Please provide as many information as possible to help us understand and reproduce your issue. There is also a docset for the documentation browsers Dash, Velocity, and Zeal that contains the full documentation as offline resource. Examples Beside the examples below, you may want to check the documentation where each function contains a separate code example (e.g., check out emplace()). All example files can be compiled and executed on their own (e.g., file emplace.cpp). JSON as first-class data type Here are some examples to give you an idea how to use the class. Assume you want to create the JSON object { \"pi\": 3.141, \"happy\": true, \"name\": \"Niels\", \"nothing\": null, \"answer\": { \"everything\": 42 }, \"list\": [1, 0, 2], \"object\": { \"currency\": \"USD\", \"value\": 42.99 } } With this library, you could write: // create an empty structure (null) json j; // add a number that is stored as double (note the implicit conversion of j to an object) j[\"pi\"] = 3.141; // add a Boolean that is stored as bool j[\"happy\"] = true; // add a string that is stored as std::string j[\"name\"] = \"Niels\"; // add another null object by passing nullptr j[\"nothing\"] = nullptr; // add an object inside the object j[\"answer\"][\"everything\"] = 42; // add an array that is stored as std::vector (using an initializer list) j[\"list\"] = { 1, 0, 2 }; // add another object (using an initializer list of pairs) j[\"object\"] = { {\"currency\", \"USD\"}, {\"value\", 42.99} }; // instead, you could also write (which looks very similar to the JSON above) json j2 = { {\"pi\", 3.141}, {\"happy\", true}, {\"name\", \"Niels\"}, {\"nothing\", nullptr}, {\"answer\", { {\"everything\", 42} }}, {\"list\", {1, 0, 2}}, {\"object\", { {\"currency\", \"USD\"}, {\"value\", 42.99} }} }; Note that in all these cases, you never need to \"tell\" the compiler which JSON value type you want to use. If you want to be explicit or express some edge cases, the functions json::array() and json::object() will help: // a way to express the empty array [] json empty_array_explicit = json::array(); // ways to express the empty object {} json empty_object_implicit = json({}); json empty_object_explicit = json::object(); // a way to express an _array_ of key/value pairs [[\"currency\", \"USD\"], [\"value\", 42.99]] json array_not_object = json::array({ {\"currency\", \"USD\"}, {\"value\", 42.99} }); Serialization / Deserialization To/from strings You can create a JSON value (deserialization) by appending _json to a string literal: // create object from string literal json j = \"{ \\\"happy\\\": true, \\\"pi\\\": 3.141 }\"_json; // or even nicer with a raw string literal auto j2 = R\"( { \"happy\": true, \"pi\": 3.141 } )\"_json; Note that without appending the _json suffix, the passed string literal is not parsed, but just used as JSON string value. That is, json j = \"{ \\\"happy\\\": true, \\\"pi\\\": 3.141 }\" would just store the string \"{ \"happy\": true, \"pi\": 3.141 }\" rather than parsing the actual object. The above example can also be expressed explicitly using json::parse(): // parse explicitly auto j3 = json::parse(R\"({\"happy\": true, \"pi\": 3.141})\"); You can also get a string representation of a JSON value (serialize): // explicit conversion to string std::string s = j.dump(); // {\"happy\":true,\"pi\":3.141} // serialization with pretty printing // pass in the amount of spaces to indent std::cout << j.dump(4) << std::endl; // { // \"happy\": true, // \"pi\": 3.141 // } Note the difference between serialization and assignment: // store a string in a JSON value json j_string = \"this is a string\"; // retrieve the string value auto cpp_string = j_string.get<std::string>(); // retrieve the string value (alternative when an variable already exists) std::string cpp_string2; j_string.get_to(cpp_string2); // retrieve the serialized value (explicit JSON serialization) std::string serialized_string = j_string.dump(); // output of original string std::cout << cpp_string << \" == \" << cpp_string2 << \" == \" << j_string.get<std::string>() << '\\n'; // output of serialized value std::cout << j_string << \" == \" << serialized_string << std::endl; .dump() returns the originally stored string value. Note the library only supports UTF-8. When you store strings with different encodings in the library, calling dump() may throw an exception unless json::error_handler_t::replace or json::error_handler_t::ignore are used as error handlers. To/from streams (e.g. files, string streams) You can also use streams to serialize and deserialize: // deserialize from standard input json j; std::cin >> j; // serialize to standard output std::cout << j; // the setw manipulator was overloaded to set the indentation for pretty printing std::cout << std::setw(4) << j << std::endl; These operators work for any subclasses of std::istream or std::ostream. Here is the same example with files: // read a JSON file std::ifstream i(\"file.json\"); json j; i >> j; // write prettified JSON to another file std::ofstream o(\"pretty.json\"); o << std::setw(4) << j << std::endl; Please note that setting the exception bit for failbit is inappropriate for this use case. It will result in program termination due to the noexcept specifier in use. Read from iterator range You can also parse JSON from an iterator range; that is, from any container accessible by iterators whose value_type is an integral type of 1, 2 or 4 bytes, which will be interpreted as UTF-8, UTF-16 and UTF-32 respectively. For instance, a std::vector<std::uint8_t>, or a std::list<std::uint16_t>: std::vector<std::uint8_t> v = {'t', 'r', 'u', 'e'}; json j = json::parse(v.begin(), v.end()); You may leave the iterators for the range [begin, end): std::vector<std::uint8_t> v = {'t', 'r', 'u', 'e'}; json j = json::parse(v); Custom data source Since the parse function accepts arbitrary iterator ranges, you can provide your own data sources by implementing the LegacyInputIterator concept. struct MyContainer { void advance(); const char& get_current(); }; struct MyIterator { using difference_type = std::ptrdiff_t; using value_type = char; using pointer = const char*; using reference = const char&; using iterator_category = std::input_iterator_tag; MyIterator& operator++() { MyContainer.advance(); return *this; } bool operator!=(const MyIterator& rhs) const { return rhs.target != target; } reference operator*() const { return target.get_current(); } MyContainer* target = nullptr; }; MyIterator begin(MyContainer& tgt) { return MyIterator{&tgt}; } MyIterator end(const MyContainer&) { return {}; } void foo() { MyContainer c; json j = json::parse(c); } SAX interface The library uses a SAX-like interface with the following functions: // called when null is parsed bool null(); // called when a boolean is parsed; value is passed bool boolean(bool val); // called when a signed or unsigned integer number is parsed; value is passed bool number_integer(number_integer_t val); bool number_unsigned(number_unsigned_t val); // called when a floating-point number is parsed; value and original string is passed bool number_float(number_float_t val, const string_t& s); // called when a string is parsed; value is passed and can be safely moved away bool string(string_t& val); // called when a binary value is parsed; value is passed and can be safely moved away bool binary(binary_t& val); // called when an object or array begins or ends, resp. The number of elements is passed (or -1 if not known) bool start_object(std::size_t elements); bool end_object(); bool start_array(std::size_t elements); bool end_array(); // called when an object key is parsed; value is passed and can be safely moved away bool key(string_t& val); // called when a parse error occurs; byte position, the last token, and an exception is passed bool parse_error(std::size_t position, const std::string& last_token, const detail::exception& ex); The return value of each function determines whether parsing should proceed. To implement your own SAX handler, proceed as follows: Implement the SAX interface in a class. You can use class nlohmann::json_sax<json> as base class, but you can also use any class where the functions described above are implemented and public. Create an object of your SAX interface class, e.g. my_sax. Call bool json::sax_parse(input, &my_sax); where the first parameter can be any input like a string or an input stream and the second parameter is a pointer to your SAX interface. Note the sax_parse function only returns a bool indicating the result of the last executed SAX event. It does not return a json value - it is up to you to decide what to do with the SAX events. Furthermore, no exceptions are thrown in case of a parse error - it is up to you what to do with the exception object passed to your parse_error implementation. Internally, the SAX interface is used for the DOM parser (class json_sax_dom_parser) as well as the acceptor (json_sax_acceptor), see file json_sax.hpp. STL-like access We designed the JSON class to behave just like an STL container. In fact, it satisfies the ReversibleContainer requirement. // create an array using push_back json j; j.push_back(\"foo\"); j.push_back(1); j.push_back(true); // also use emplace_back j.emplace_back(1.78); // iterate the array for (json::iterator it = j.begin(); it != j.end(); ++it) { std::cout << *it << '\\n'; } // range-based for for (auto& element : j) { std::cout << element << '\\n'; } // getter/setter const auto tmp = j[0].get<std::string>(); j[1] = 42; bool foo = j.at(2); // comparison j == R\"([\"foo\", 1, true, 1.78])\"_json; // true // other stuff j.size(); // 4 entries j.empty(); // false j.type(); // json::value_t::array j.clear(); // the array is empty again // convenience type checkers j.is_null(); j.is_boolean(); j.is_number(); j.is_object(); j.is_array(); j.is_string(); // create an object json o; o[\"foo\"] = 23; o[\"bar\"] = false; o[\"baz\"] = 3.141; // also use emplace o.emplace(\"weather\", \"sunny\"); // special iterator member functions for objects for (json::iterator it = o.begin(); it != o.end(); ++it) { std::cout << it.key() << \" : \" << it.value() << \"\\n\"; } // the same code as range for for (auto& el : o.items()) { std::cout << el.key() << \" : \" << el.value() << \"\\n\"; } // even easier with structured bindings (C++17) for (auto& [key, value] : o.items()) { std::cout << key << \" : \" << value << \"\\n\"; } // find an entry if (o.contains(\"foo\")) { // there is an entry with key \"foo\" } // or via find and an iterator if (o.find(\"foo\") != o.end()) { // there is an entry with key \"foo\" } // or simpler using count() int foo_present = o.count(\"foo\"); // 1 int fob_present = o.count(\"fob\"); // 0 // delete an entry o.erase(\"foo\"); Conversion from STL containers Any sequence container (std::array, std::vector, std::deque, std::forward_list, std::list) whose values can be used to construct JSON values (e.g., integers, floating point numbers, Booleans, string types, or again STL containers described in this section) can be used to create a JSON array. The same holds for similar associative containers (std::set, std::multiset, std::unordered_set, std::unordered_multiset), but in these cases the order of the elements of the array depends on how the elements are ordered in the respective STL container. std::vector<int> c_vector {1, 2, 3, 4}; json j_vec(c_vector); // [1, 2, 3, 4] std::deque<double> c_deque {1.2, 2.3, 3.4, 5.6}; json j_deque(c_deque); // [1.2, 2.3, 3.4, 5.6] std::list<bool> c_list {true, true, false, true}; json j_list(c_list); // [true, true, false, true] std::forward_list<int64_t> c_flist {12345678909876, 23456789098765, 34567890987654, 45678909876543}; json j_flist(c_flist); // [12345678909876, 23456789098765, 34567890987654, 45678909876543] std::array<unsigned long, 4> c_array {{1, 2, 3, 4}}; json j_array(c_array); // [1, 2, 3, 4] std::set<std::string> c_set {\"one\", \"two\", \"three\", \"four\", \"one\"}; json j_set(c_set); // only one entry for \"one\" is used // [\"four\", \"one\", \"three\", \"two\"] std::unordered_set<std::string> c_uset {\"one\", \"two\", \"three\", \"four\", \"one\"}; json j_uset(c_uset); // only one entry for \"one\" is used // maybe [\"two\", \"three\", \"four\", \"one\"] std::multiset<std::string> c_mset {\"one\", \"two\", \"one\", \"four\"}; json j_mset(c_mset); // both entries for \"one\" are used // maybe [\"one\", \"two\", \"one\", \"four\"] std::unordered_multiset<std::string> c_umset {\"one\", \"two\", \"one\", \"four\"}; json j_umset(c_umset); // both entries for \"one\" are used // maybe [\"one\", \"two\", \"one\", \"four\"] Likewise, any associative key-value containers (std::map, std::multimap, std::unordered_map, std::unordered_multimap) whose keys can construct an std::string and whose values can be used to construct JSON values (see examples above) can be used to create a JSON object. Note that in case of multimaps only one key is used in the JSON object and the value depends on the internal order of the STL container. std::map<std::string, int> c_map { {\"one\", 1}, {\"two\", 2}, {\"three\", 3} }; json j_map(c_map); // {\"one\": 1, \"three\": 3, \"two\": 2 } std::unordered_map<const char*, double> c_umap { {\"one\", 1.2}, {\"two\", 2.3}, {\"three\", 3.4} }; json j_umap(c_umap); // {\"one\": 1.2, \"two\": 2.3, \"three\": 3.4} std::multimap<std::string, bool> c_mmap { {\"one\", true}, {\"two\", true}, {\"three\", false}, {\"three\", true} }; json j_mmap(c_mmap); // only one entry for key \"three\" is used // maybe {\"one\": true, \"two\": true, \"three\": true} std::unordered_multimap<std::string, bool> c_ummap { {\"one\", true}, {\"two\", true}, {\"three\", false}, {\"three\", true} }; json j_ummap(c_ummap); // only one entry for key \"three\" is used // maybe {\"one\": true, \"two\": true, \"three\": true} JSON Pointer and JSON Patch The library supports JSON Pointer (RFC 6901) as alternative means to address structured values. On top of this, JSON Patch (RFC 6902) allows to describe differences between two JSON values - effectively allowing patch and diff operations known from Unix. // a JSON value json j_original = R\"({ \"baz\": [\"one\", \"two\", \"three\"], \"foo\": \"bar\" })\"_json; // access members with a JSON pointer (RFC 6901) j_original[\"/baz/1\"_json_pointer]; // \"two\" // a JSON patch (RFC 6902) json j_patch = R\"([ { \"op\": \"replace\", \"path\": \"/baz\", \"value\": \"boo\" }, { \"op\": \"add\", \"path\": \"/hello\", \"value\": [\"world\"] }, { \"op\": \"remove\", \"path\": \"/foo\"} ])\"_json; // apply the patch json j_result = j_original.patch(j_patch); // { // \"baz\": \"boo\", // \"hello\": [\"world\"] // } // calculate a JSON patch from two JSON values json::diff(j_result, j_original); // [ // { \"op\":\" replace\", \"path\": \"/baz\", \"value\": [\"one\", \"two\", \"three\"] }, // { \"op\": \"remove\",\"path\": \"/hello\" }, // { \"op\": \"add\", \"path\": \"/foo\", \"value\": \"bar\" } // ] JSON Merge Patch The library supports JSON Merge Patch (RFC 7386) as a patch format. Instead of using JSON Pointer (see above) to specify values to be manipulated, it describes the changes using a syntax that closely mimics the document being modified. // a JSON value json j_document = R\"({ \"a\": \"b\", \"c\": { \"d\": \"e\", \"f\": \"g\" } })\"_json; // a patch json j_patch = R\"({ \"a\":\"z\", \"c\": { \"f\": null } })\"_json; // apply the patch j_document.merge_patch(j_patch); // { // \"a\": \"z\", // \"c\": { // \"d\": \"e\" // } // } Implicit conversions Supported types can be implicitly converted to JSON values. It is recommended to NOT USE implicit conversions FROM a JSON value. You can find more details about this recommendation here. You can switch off implicit conversions by defining JSON_USE_IMPLICIT_CONVERSIONS to 0 before including the json.hpp header. When using CMake, you can also achieve this by setting the option JSON_ImplicitConversions to OFF. // strings std::string s1 = \"Hello, world!\"; json js = s1; auto s2 = js.get<std::string>(); // NOT RECOMMENDED std::string s3 = js; std::string s4; s4 = js; // Booleans bool b1 = true; json jb = b1; auto b2 = jb.get<bool>(); // NOT RECOMMENDED bool b3 = jb; bool b4; b4 = jb; // numbers int i = 42; json jn = i; auto f = jn.get<double>(); // NOT RECOMMENDED double f2 = jb; double f3; f3 = jb; // etc. Note that char types are not automatically converted to JSON strings, but to integer numbers. A conversion to a string must be specified explicitly: char ch = 'A'; // ASCII value 65 json j_default = ch; // stores integer number 65 json j_string = std::string(1, ch); // stores string \"A\" Arbitrary types conversions Every type can be serialized in JSON, not just STL containers and scalar types. Usually, you would do something along those lines: namespace ns { // a simple struct to model a person struct person { std::string name; std::string address; int age; }; } ns::person p = {\"Ned Flanders\", \"744 Evergreen Terrace\", 60}; // convert to JSON: copy each value into the JSON object json j; j[\"name\"] = p.name; j[\"address\"] = p.address; j[\"age\"] = p.age; // ... // convert from JSON: copy each value from the JSON object ns::person p { j[\"name\"].get<std::string>(), j[\"address\"].get<std::string>(), j[\"age\"].get<int>() }; It works, but that's quite a lot of boilerplate... Fortunately, there's a better way: // create a person ns::person p {\"Ned Flanders\", \"744 Evergreen Terrace\", 60}; // conversion: person -> json json j = p; std::cout << j << std::endl; // {\"address\":\"744 Evergreen Terrace\",\"age\":60,\"name\":\"Ned Flanders\"} // conversion: json -> person auto p2 = j.get<ns::person>(); // that's it assert(p == p2); Basic usage To make this work with one of your types, you only need to provide two functions: using json = nlohmann::json; namespace ns { void to_json(json& j, const person& p) { j = json{{\"name\", p.name}, {\"address\", p.address}, {\"age\", p.age}}; } void from_json(const json& j, person& p) { j.at(\"name\").get_to(p.name); j.at(\"address\").get_to(p.address); j.at(\"age\").get_to(p.age); } } // namespace ns That's all! When calling the json constructor with your type, your custom to_json method will be automatically called. Likewise, when calling get<your_type>() or get_to(your_type&), the from_json method will be called. Some important things: Those methods MUST be in your type's namespace (which can be the global namespace), or the library will not be able to locate them (in this example, they are in namespace ns, where person is defined). Those methods MUST be available (e.g., proper headers must be included) everywhere you use these conversions. Look at issue 1108 for errors that may occur otherwise. When using get<your_type>(), your_type MUST be DefaultConstructible. (There is a way to bypass this requirement described later.) In function from_json, use function at() to access the object values rather than operator[]. In case a key does not exist, at throws an exception that you can handle, whereas operator[] exhibits undefined behavior. You do not need to add serializers or deserializers for STL types like std::vector: the library already implements these. Simplify your life with macros If you just want to serialize/deserialize some structs, the to_json/from_json functions can be a lot of boilerplate. There are two macros to make your life easier as long as you (1) want to use a JSON object as serialization and (2) want to use the member variable names as object keys in that object: NLOHMANN_DEFINE_TYPE_NON_INTRUSIVE(name, member1, member2, ...) is to be defined inside of the namespace of the class/struct to create code for. NLOHMANN_DEFINE_TYPE_INTRUSIVE(name, member1, member2, ...) is to be defined inside of the class/struct to create code for. This macro can also access private members. In both macros, the first parameter is the name of the class/struct, and all remaining parameters name the members. Examples The to_json/from_json functions for the person struct above can be created with: namespace ns { NLOHMANN_DEFINE_TYPE_NON_INTRUSIVE(person, name, address, age) } Here is an example with private members, where NLOHMANN_DEFINE_TYPE_INTRUSIVE is needed: namespace ns { class address { private: std::string street; int housenumber; int postcode; public: NLOHMANN_DEFINE_TYPE_INTRUSIVE(address, street, housenumber, postcode) }; } How do I convert third-party types? This requires a bit more advanced technique. But first, let's see how this conversion mechanism works: The library uses JSON Serializers to convert types to json. The default serializer for nlohmann::json is nlohmann::adl_serializer (ADL means Argument-Dependent Lookup). It is implemented like this (simplified): template <typename T> struct adl_serializer { static void to_json(json& j, const T& value) { // calls the \"to_json\" method in T's namespace } static void from_json(const json& j, T& value) { // same thing, but with the \"from_json\" method } }; This serializer works fine when you have control over the type's namespace. However, what about boost::optional or std::filesystem::path (C++17)? Hijacking the boost namespace is pretty bad, and it's illegal to add something other than template specializations to std... To solve this, you need to add a specialization of adl_serializer to the nlohmann namespace, here's an example: // partial specialization (full specialization works too) namespace nlohmann { template <typename T> struct adl_serializer<boost::optional<T>> { static void to_json(json& j, const boost::optional<T>& opt) { if (opt == boost::none) { j = nullptr; } else { j = *opt; // this will call adl_serializer<T>::to_json which will // find the free function to_json in T's namespace! } } static void from_json(const json& j, boost::optional<T>& opt) { if (j.is_null()) { opt = boost::none; } else { opt = j.get<T>(); // same as above, but with // adl_serializer<T>::from_json } } }; } How can I use get() for non-default constructible/non-copyable types? There is a way, if your type is MoveConstructible. You will need to specialize the adl_serializer as well, but with a special from_json overload: struct move_only_type { move_only_type() = delete; move_only_type(int ii): i(ii) {} move_only_type(const move_only_type&) = delete; move_only_type(move_only_type&&) = default; int i; }; namespace nlohmann { template <> struct adl_serializer<move_only_type> { // note: the return type is no longer 'void', and the method only takes // one argument static move_only_type from_json(const json& j) { return {j.get<int>()}; } // Here's the catch! You must provide a to_json method! Otherwise you // will not be able to convert move_only_type to json, since you fully // specialized adl_serializer on that type static void to_json(json& j, move_only_type t) { j = t.i; } }; } Can I write my own serializer? (Advanced use) Yes. You might want to take a look at unit-udt.cpp in the test suite, to see a few examples. If you write your own serializer, you'll need to do a few things: use a different basic_json alias than nlohmann::json (the last template parameter of basic_json is the JSONSerializer) use your basic_json alias (or a template parameter) in all your to_json/from_json methods use nlohmann::to_json and nlohmann::from_json when you need ADL Here is an example, without simplifications, that only accepts types with a size <= 32, and uses ADL. // You should use void as a second template argument // if you don't need compile-time checks on T template<typename T, typename SFINAE = typename std::enable_if<sizeof(T) <= 32>::type> struct less_than_32_serializer { template <typename BasicJsonType> static void to_json(BasicJsonType& j, T value) { // we want to use ADL, and call the correct to_json overload using nlohmann::to_json; // this method is called by adl_serializer, // this is where the magic happens to_json(j, value); } template <typename BasicJsonType> static void from_json(const BasicJsonType& j, T& value) { // same thing here using nlohmann::from_json; from_json(j, value); } }; Be very careful when reimplementing your serializer, you can stack overflow if you don't pay attention: template <typename T, void> struct bad_serializer { template <typename BasicJsonType> static void to_json(BasicJsonType& j, const T& value) { // this calls BasicJsonType::json_serializer<T>::to_json(j, value); // if BasicJsonType::json_serializer == bad_serializer ... oops! j = value; } template <typename BasicJsonType> static void to_json(const BasicJsonType& j, T& value) { // this calls BasicJsonType::json_serializer<T>::from_json(j, value); // if BasicJsonType::json_serializer == bad_serializer ... oops! value = j.template get<T>(); // oops! } }; Specializing enum conversion By default, enum values are serialized to JSON as integers. In some cases this could result in undesired behavior. If an enum is modified or re-ordered after data has been serialized to JSON, the later de-serialized JSON data may be undefined or a different enum value than was originally intended. It is possible to more precisely specify how a given enum is mapped to and from JSON as shown below: // example enum type declaration enum TaskState { TS_STOPPED, TS_RUNNING, TS_COMPLETED, TS_INVALID=-1, }; // map TaskState values to JSON as strings NLOHMANN_JSON_SERIALIZE_ENUM( TaskState, { {TS_INVALID, nullptr}, {TS_STOPPED, \"stopped\"}, {TS_RUNNING, \"running\"}, {TS_COMPLETED, \"completed\"}, }) The NLOHMANN_JSON_SERIALIZE_ENUM() macro declares a set of to_json() / from_json() functions for type TaskState while avoiding repetition and boilerplate serialization code. Usage: // enum to JSON as string json j = TS_STOPPED; assert(j == \"stopped\"); // json string to enum json j3 = \"running\"; assert(j3.get<TaskState>() == TS_RUNNING); // undefined json value to enum (where the first map entry above is the default) json jPi = 3.14; assert(jPi.get<TaskState>() == TS_INVALID ); Just as in Arbitrary Type Conversions above, NLOHMANN_JSON_SERIALIZE_ENUM() MUST be declared in your enum type's namespace (which can be the global namespace), or the library will not be able to locate it and it will default to integer serialization. It MUST be available (e.g., proper headers must be included) everywhere you use the conversions. Other Important points: When using get<ENUM_TYPE>(), undefined JSON values will default to the first pair specified in your map. Select this default pair carefully. If an enum or JSON value is specified more than once in your map, the first matching occurrence from the top of the map will be returned when converting to or from JSON. Binary formats (BSON, CBOR, MessagePack, and UBJSON) Though JSON is a ubiquitous data format, it is not a very compact format suitable for data exchange, for instance over a network. Hence, the library supportsBSON (Binary JSON), CBOR (Concise Binary Object Representation), MessagePack, and UBJSON (Universal Binary JSON Specification) to efficiently encode JSON values to byte vectors and to decode such vectors. // create a JSON value json j = R\"({\"compact\": true, \"schema\": 0})\"_json; // serialize to BSON std::vector<std::uint8_t> v_bson = json::to_bson(j); // 0x1B, 0x00, 0x00, 0x00, 0x08, 0x63, 0x6F, 0x6D, 0x70, 0x61, 0x63, 0x74, 0x00, 0x01, 0x10, 0x73, 0x63, 0x68, 0x65, 0x6D, 0x61, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00 // roundtrip json j_from_bson = json::from_bson(v_bson); // serialize to CBOR std::vector<std::uint8_t> v_cbor = json::to_cbor(j); // 0xA2, 0x67, 0x63, 0x6F, 0x6D, 0x70, 0x61, 0x63, 0x74, 0xF5, 0x66, 0x73, 0x63, 0x68, 0x65, 0x6D, 0x61, 0x00 // roundtrip json j_from_cbor = json::from_cbor(v_cbor); // serialize to MessagePack std::vector<std::uint8_t> v_msgpack = json::to_msgpack(j); // 0x82, 0xA7, 0x63, 0x6F, 0x6D, 0x70, 0x61, 0x63, 0x74, 0xC3, 0xA6, 0x73, 0x63, 0x68, 0x65, 0x6D, 0x61, 0x00 // roundtrip json j_from_msgpack = json::from_msgpack(v_msgpack); // serialize to UBJSON std::vector<std::uint8_t> v_ubjson = json::to_ubjson(j); // 0x7B, 0x69, 0x07, 0x63, 0x6F, 0x6D, 0x70, 0x61, 0x63, 0x74, 0x54, 0x69, 0x06, 0x73, 0x63, 0x68, 0x65, 0x6D, 0x61, 0x69, 0x00, 0x7D // roundtrip json j_from_ubjson = json::from_ubjson(v_ubjson); The library also supports binary types from BSON, CBOR (byte strings), and MessagePack (bin, ext, fixext). They are stored by default as std::vector<std::uint8_t> to be processed outside of the library. // CBOR byte string with payload 0xCAFE std::vector<std::uint8_t> v = {0x42, 0xCA, 0xFE}; // read value json j = json::from_cbor(v); // the JSON value has type binary j.is_binary(); // true // get reference to stored binary value auto& binary = j.get_binary(); // the binary value has no subtype (CBOR has no binary subtypes) binary.has_subtype(); // false // access std::vector<std::uint8_t> member functions binary.size(); // 2 binary[0]; // 0xCA binary[1]; // 0xFE // set subtype to 0x10 binary.set_subtype(0x10); // serialize to MessagePack auto cbor = json::to_msgpack(j); // 0xD5 (fixext2), 0x10, 0xCA, 0xFE Supported compilers Though it's 2021 already, the support for C++11 is still a bit sparse. Currently, the following compilers are known to work: GCC 4.8 - 11.0 (and possibly later) Clang 3.4 - 13.0 (and possibly later) Apple Clang 9.1 - 12.4 (and possibly later) Intel C++ Compiler 17.0.2 (and possibly later) Microsoft Visual C++ 2015 / Build Tools 14.0.25123.0 (and possibly later) Microsoft Visual C++ 2017 / Build Tools 15.5.180.51428 (and possibly later) Microsoft Visual C++ 2019 / Build Tools 16.3.1+1def00d3d (and possibly later) I would be happy to learn about other compilers/versions. Please note: GCC 4.8 has a bug 57824): multiline raw strings cannot be the arguments to macros. Don't use multiline raw strings directly in macros with this compiler. Android defaults to using very old compilers and C++ libraries. To fix this, add the following to your Application.mk. This will switch to the LLVM C++ library, the Clang compiler, and enable C++11 and other features disabled by default. APP_STL := c++_shared NDK_TOOLCHAIN_VERSION := clang3.6 APP_CPPFLAGS += -frtti -fexceptions The code compiles successfully with Android NDK, Revision 9 - 11 (and possibly later) and CrystaX's Android NDK version 10. For GCC running on MinGW or Android SDK, the error 'to_string' is not a member of 'std' (or similarly, for strtod or strtof) may occur. Note this is not an issue with the code, but rather with the compiler itself. On Android, see above to build with a newer environment. For MinGW, please refer to this site and this discussion for information on how to fix this bug. For Android NDK using APP_STL := gnustl_static, please refer to this discussion. Unsupported versions of GCC and Clang are rejected by #error directives. This can be switched off by defining JSON_SKIP_UNSUPPORTED_COMPILER_CHECK. Note that you can expect no support in this case. The following compilers are currently used in continuous integration at Travis, AppVeyor, Drone CI, and GitHub Actions: Compiler Operating System CI Provider Apple Clang 10.0.1 (clang-1001.0.46.4); Xcode 10.2.1 macOS 10.14.4 Travis Apple Clang 10.0.1 (clang-1001.0.46.4); Xcode 10.3 macOS 10.15.7 GitHub Actions Apple Clang 11.0.0 (clang-1100.0.33.12); Xcode 11.2.1 macOS 10.15.7 GitHub Actions Apple Clang 11.0.0 (clang-1100.0.33.17); Xcode 11.3.1 macOS 10.15.7 GitHub Actions Apple Clang 11.0.3 (clang-1103.0.32.59); Xcode 11.4.1 macOS 10.15.7 GitHub Actions Apple Clang 11.0.3 (clang-1103.0.32.62); Xcode 11.5 macOS 10.15.7 GitHub Actions Apple Clang 11.0.3 (clang-1103.0.32.62); Xcode 11.6 macOS 10.15.7 GitHub Actions Apple Clang 11.0.3 (clang-1103.0.32.62); Xcode 11.7 macOS 10.15.7 GitHub Actions Apple Clang 12.0.0 (clang-1200.0.32.2); Xcode 12 macOS 10.15.7 GitHub Actions Apple Clang 12.0.0 (clang-1200.0.32.21); Xcode 12.1 macOS 10.15.7 GitHub Actions Apple Clang 12.0.0 (clang-1200.0.32.21); Xcode 12.1.1 macOS 10.15.7 GitHub Actions Apple Clang 12.0.0 (clang-1200.0.32.27); Xcode 12.2 macOS 10.15.7 GitHub Actions Apple Clang 12.0.0 (clang-1200.0.32.28); Xcode 12.3 macOS 10.15.7 GitHub Actions Apple Clang 12.0.0 (clang-1200.0.32.29); Xcode 12.4 macOS 10.15.7 GitHub Actions GCC 4.8.5 (Ubuntu 4.8.5-4ubuntu2) Ubuntu 20.04.3 LTS GitHub Actions GCC 4.9.3 (Ubuntu 4.9.3-13ubuntu2) Ubuntu 20.04.3 LTS GitHub Actions GCC 5.4.0 (Ubuntu 5.4.0-6ubuntu1~16.04.12) Ubuntu 20.04.3 LTS GitHub Actions GCC 6.4.0 (Ubuntu 6.4.0-17ubuntu1) Ubuntu 20.04.3 LTS GitHub Actions GCC 6.5.0 (Ubuntu 6.5.0-2ubuntu1~14.04.1) Ubuntu 14.04.5 LTS Travis GCC 7.5.0 (Ubuntu 7.5.0-6ubuntu2) Ubuntu 20.04.3 LTS GitHub Actions GCC 8.1.0 (x86_64-posix-seh-rev0, Built by MinGW-W64 project) Windows-10.0.17763 GitHub Actions GCC 8.1.0 (i686-posix-dwarf-rev0, Built by MinGW-W64 project) Windows-10.0.17763 GitHub Actions GCC 8.4.0 (Ubuntu 8.4.0-3ubuntu2) Ubuntu 20.04.3 LTS GitHub Actions GCC 9.3.0 (Ubuntu 9.3.0-17ubuntu1~20.04) Ubuntu 20.04.3 LTS GitHub Actions GCC 10.2.0 (Ubuntu 10.2.0-5ubuntu1~20.04) Ubuntu 20.04.3 LTS GitHub Actions GCC 11.0.1 20210321 (experimental) Ubuntu 20.04.3 LTS GitHub Actions GCC 11.1.0 Ubuntu (aarch64) Drone CI Clang 3.5.2 (3.5.2-3ubuntu1) Ubuntu 20.04.3 LTS GitHub Actions Clang 3.6.2 (3.6.2-3ubuntu2) Ubuntu 20.04.3 LTS GitHub Actions Clang 3.7.1 (3.7.1-2ubuntu2) Ubuntu 20.04.3 LTS GitHub Actions Clang 3.8.0 (3.8.0-2ubuntu4) Ubuntu 20.04.3 LTS GitHub Actions Clang 3.9.1 (3.9.1-4ubuntu3~16.04.2) Ubuntu 20.04.3 LTS GitHub Actions Clang 4.0.0 (4.0.0-1ubuntu1~16.04.2) Ubuntu 20.04.3 LTS GitHub Actions Clang 5.0.0 (5.0.0-3~16.04.1) Ubuntu 20.04.3 LTS GitHub Actions Clang 6.0.1 (6.0.1-14) Ubuntu 20.04.3 LTS GitHub Actions Clang 7.0.1 (7.0.1-12) Ubuntu 20.04.3 LTS GitHub Actions Clang 8.0.1 (8.0.1-9) Ubuntu 20.04.3 LTS GitHub Actions Clang 9.0.1 (9.0.1-12) Ubuntu 20.04.3 LTS GitHub Actions Clang 10.0.0 (10.0.0-4ubuntu1) Ubuntu 20.04.3 LTS GitHub Actions Clang 10.0.0 with GNU-like command-line Windows-10.0.17763 GitHub Actions Clang 11.0.0 with GNU-like command-line Windows-10.0.17763 GitHub Actions Clang 11.0.0 with MSVC-like command-line Windows-10.0.17763 GitHub Actions Clang 11.0.0 (11.0.0-2~ubuntu20.04.1) Ubuntu 20.04.3 LTS GitHub Actions Clang 12.0.0 (12.0.0-3ubuntu1~20.04.3) Ubuntu 20.04.3 LTS GitHub Actions Clang 13.0.1 (13.0.1-++20211015123032+cf15ccdeb6d5-1exp120211015003613.5 Ubuntu 20.04.3 LTS GitHub Actions Clang 14.0.0 (14.0.0-++20211015062452+81e9c90686f7-1exp120211015063048.20 Ubuntu 20.04.3 LTS GitHub Actions Visual Studio 14 2015 MSVC 19.0.24241.7 (Build Engine version 14.0.25420.1) Windows-6.3.9600 AppVeyor Visual Studio 15 2017 MSVC 19.16.27035.0 (Build Engine version 15.9.21+g9802d43bc3 for .NET Framework) Windows-10.0.14393 AppVeyor Visual Studio 15 2017 MSVC 19.16.27045.0 (Build Engine version 15.9.21+g9802d43bc3 for .NET Framework) Windows-10.0.14393 GitHub Actions Visual Studio 16 2019 MSVC 19.28.29912.0 (Build Engine version 16.9.0+57a23d249 for .NET Framework) Windows-10.0.17763 GitHub Actions Visual Studio 16 2019 MSVC 19.28.29912.0 (Build Engine version 16.9.0+57a23d249 for .NET Framework) Windows-10.0.17763 AppVeyor Integration json.hpp is the single required file in single_include/nlohmann or released here. You need to add #include <nlohmann/json.hpp> // for convenience using json = nlohmann::json; to the files you want to process JSON and set the necessary switches to enable C++11 (e.g., -std=c++11 for GCC and Clang). You can further use file include/nlohmann/json_fwd.hpp for forward-declarations. The installation of json_fwd.hpp (as part of cmake's install step), can be achieved by setting -DJSON_MultipleHeaders=ON. CMake You can also use the nlohmann_json::nlohmann_json interface target in CMake. This target populates the appropriate usage requirements for INTERFACE_INCLUDE_DIRECTORIES to point to the appropriate include directories and INTERFACE_COMPILE_FEATURES for the necessary C++11 flags. External To use this library from a CMake project, you can locate it directly with find_package() and use the namespaced imported target from the generated package configuration: # CMakeLists.txt find_package(nlohmann_json 3.2.0 REQUIRED) ... add_library(foo ...) ... target_link_libraries(foo PRIVATE nlohmann_json::nlohmann_json) The package configuration file, nlohmann_jsonConfig.cmake, can be used either from an install tree or directly out of the build tree. Embedded To embed the library directly into an existing CMake project, place the entire source tree in a subdirectory and call add_subdirectory() in your CMakeLists.txt file: # Typically you don't care so much for a third party library's tests to be # run from your own project's code. set(JSON_BuildTests OFF CACHE INTERNAL \"\") # If you only include this third party in PRIVATE source files, you do not # need to install it when your main project gets installed. # set(JSON_Install OFF CACHE INTERNAL \"\") # Don't use include(nlohmann_json/CMakeLists.txt) since that carries with it # unintended consequences that will break the build. It's generally # discouraged (although not necessarily well documented as such) to use # include(...) for pulling in other CMake projects anyways. add_subdirectory(nlohmann_json) ... add_library(foo ...) ... target_link_libraries(foo PRIVATE nlohmann_json::nlohmann_json) Embedded (FetchContent) Since CMake v3.11, FetchContent can be used to automatically download the repository as a dependency at configure time. Example: include(FetchContent) FetchContent_Declare(json GIT_REPOSITORY https://github.com/nlohmann/json.git GIT_TAG v3.7.3) FetchContent_GetProperties(json) if(NOT json_POPULATED) FetchContent_Populate(json) add_subdirectory(${json_SOURCE_DIR} ${json_BINARY_DIR} EXCLUDE_FROM_ALL) endif() target_link_libraries(foo PRIVATE nlohmann_json::nlohmann_json) Note: The repository https://github.com/nlohmann/json download size is huge. It contains all the dataset used for the benchmarks. You might want to depend on a smaller repository. For instance, you might want to replace the URL above by https://github.com/ArthurSonzogni/nlohmann_json_cmake_fetchcontent Supporting Both To allow your project to support either an externally supplied or an embedded JSON library, you can use a pattern akin to the following: # Top level CMakeLists.txt project(FOO) ... option(FOO_USE_EXTERNAL_JSON \"Use an external JSON library\" OFF) ... add_subdirectory(thirdparty) ... add_library(foo ...) ... # Note that the namespaced target will always be available regardless of the # import method target_link_libraries(foo PRIVATE nlohmann_json::nlohmann_json) # thirdparty/CMakeLists.txt ... if(FOO_USE_EXTERNAL_JSON) find_package(nlohmann_json 3.2.0 REQUIRED) else() set(JSON_BuildTests OFF CACHE INTERNAL \"\") add_subdirectory(nlohmann_json) endif() ... thirdparty/nlohmann_json is then a complete copy of this source tree. Package Managers If you are using OS X and Homebrew, just type brew install nlohmann-json and you're set. If you want the bleeding edge rather than the latest release, use brew install nlohmann-json --HEAD. See nlohmann-json for more information. If you are using the Meson Build System, add this source tree as a meson subproject. You may also use the include.zip published in this project's Releases to reduce the size of the vendored source tree. Alternatively, you can get a wrap file by downloading it from Meson WrapDB, or simply use meson wrap install nlohmann_json. Please see the meson project for any issues regarding the packaging. The provided meson.build can also be used as an alternative to cmake for installing nlohmann_json system-wide in which case a pkg-config file is installed. To use it, simply have your build system require the nlohmann_json pkg-config dependency. In Meson, it is preferred to use the dependency() object with a subproject fallback, rather than using the subproject directly. If you are using Conan to manage your dependencies, merely add nlohmann_json/x.y.z to your conanfile's requires, where x.y.z is the release version you want to use. Please file issues here if you experience problems with the packages. If you are using Spack to manage your dependencies, you can use the nlohmann-json package. Please see the spack project for any issues regarding the packaging. If you are using hunter on your project for external dependencies, then you can use the nlohmann_json package. Please see the hunter project for any issues regarding the packaging. If you are using Buckaroo, you can install this library's module with buckaroo add github.com/buckaroo-pm/nlohmann-json. Please file issues here. There is a demo repo here. If you are using vcpkg on your project for external dependencies, then you can install the nlohmann-json package with vcpkg install nlohmann-json and follow the then displayed descriptions. Please see the vcpkg project for any issues regarding the packaging. If you are using cget, you can install the latest development version with cget install nlohmann/json. A specific version can be installed with cget install nlohmann/json@v3.1.0. Also, the multiple header version can be installed by adding the -DJSON_MultipleHeaders=ON flag (i.e., cget install nlohmann/json -DJSON_MultipleHeaders=ON). If you are using CocoaPods, you can use the library by adding pod \"nlohmann_json\", '~>3.1.2' to your podfile (see an example). Please file issues here. If you are using NuGet, you can use the package nlohmann.json. Please check this extensive description on how to use the package. Please files issues here. If you are using conda, you can use the package nlohmann_json from conda-forge executing conda install -c conda-forge nlohmann_json. Please file issues here. If you are using MSYS2, you can use the mingw-w64-nlohmann-json package, just type pacman -S mingw-w64-i686-nlohmann-json or pacman -S mingw-w64-x86_64-nlohmann-json for installation. Please file issues here if you experience problems with the packages. If you are using MacPorts, execute sudo port install nlohmann-json to install the nlohmann-json package. If you are using build2, you can use the nlohmann-json package from the public repository https://cppget.org or directly from the package's sources repository. In your project's manifest file, just add depends: nlohmann-json (probably with some version constraints). If you are not familiar with using dependencies in build2, please read this introduction. Please file issues here if you experience problems with the packages. If you are using wsjcpp, you can use the command wsjcpp install \"https://github.com/nlohmann/json:develop\" to get the latest version. Note you can change the branch \":develop\" to an existing tag or another branch. If you are using CPM.cmake, you can check this example. After adding CPM script to your project, implement the following snippet to your CMake: CPMAddPackage( NAME nlohmann_json GITHUB_REPOSITORY nlohmann/json VERSION 3.9.1) Pkg-config If you are using bare Makefiles, you can use pkg-config to generate the include flags that point to where the library is installed: pkg-config nlohmann_json --cflags Users of the Meson build system will also be able to use a system wide library, which will be found by pkg-config: json = dependency('nlohmann_json', required: true) License The class is licensed under the MIT License: Copyright 2013-2021 Niels Lohmann Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the Software), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED AS IS, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. The class contains the UTF-8 Decoder from Bjoern Hoehrmann which is licensed under the MIT License (see above). Copyright 2008-2009 Bjrn Hoehrmann bjoern@hoehrmann.de The class contains a slightly modified version of the Grisu2 algorithm from Florian Loitsch which is licensed under the MIT License (see above). Copyright 2009 Florian Loitsch The class contains a copy of Hedley from Evan Nemerson which is licensed as CC0-1.0. The class contains parts of Google Abseil which is licensed under the Apache 2.0 License. Contact If you have questions regarding the library, I would like to invite you to open an issue at GitHub. Please describe your request, problem, or question as detailed as possible, and also mention the version of the library you are using as well as the version of your compiler and operating system. Opening an issue at GitHub allows other users and contributors to this library to collaborate. For instance, I have little experience with MSVC, and most issues in this regard have been solved by a growing community. If you have a look at the closed issues, you will see that we react quite timely in most cases. Only if your request would contain confidential information, please send me an email. For encrypted messages, please use this key. Security Commits by Niels Lohmann and releases are signed with this PGP Key. Thanks I deeply appreciate the help of the following people. Teemperor implemented CMake support and lcov integration, realized escape and Unicode handling in the string parser, and fixed the JSON serialization. elliotgoodrich fixed an issue with double deletion in the iterator classes. kirkshoop made the iterators of the class composable to other libraries. wancw fixed a bug that hindered the class to compile with Clang. Tomas blad found a bug in the iterator implementation. Joshua C. Randall fixed a bug in the floating-point serialization. Aaron Burghardt implemented code to parse streams incrementally. Furthermore, he greatly improved the parser class by allowing the definition of a filter function to discard undesired elements while parsing. Daniel Kopeek fixed a bug in the compilation with GCC 5.0. Florian Weber fixed a bug in and improved the performance of the comparison operators. Eric Cornelius pointed out a bug in the handling with NaN and infinity values. He also improved the performance of the string escaping. implemented a conversion from anonymous enums. kepkin patiently pushed forward the support for Microsoft Visual studio. gregmarr simplified the implementation of reverse iterators and helped with numerous hints and improvements. In particular, he pushed forward the implementation of user-defined types. Caio Luppi fixed a bug in the Unicode handling. dariomt fixed some typos in the examples. Daniel Frey cleaned up some pointers and implemented exception-safe memory allocation. Colin Hirsch took care of a small namespace issue. Huu Nguyen correct a variable name in the documentation. Silverweed overloaded parse() to accept an rvalue reference. dariomt fixed a subtlety in MSVC type support and implemented the get_ref() function to get a reference to stored values. ZahlGraf added a workaround that allows compilation using Android NDK. whackashoe replaced a function that was marked as unsafe by Visual Studio. 406345 fixed two small warnings. Glen Fernandes noted a potential portability problem in the has_mapped_type function. Corbin Hughes fixed some typos in the contribution guidelines. twelsby fixed the array subscript operator, an issue that failed the MSVC build, and floating-point parsing/dumping. He further added support for unsigned integer numbers and implemented better roundtrip support for parsed numbers. Volker Diels-Grabsch fixed a link in the README file. msm- added support for American Fuzzy Lop. Annihil fixed an example in the README file. Themercee noted a wrong URL in the README file. Lv Zheng fixed a namespace issue with int64_t and uint64_t. abc100m analyzed the issues with GCC 4.8 and proposed a partial solution. zewt added useful notes to the README file about Android. Rbert Mrki added a fix to use move iterators and improved the integration via CMake. Chris Kitching cleaned up the CMake files. Tom Needham fixed a subtle bug with MSVC 2015 which was also proposed by Michael K.. Mrio Feroldi fixed a small typo. duncanwerner found a really embarrassing performance regression in the 2.0.0 release. Damien fixed one of the last conversion warnings. Thomas Braun fixed a warning in a test case and adjusted MSVC calls in the CI. Tho DELRIEU patiently and constructively oversaw the long way toward iterator-range parsing. He also implemented the magic behind the serialization/deserialization of user-defined types and split the single header file into smaller chunks. Stefan fixed a minor issue in the documentation. Vasil Dimov fixed the documentation regarding conversions from std::multiset. ChristophJud overworked the CMake files to ease project inclusion. Vladimir Petrigo made a SFINAE hack more readable and added Visual Studio 17 to the build matrix. Denis Andrejew fixed a grammar issue in the README file. Pierre-Antoine Lacaze found a subtle bug in the dump() function. TurpentineDistillery pointed to std::locale::classic() to avoid too much locale joggling, found some nice performance improvements in the parser, improved the benchmarking code, and realized locale-independent number parsing and printing. cgzones had an idea how to fix the Coverity scan. Jared Grubb silenced a nasty documentation warning. Yixin Zhang fixed an integer overflow check. Bosswestfalen merged two iterator classes into a smaller one. Daniel599 helped to get Travis execute the tests with Clang's sanitizers. Jonathan Lee fixed an example in the README file. gnzlbg supported the implementation of user-defined types. Alexej Harm helped to get the user-defined types working with Visual Studio. Jared Grubb supported the implementation of user-defined types. EnricoBilla noted a typo in an example. Martin Hoeovsk found a way for a 2x speedup for the compilation time of the test suite. ukhegg found proposed an improvement for the examples section. rswanson-ihi noted a typo in the README. Mihai Stan fixed a bug in the comparison with nullptrs. Tushar Maheshwari added cotire support to speed up the compilation. TedLyngmo noted a typo in the README, removed unnecessary bit arithmetic, and fixed some -Weffc++ warnings. Krzysztof Wo made exceptions more visible. ftillier fixed a compiler warning. tinloaf made sure all pushed warnings are properly popped. Fytch found a bug in the documentation. Jay Sistar implemented a Meson build description. Henry Lee fixed a warning in ICC and improved the iterator implementation. Vincent Thiery maintains a package for the Conan package manager. Steffen fixed a potential issue with MSVC and std::min. Mike Tzou fixed some typos. amrcode noted a misleading documentation about comparison of floats. Oleg Endo reduced the memory consumption by replacing <iostream> with <iosfwd>. dan-42 cleaned up the CMake files to simplify including/reusing of the library. Nikita Ofitserov allowed for moving values from initializer lists. Greg Hurrell fixed a typo. Dmitry Kukovinets fixed a typo. kbthomp1 fixed an issue related to the Intel OSX compiler. Markus Werle fixed a typo. WebProdPP fixed a subtle error in a precondition check. Alex noted an error in a code sample. Tom de Geus reported some warnings with ICC and helped fixing them. Perry Kundert simplified reading from input streams. Sonu Lohani fixed a small compilation error. Jamie Seward fixed all MSVC warnings. Nate Vargas added a Doxygen tag file. pvleuven helped fixing a warning in ICC. Pavel helped fixing some warnings in MSVC. Jamie Seward avoided unnecessary string copies in find() and count(). Mitja fixed some typos. Jorrit Wronski updated the Hunter package links. Matthias Mller added a .natvis for the MSVC debug view. bogemic fixed some C++17 deprecation warnings. Eren Okka fixed some MSVC warnings. abolz integrated the Grisu2 algorithm for proper floating-point formatting, allowing more roundtrip checks to succeed. Vadim Evard fixed a Markdown issue in the README. zerodefect fixed a compiler warning. Kert allowed to template the string type in the serialization and added the possibility to override the exceptional behavior. mark-99 helped fixing an ICC error. Patrik Huber fixed links in the README file. johnfb found a bug in the implementation of CBOR's indefinite length strings. Paul Fultz II added a note on the cget package manager. Wilson Lin made the integration section of the README more concise. RalfBielig detected and fixed a memory leak in the parser callback. agrianius allowed to dump JSON to an alternative string type. Kevin Tonon overworked the C++11 compiler checks in CMake. Axel Huebl simplified a CMake check and added support for the Spack package manager. Carlos O'Ryan fixed a typo. James Upjohn fixed a version number in the compilers section. Chuck Atkins adjusted the CMake files to the CMake packaging guidelines and provided documentation for the CMake integration. Jan Schppach fixed a typo. martin-mfg fixed a typo. Matthias Mller removed the dependency from std::stringstream. agrianius added code to use alternative string implementations. Daniel599 allowed to use more algorithms with the items() function. Julius Rakow fixed the Meson include directory and fixed the links to cppreference.com. Sonu Lohani fixed the compilation with MSVC 2015 in debug mode. grembo fixed the test suite and re-enabled several test cases. Hyeon Kim introduced the macro JSON_INTERNAL_CATCH to control the exception handling inside the library. thyu fixed a compiler warning. David Guthrie fixed a subtle compilation error with Clang 3.4.2. Dennis Fischer allowed to call find_package without installing the library. Hyeon Kim fixed an issue with a double macro definition. Ben Berman made some error messages more understandable. zakalibit fixed a compilation problem with the Intel C++ compiler. mandreyel fixed a compilation problem. Kostiantyn Ponomarenko added version and license information to the Meson build file. Henry Schreiner added support for GCC 4.8. knilch made sure the test suite does not stall when run in the wrong directory. Antonio Borondo fixed an MSVC 2017 warning. Dan Gendreau implemented the NLOHMANN_JSON_SERIALIZE_ENUM macro to quickly define a enum/JSON mapping. efp added line and column information to parse errors. julian-becker added BSON support. Pratik Chowdhury added support for structured bindings. David Avedissian added support for Clang 5.0.1 (PS4 version). Jonathan Dumaresq implemented an input adapter to read from FILE*. kjpus fixed a link in the documentation. Manvendra Singh fixed a typo in the documentation. ziggurat29 fixed an MSVC warning. Sylvain Corlay added code to avoid an issue with MSVC. mefyl fixed a bug when JSON was parsed from an input stream. Millian Poquet allowed to install the library via Meson. Michael Behrns-Miller found an issue with a missing namespace. Nasztanovics Ferenc fixed a compilation issue with libc 2.12. Andreas Schwab fixed the endian conversion. Mark-Dunning fixed a warning in MSVC. Gareth Sylvester-Bradley added operator/ for JSON Pointers. John-Mark noted a missing header. Vitaly Zaitsev fixed compilation with GCC 9.0. Laurent Stacul fixed compilation with GCC 9.0. Ivor Wanders helped reducing the CMake requirement to version 3.1. njlr updated the Buckaroo instructions. Lion fixed a compilation issue with GCC 7 on CentOS. Isaac Nickaein improved the integer serialization performance and implemented the contains() function. past-due suppressed an unfixable warning. Elvis Oric improved Meson support. Matj Plch fixed an example in the README. Mark Beckwith fixed a typo. scinart fixed bug in the serializer. Patrick Boettcher implemented push_back() and pop_back() for JSON Pointers. Bruno Oliveira added support for Conda. Michele Caini fixed links in the README. Hani documented how to install the library with NuGet. Mark Beckwith fixed a typo. yann-morin-1998 helped reducing the CMake requirement to version 3.1. Konstantin Podsvirov maintains a package for the MSYS2 software distro. remyabel added GNUInstallDirs to the CMake files. Taylor Howard fixed a unit test. Gabe Ron implemented the to_string method. Watal M. Iwasaki fixed a Clang warning. Viktor Kirilov switched the unit tests from Catch to doctest Juncheng E fixed a typo. tete17 fixed a bug in the contains function. Xav83 fixed some cppcheck warnings. 0xflotus fixed some typos. Christian Deneke added a const version of json_pointer::back. Julien Hamaide made the items() function work with custom string types. Evan Nemerson updated fixed a bug in Hedley and updated this library accordingly. Florian Pigorsch fixed a lot of typos. Camille Bgu fixed an issue in the conversion from std::pair and std::tuple to json. Anthony VH fixed a compile error in an enum deserialization. Yuriy Vountesmery noted a subtle bug in a preprocessor check. Chen fixed numerous issues in the library. Antony Kellermann added a CI step for GCC 10.1. Alex fixed an MSVC warning. Rainer proposed an improvement in the floating-point serialization in CBOR. Francois Chabot made performance improvements in the input adapters. Arthur Sonzogni documented how the library can be included via FetchContent. Rimas Miseviius fixed an error message. Alexander Myasnikov fixed some examples and a link in the README. Hubert Chathi made CMake's version config file architecture-independent. OmnipotentEntity implemented the binary values for CBOR, MessagePack, BSON, and UBJSON. ArtemSarmini fixed a compilation issue with GCC 10 and fixed a leak. Evgenii Sopov integrated the library to the wsjcpp package manager. Sergey Linev fixed a compiler warning. Miguel Magalhes fixed the year in the copyright. Gareth Sylvester-Bradley fixed a compilation issue with MSVC. Alexander weej Jones fixed an example in the README. Antoine Cur fixed some typos in the documentation. jothepro updated links to the Hunter package. Dave Lee fixed link in the README. Jol Lamotte added instruction for using Build2's package manager. Paul Jurczak fixed an example in the README. Sonu Lohani fixed a warning. Carlos Gomes Martinho updated the Conan package source. Konstantin Podsvirov fixed the MSYS2 package documentation. Tridacnid improved the CMake tests. Michael fixed MSVC warnings. Quentin Barbarat fixed an example in the documentation. XyFreak fixed a compiler warning. TotalCaesar659 fixed links in the README. Tanuj Garg improved the fuzzer coverage for UBSAN input. AODQ fixed a compiler warning. jwittbrodt made NLOHMANN_DEFINE_TYPE_NON_INTRUSIVE inline. pfeatherstone improved the upper bound of arguments of the NLOHMANN_DEFINE_TYPE_NON_INTRUSIVE/NLOHMANN_DEFINE_TYPE_INTRUSIVE macros. Jan Prochzka fixed a bug in the CBOR parser for binary and string values. T0b1-iOS fixed a bug in the new hash implementation. Matthew Bauer adjusted the CBOR writer to create tags for binary subtypes. gatopeich implemented an ordered map container for nlohmann::ordered_json. rico Nogueira Rolim added support for pkg-config. KonanM proposed an implementation for the NLOHMANN_DEFINE_TYPE_NON_INTRUSIVE/NLOHMANN_DEFINE_TYPE_INTRUSIVE macros. Guillaume Racicot implemented string_view support and allowed C++20 support. Alex Reinking improved CMake support for FetchContent. Hannes Domani provided a GDB pretty printer. Lars Wirzenius reviewed the README file. Jun Jie fixed a compiler path in the CMake scripts. Ronak Buch fixed typos in the documentation. Alexander Karzhenkov fixed a move constructor and the Travis builds. Leonardo Lima added CPM.Cmake support. Joseph Blackman fixed a warning. Yaroslav updated doctest and implemented unit tests. Martin Stump fixed a bug in the CMake files. Jaakko Moisio fixed a bug in the input adapters. bl-ue fixed some Markdown issues in the README file. William A. Wieselquist fixed an example from the README. abbaswasim fixed an example from the README. Remy Jette fixed a warning. Fraser fixed the documentation. Ben Beasley updated doctest. Doron Behar fixed pkg-config.pc. raduteo fixed a warning. David Pfahler added the possibility to compile the library without I/O support. Morten Fyhn Amundsen fixed a typo. jpl-mac allowed to treat the library as a system header in CMake. Jason Dsouza fixed the indentation of the CMake file. offa added a link to Conan Center to the documentation. TotalCaesar659 updated the links in the documentation to use HTTPS. Rafail Giavrimis fixed the Google Benchmark default branch. Louis Dionne fixed a conversion operator. justanotheranonymoususer made the examples in the README more consistent. Finkman suppressed some -Wfloat-equal warnings. Ferry Huberts fixed -Wswitch-enum warnings. Arseniy Terekhin made the GDB pretty-printer robust against unset variable names. Amir Masoud Abdol updated the Homebrew command as nlohmann/json is now in homebrew-core. Hallot fixed some -Wextra-semi-stmt warnings. Giovanni Cerretani fixed -Wunused warnings on JSON_DIAGNOSTICS. Bogdan Popescu hosts the docset for offline documentation viewers. Carl Smedstad fixed an assertion error when using JSON_DIAGNOSTICS. Thanks a lot for helping out! Please let me know if I forgot someone. Used third-party tools The library itself consists of a single header file licensed under the MIT license. However, it is built, tested, documented, and whatnot using a lot of third-party tools and services. Thanks a lot! amalgamate.py - Amalgamate C source and header files to create a single header file American fuzzy lop for fuzz testing AppVeyor for continuous integration on Windows Artistic Style for automatic source code indentation Clang for compilation with code sanitizers CMake for build automation Codacity for further code analysis Coveralls to measure code coverage Coverity Scan for static analysis cppcheck for static analysis doctest for the unit tests Doxygen to generate documentation git-update-ghpages to upload the documentation to gh-pages GitHub Changelog Generator to generate the ChangeLog Google Benchmark to implement the benchmarks Hedley to avoid re-inventing several compiler-agnostic feature macros lcov to process coverage information and create a HTML view libFuzzer to implement fuzz testing for OSS-Fuzz OSS-Fuzz for continuous fuzz testing of the library (project repository) Probot for automating maintainer tasks such as closing stale issues, requesting missing information, or detecting toxic comments. send_to_wandbox to send code examples to Wandbox Travis for continuous integration on Linux and macOS Valgrind to check for correct memory management Wandbox for online examples Projects using JSON for Modern C++ The library is currently used in Apple macOS Sierra and iOS 10. I am not sure what they are using the library for, but I am happy that it runs on so many devices. Notes Character encoding The library supports Unicode input as follows: Only UTF-8 encoded input is supported which is the default encoding for JSON according to RFC 8259. std::u16string and std::u32string can be parsed, assuming UTF-16 and UTF-32 encoding, respectively. These encodings are not supported when reading from files or other input containers. Other encodings such as Latin-1 or ISO 8859-1 are not supported and will yield parse or serialization errors. Unicode noncharacters will not be replaced by the library. Invalid surrogates (e.g., incomplete pairs such as \\uDEAD) will yield parse errors. The strings stored in the library are UTF-8 encoded. When using the default string type (std::string), note that its length/size functions return the number of stored bytes rather than the number of characters or glyphs. When you store strings with different encodings in the library, calling dump() may throw an exception unless json::error_handler_t::replace or json::error_handler_t::ignore are used as error handlers. To store wide strings (e.g., std::wstring), you need to convert them to a a UTF-8 encoded std::string before, see an example. Comments in JSON This library does not support comments by default. It does so for three reasons: Comments are not part of the JSON specification. You may argue that // or /* */ are allowed in JavaScript, but JSON is not JavaScript. This was not an oversight: Douglas Crockford wrote on this in May 2012: I removed comments from JSON because I saw people were using them to hold parsing directives, a practice which would have destroyed interoperability. I know that the lack of comments makes some people sad, but it shouldn't. Suppose you are using JSON to keep configuration files, which you would like to annotate. Go ahead and insert all the comments you like. Then pipe it through JSMin before handing it to your JSON parser. It is dangerous for interoperability if some libraries would add comment support while others don't. Please check The Harmful Consequences of the Robustness Principle on this. However, you can pass set parameter ignore_comments to true in the parse function to ignore // or /* */ comments. Comments will then be treated as whitespace. Order of object keys By default, the library does not preserve the insertion order of object elements. This is standards-compliant, as the JSON standard defines objects as \"an unordered collection of zero or more name/value pairs\". If you do want to preserve the insertion order, you can try the type nlohmann::ordered_json. Alternatively, you can use a more sophisticated ordered map like tsl::ordered_map (integration) or nlohmann::fifo_map (integration). Memory Release We checked with Valgrind and the Address Sanitizer (ASAN) that there are no memory leaks. If you find that a parsing program with this library does not release memory, please consider the following case and it maybe unrelated to this library. Your program is compiled with glibc. There is a tunable threshold that glibc uses to decide whether to actually return memory to the system or whether to cache it for later reuse. If in your program you make lots of small allocations and those small allocations are not a contiguous block and are presumably below the threshold, then they will not get returned to the OS. Here is a related issue #1924. Further notes The code contains numerous debug assertions which can be switched off by defining the preprocessor macro NDEBUG, see the documentation of assert. In particular, note operator[] implements unchecked access for const objects: If the given key is not present, the behavior is undefined (think of a dereferenced null pointer) and yields an assertion failure if assertions are switched on. If you are not sure whether an element in an object exists, use checked access with the at() function. Furthermore, you can define JSON_ASSERT(x) to replace calls to assert(x). As the exact type of a number is not defined in the JSON specification, this library tries to choose the best fitting C++ number type automatically. As a result, the type double may be used to store numbers which may yield floating-point exceptions in certain rare situations if floating-point exceptions have been unmasked in the calling code. These exceptions are not caused by the library and need to be fixed in the calling code, such as by re-masking the exceptions prior to calling library functions. The code can be compiled without C++ runtime type identification features; that is, you can use the -fno-rtti compiler flag. Exceptions are used widely within the library. They can, however, be switched off with either using the compiler flag -fno-exceptions or by defining the symbol JSON_NOEXCEPTION. In this case, exceptions are replaced by abort() calls. You can further control this behavior by defining JSON_THROW_USER (overriding throw), JSON_TRY_USER (overriding try), and JSON_CATCH_USER (overriding catch). Note that JSON_THROW_USER should leave the current scope (e.g., by throwing or aborting), as continuing after it may yield undefined behavior. Note the explanatory what() string of exceptions is not available for MSVC if exceptions are disabled, see #2824. Execute unit tests To compile and run the tests, you need to execute $ mkdir build $ cd build $ cmake .. -DJSON_BuildTests=On $ cmake --build . $ ctest --output-on-failure Note that during the ctest stage, several JSON test files are downloaded from an external repository. If policies forbid downloading artifacts during testing, you can download the files yourself and pass the directory with the test files via -DJSON_TestDataDirectory=path to CMake. Then, no Internet connectivity is required. See issue #2189 for more information. In case you have downloaded the library rather than checked out the code via Git, test cmake_fetch_content_configure will fail. Please execute ctest -LE git_required to skip these tests. See issue #2189 for more information. Some tests change the installed files and hence make the whole process not reproducible. Please execute ctest -LE not_reproducible to skip these tests. See issue #2324 for more information. Note you need to call cmake -LE \"not_reproducible|git_required\" to exclude both labels. See issue #2596 for more information. As Intel compilers use unsafe floating point optimization by default, the unit tests may fail. Use flag /fp:precise then. ",
        "_version_":1718938224702783488},
      {
        "story_id":18895856,
        "story_author":"ingve",
        "story_descendants":41,
        "story_score":96,
        "story_time":"2019-01-13T09:04:55Z",
        "story_title":"Writing custom tools with Swift",
        "search":["Writing custom tools with Swift",
          "Normal",
          "https://paul-samuels.com/blog/2019/01/12/writing-custom-tools-with-swift/",
          "12 Jan 2019I write a lot of custom command line tools that live alongside my projects. The tools vary in complexity and implementation. From simplest to most involved heres my high level implementation strategy: A single file containing a shebang #!/usr/bin/swift. A Swift Package Manager project of type executable. A Swift Package Manager project of type executable that builds using sources from the main project (Ive written about this here). The same as above but special care has been taken to ensure that the tool can be dockerized and run on Linux. The hardest part with writing custom tools is knowing how to get started, this post will run through creating a single file tool. Problem Outline Lets imagine that we want to grab our most recent app store reviews, get a high level overview of star distribution of the recent reviews and look at any comments that have a rating of 3 stars or below. Skeleton Lets start by making sure we can get an executable Swift file. In your terminal you can do the following: echo '#!/usr/bin/swift\\nprint(\"It works!!\")' > reviews chmod u+x reviews ./reviews The result will be It works!! The first line is equivalent to just creating a file called reviews with the following contents #!/usr/bin/swift print(\"It works!!\") Its not the most exciting file but its good enough to get us rolling. The next command chmod u+x reviews makes the file executable and finally we execute it with ./reviews. Now that we have an executable file lets figure out what our data looks like. Source data Before we progress with writing the rest of the script we need to figure out how to get the data, Im going to do this using curl and jq. This is a useful step because it helps me figure out what the structure of the data is and allows me to experiment with the transformations that I need to apply in my tool. First lets checkout the URL that I grabbed from Stack Overflow (for this example Im just using the Apple Support apps id for reviews): curl \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\" To see how this looks I can pretty print it by piping it through jq: curl \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\" \\ | jq . Response structure ``` { \"feed\": { \"author\": { \"name\": { \"label\": \"...\" }, \"uri\": { \"label\": \"...\" } }, \"entry\": [ { \"author\": { \"uri\": { \"label\": \"...\" }, \"name\": { \"label\": \"...\" }, \"label\": \"\" }, \"im:version\": { \"label\": \"...\" }, \"im:rating\": { \"label\": \"...\" }, \"id\": { \"label\": \"...\" }, \"title\": { \"label\": \"...\" }, \"content\": { \"label\": \"...\", \"attributes\": { \"type\": \"text\" } }, \"link\": { \"attributes\": { \"rel\": \"related\", \"href\": \"...\" } }, \"im:voteSum\": { \"label\": \"...\" }, \"im:contentType\": { \"attributes\": { \"term\": \"Application\", \"label\": \"Application\" } }, \"im:voteCount\": { \"label\": \"...\" } } ], \"updated\": { \"label\": \"...\" }, \"rights\": { \"label\": \"...\" }, \"title\": { \"label\": \"...\" }, \"icon\": { \"label\": \"...\" }, \"link\": [ { \"attributes\": { \"rel\": \"...\", \"type\": \"text/html\", \"href\": \"...\" } }, { \"attributes\": { \"rel\": \"self\", \"href\": \"...\" } }, { \"attributes\": { \"rel\": \"first\", \"href\": \"...\" } }, { \"attributes\": { \"rel\": \"last\", \"href\": \"...\" } }, { \"attributes\": { \"rel\": \"previous\", \"href\": \"...\" } }, { \"attributes\": { \"rel\": \"next\", \"href\": \"...\" } } ], \"id\": { \"label\": \"...\" } } } ``` Looking at the structure I can see that the data I really care about is under feed.entry so I update my jq filter to scope the data a little better: curl \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\" \\ | jq '.feed.entry' Response structure ``` [ { \"author\": { \"uri\": { \"label\": \"...\" }, \"name\": { \"label\": \"...\" }, \"label\": \"\" }, \"im:version\": { \"label\": \"...\" }, \"im:rating\": { \"label\": \"...\" }, \"id\": { \"label\": \"...\" }, \"title\": { \"label\": \"...\" }, \"content\": { \"label\": \"...\", \"attributes\": { \"type\": \"text\" } }, \"link\": { \"attributes\": { \"rel\": \"related\", \"href\": \"...\" } }, \"im:voteSum\": { \"label\": \"...\" }, \"im:contentType\": { \"attributes\": { \"term\": \"Application\", \"label\": \"Application\" } }, \"im:voteCount\": { \"label\": \"...\" } } ] ``` Finally I pull out the fields that I feel will be important for the tool we are writing: curl \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\" \\ | jq '.feed.entry[] | {title: .title.label, rating: .\"im:rating\".label, comment: .content.label}' Response structure ``` [ { \"title\" : \"...\", \"rating\" : \"...\", \"comment\" : \"...\" } ] ``` This is a really fast way of experimenting with data and as well see later its helpful when we come to write the Swift code. The result of the jq filter above is that the large feed will be reduced down to an array of objects with just the title, rating and comment. At this point Im feeling pretty confident that I know what my data will look like so I can go ahead and write this in Swift. Network Request in swift Well use URLSession to make our request - a first attempt might look like: 1 2 3 4 5 6 7 8 #!/usr/bin/swift import Foundation let url = URL(string: \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\")! URLSession.shared.dataTask(with: url, completionHandler: { data, response, error in print(response as Any) }).resume() 2 we need to import Foundation in order to use URLSession and URL. 6 well use the default session as we dont need anything custom. 7 to start well just print anything to check this works. 8 lets not forget to resume the task or nothing will happen. Taking the above we can return to terminal and run ./reviews. nothing happened. The issue here is that dataTask is an asynchronous operation and our script will exit immediately without waiting for the completion to be called. Modifying the code to call dispatchMain() at the end resolves this: #!/usr/bin/swift import Foundation let url = URL(string: \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\")! URLSession.shared.dataTask(with: url, completionHandler: { data, response, error in print(response as Any) }).resume() dispatchMain() Heading back to terminal and running ./reviews we should get some output like Optional(41678 bytes) but weve also introduced a new problem - the programme didnt terminate. Lets fix this and then we can crack on with the rest of our tasks: 1 2 3 4 5 6 7 8 9 10 11 #!/usr/bin/swift import Foundation let url = URL(string: \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\")! URLSession.shared.dataTask(with: url, completionHandler: { data, response, error in print(response as Any) exit(EXIT_SUCCESS) }).resume() dispatchMain() On line 8 Ive added an exit, well provide different exit codes later on depending on whether the tool succeeded or not. To prepare for the next steps well just add some error handling: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 #!/usr/bin/swift import Foundation let url = URL(string: \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\")! URLSession.shared.dataTask(with: url, completionHandler: { data, response, error in if let error = error { print(error.localizedDescription) exit(EXIT_FAILURE) } guard let httpResponse = response as? HTTPURLResponse, 200..<300 ~= httpResponse.statusCode else { print(\"Invalid response \\(String(describing: response))\") exit(EXIT_FAILURE) } if let data = data, data.count > 0 { print(data as Any) exit(EXIT_SUCCESS) } else { print(\"No data!!\") exit(EXIT_FAILURE) } }).resume() dispatchMain() Lines 7-10 are covering cases where there is a failure at the task level. Lines 12-15 are covering errors at the http level. Lines 20-23 are covering cases where there is no data returned. The happy path is hidden in lines 17-20. Side note: Depending on the usage of your scripts you may choose to tailor the level of error reporting and decide if things like force unwraps are acceptable. I tend to find its worth putting error handling in as Ill rarely look at this code, so when it goes wrong it will be a pain to debug without some guidance. Parsing the data We can look at the jq filter we created earlier to guide us on what we need to build. jq .feed.entry[] | {title: .title.label, rating: .\"im:rating\".label, comment: .content.label} We need to dive into the JSON through feed and entry - we can do this by mirroring this structure and using Swifts Decodable: struct Response: Decodable { let feed: Feed struct Feed: Decodable { let entry: [Entry] struct Entry: Decodable { } } } In order to decode an Entry well provide a custom implementation of init(from:) - this will allow us to flatten the data e.g. instead of having entry.title.label we end up with just entry.title. We can do this with the following: struct Entry: Decodable { let comment: String let rating: Int let title: String init(from decoder: Decoder) throws { let container = try decoder.container(keyedBy: CodingKeys.self) comment = try container.nestedContainer(keyedBy: CodingKeys.self, forKey: .comment).decode(String.self, forKey: .label) rating = Int(try container.nestedContainer(keyedBy: CodingKeys.self, forKey: .rating).decode(String.self, forKey: .label))! title = try container.nestedContainer(keyedBy: CodingKeys.self, forKey: .title).decode(String.self, forKey: .label) } private enum CodingKeys: String, CodingKey { case comment = \"content\" case rating = \"im:rating\" case title case label } } With this done we can wire it all up - well go back to the happy path and add: do { print(try JSONDecoder().decode(Response.self, from: data)) exit(EXIT_SUCCESS) } catch { print(\"Failed to decode - \\(error.localizedDescription)\") exit(EXIT_FAILURE) } Thats the complicated stuff out of the way - the next part is the data manipulation that makes the tool actually useful. Processing the data Lets start by printing a summary of the different star ratings. The high level approach will be to loop over all the reviews and keep a track of how many times each star rating was used. Well then return a string that shows the rating number and then an asterisk to represent the number of ratings. func ratings(entries: [Response.Feed.Entry]) -> String { let countedSet = NSCountedSet() entries.forEach { countedSet.add($0.rating) } return (countedSet.allObjects as! [Int]) .sorted(by: >) .reduce(into: \"\") { result, key in result.append(\"\\(key): \\(String(repeating: \"*\", count: countedSet.count(for: key)))\\n\") } } This will yield output like: 5: ***************** 4: ** 3: * 2: **** 1: ************************** The other task we wanted to do was print all the comments that had a rating of 3 or less. This is the simpler of the two tasks as we just need to filter the entries and then format for printing: func reviews(entries: [Response.Feed.Entry]) -> String { return entries .filter { $0.rating <= 3 } .map({ \"\"\" (\\($0.rating)) - \\($0.title) > \\($0.comment) \"\"\" }).joined(separator: \"\\n\\n-\\n\\n\") } This will yield output like: (3) - Love it > This is my favourite app. Putting it all together we end up with: #!/usr/bin/swift import Foundation struct Response: Decodable { let feed: Feed struct Feed: Decodable { let entry: [Entry] struct Entry: Decodable { let comment: String let rating: Int let title: String init(from decoder: Decoder) throws { let container = try decoder.container(keyedBy: CodingKeys.self) comment = try container.nestedContainer(keyedBy: CodingKeys.self, forKey: .comment).decode(String.self, forKey: .label) rating = Int(try container.nestedContainer(keyedBy: CodingKeys.self, forKey: .rating).decode(String.self, forKey: .label))! title = try container.nestedContainer(keyedBy: CodingKeys.self, forKey: .title).decode(String.self, forKey: .label) } private enum CodingKeys: String, CodingKey { case comment = \"content\" case rating = \"im:rating\" case title case label } } } } func ratings(entries: [Response.Feed.Entry]) -> String { let countedSet = NSCountedSet() entries.forEach { countedSet.add($0.rating) } return (countedSet.allObjects as! [Int]) .sorted(by: >) .reduce(into: \"\") { result, key in result.append(\"\\(key): \\(String(repeating: \"*\", count: countedSet.count(for: key)))\\n\") } } func reviews(entries: [Response.Feed.Entry]) -> String { return entries .filter { $0.rating <= 3 } .map({ \"\"\" (\\($0.rating)) - \\($0.title) > \\($0.comment) \"\"\" }).joined(separator: \"\\n\\n-\\n\\n\") } let url = URL(string: \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\")! URLSession.shared.dataTask(with: url, completionHandler: { data, response, error in if let error = error { print(error.localizedDescription) exit(EXIT_FAILURE) } guard let httpResponse = response as? HTTPURLResponse, 200..<300 ~= httpResponse.statusCode else { print(\"Invalid response \\(String(describing: response))\") exit(EXIT_FAILURE) } if let data = data, data.count > 0 { do { let entries = try JSONDecoder().decode(Response.self, from: data).feed.entry print(ratings(entries: entries)) print() print(reviews(entries: entries)) exit(EXIT_SUCCESS) } catch { print(\"Failed to decode - \\(error.localizedDescription)\") exit(EXIT_FAILURE) } } else { print(\"No data!!\") exit(EXIT_FAILURE) } }).resume() dispatchMain() Conclusion Creating tools is a lot of fun and isnt as scary as it might seem at first. Weve done networking, data parsing and some data munging all in one file with not too much effort, which is very rewarding. The single file approach is probably best for shorter tasks. In the example above its already becoming unwieldy and it would be worth considering moving to a Swift Package Manager tool (maybe thats a future post). ",
          "Swift tries to be a jack of all trades, but I think as a scripting language (as in this example) it falls short. Having to start a run loop, write asynchronous callbacks (completion handlers), and implement custom JSON decoders just to make a web request is introducing a ton of complexity that might make sense in an event-driven interactive GUI application, but not so much in a quick shell script.<p>Swift tools might be a good choice for use cases where you need to integrate with an existing Swift project, or if you need lower level APIs.<p>In this example, a Bash script would have been almost done by the time you worked out the `curl | jq` command. But in other similar cases I would suggest Python Requests, which will take perhaps 10% as much code and avoid issues of Linux compatibility and mistakes like forgetting to call `resume()` on your download task or `exit()` in some branch (there are five calls just to keep the program from looping forever).<p>That said, I think this blog post is very informative and well-made for a beginner interested in talking to a web-based JSON API from Swift.",
          "I like it, being able to start with a shell script and then evolve the tool to a static binary is pretty neat, I do that all the time with ActionScript<p>I don't think it's trying to be a \"jack of all trades\", sure Bash can do plenty but when you reach few hundred lines of Bash and start to fight against the syntax to do simple things well...<p>The shebang line is there to be used, you can use sh, bash, etc. but there are other citizen like perl, php, python, etc. so why not swift or anything else if it help you build quickly command-line tools?"],
        "story_type":"Normal",
        "url_raw":"https://paul-samuels.com/blog/2019/01/12/writing-custom-tools-with-swift/",
        "comments.comment_id":[18898126,
          18899738],
        "comments.comment_author":["rgovostes",
          "zwetan"],
        "comments.comment_descendants":[6,
          0],
        "comments.comment_time":["2019-01-13T19:08:41Z",
          "2019-01-14T00:46:57Z"],
        "comments.comment_text":["Swift tries to be a jack of all trades, but I think as a scripting language (as in this example) it falls short. Having to start a run loop, write asynchronous callbacks (completion handlers), and implement custom JSON decoders just to make a web request is introducing a ton of complexity that might make sense in an event-driven interactive GUI application, but not so much in a quick shell script.<p>Swift tools might be a good choice for use cases where you need to integrate with an existing Swift project, or if you need lower level APIs.<p>In this example, a Bash script would have been almost done by the time you worked out the `curl | jq` command. But in other similar cases I would suggest Python Requests, which will take perhaps 10% as much code and avoid issues of Linux compatibility and mistakes like forgetting to call `resume()` on your download task or `exit()` in some branch (there are five calls just to keep the program from looping forever).<p>That said, I think this blog post is very informative and well-made for a beginner interested in talking to a web-based JSON API from Swift.",
          "I like it, being able to start with a shell script and then evolve the tool to a static binary is pretty neat, I do that all the time with ActionScript<p>I don't think it's trying to be a \"jack of all trades\", sure Bash can do plenty but when you reach few hundred lines of Bash and start to fight against the syntax to do simple things well...<p>The shebang line is there to be used, you can use sh, bash, etc. but there are other citizen like perl, php, python, etc. so why not swift or anything else if it help you build quickly command-line tools?"],
        "id":"2d1e4569-f515-405e-a4b5-b3ac7572b879",
        "url_text":"12 Jan 2019I write a lot of custom command line tools that live alongside my projects. The tools vary in complexity and implementation. From simplest to most involved heres my high level implementation strategy: A single file containing a shebang #!/usr/bin/swift. A Swift Package Manager project of type executable. A Swift Package Manager project of type executable that builds using sources from the main project (Ive written about this here). The same as above but special care has been taken to ensure that the tool can be dockerized and run on Linux. The hardest part with writing custom tools is knowing how to get started, this post will run through creating a single file tool. Problem Outline Lets imagine that we want to grab our most recent app store reviews, get a high level overview of star distribution of the recent reviews and look at any comments that have a rating of 3 stars or below. Skeleton Lets start by making sure we can get an executable Swift file. In your terminal you can do the following: echo '#!/usr/bin/swift\\nprint(\"It works!!\")' > reviews chmod u+x reviews ./reviews The result will be It works!! The first line is equivalent to just creating a file called reviews with the following contents #!/usr/bin/swift print(\"It works!!\") Its not the most exciting file but its good enough to get us rolling. The next command chmod u+x reviews makes the file executable and finally we execute it with ./reviews. Now that we have an executable file lets figure out what our data looks like. Source data Before we progress with writing the rest of the script we need to figure out how to get the data, Im going to do this using curl and jq. This is a useful step because it helps me figure out what the structure of the data is and allows me to experiment with the transformations that I need to apply in my tool. First lets checkout the URL that I grabbed from Stack Overflow (for this example Im just using the Apple Support apps id for reviews): curl \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\" To see how this looks I can pretty print it by piping it through jq: curl \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\" \\ | jq . Response structure ``` { \"feed\": { \"author\": { \"name\": { \"label\": \"...\" }, \"uri\": { \"label\": \"...\" } }, \"entry\": [ { \"author\": { \"uri\": { \"label\": \"...\" }, \"name\": { \"label\": \"...\" }, \"label\": \"\" }, \"im:version\": { \"label\": \"...\" }, \"im:rating\": { \"label\": \"...\" }, \"id\": { \"label\": \"...\" }, \"title\": { \"label\": \"...\" }, \"content\": { \"label\": \"...\", \"attributes\": { \"type\": \"text\" } }, \"link\": { \"attributes\": { \"rel\": \"related\", \"href\": \"...\" } }, \"im:voteSum\": { \"label\": \"...\" }, \"im:contentType\": { \"attributes\": { \"term\": \"Application\", \"label\": \"Application\" } }, \"im:voteCount\": { \"label\": \"...\" } } ], \"updated\": { \"label\": \"...\" }, \"rights\": { \"label\": \"...\" }, \"title\": { \"label\": \"...\" }, \"icon\": { \"label\": \"...\" }, \"link\": [ { \"attributes\": { \"rel\": \"...\", \"type\": \"text/html\", \"href\": \"...\" } }, { \"attributes\": { \"rel\": \"self\", \"href\": \"...\" } }, { \"attributes\": { \"rel\": \"first\", \"href\": \"...\" } }, { \"attributes\": { \"rel\": \"last\", \"href\": \"...\" } }, { \"attributes\": { \"rel\": \"previous\", \"href\": \"...\" } }, { \"attributes\": { \"rel\": \"next\", \"href\": \"...\" } } ], \"id\": { \"label\": \"...\" } } } ``` Looking at the structure I can see that the data I really care about is under feed.entry so I update my jq filter to scope the data a little better: curl \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\" \\ | jq '.feed.entry' Response structure ``` [ { \"author\": { \"uri\": { \"label\": \"...\" }, \"name\": { \"label\": \"...\" }, \"label\": \"\" }, \"im:version\": { \"label\": \"...\" }, \"im:rating\": { \"label\": \"...\" }, \"id\": { \"label\": \"...\" }, \"title\": { \"label\": \"...\" }, \"content\": { \"label\": \"...\", \"attributes\": { \"type\": \"text\" } }, \"link\": { \"attributes\": { \"rel\": \"related\", \"href\": \"...\" } }, \"im:voteSum\": { \"label\": \"...\" }, \"im:contentType\": { \"attributes\": { \"term\": \"Application\", \"label\": \"Application\" } }, \"im:voteCount\": { \"label\": \"...\" } } ] ``` Finally I pull out the fields that I feel will be important for the tool we are writing: curl \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\" \\ | jq '.feed.entry[] | {title: .title.label, rating: .\"im:rating\".label, comment: .content.label}' Response structure ``` [ { \"title\" : \"...\", \"rating\" : \"...\", \"comment\" : \"...\" } ] ``` This is a really fast way of experimenting with data and as well see later its helpful when we come to write the Swift code. The result of the jq filter above is that the large feed will be reduced down to an array of objects with just the title, rating and comment. At this point Im feeling pretty confident that I know what my data will look like so I can go ahead and write this in Swift. Network Request in swift Well use URLSession to make our request - a first attempt might look like: 1 2 3 4 5 6 7 8 #!/usr/bin/swift import Foundation let url = URL(string: \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\")! URLSession.shared.dataTask(with: url, completionHandler: { data, response, error in print(response as Any) }).resume() 2 we need to import Foundation in order to use URLSession and URL. 6 well use the default session as we dont need anything custom. 7 to start well just print anything to check this works. 8 lets not forget to resume the task or nothing will happen. Taking the above we can return to terminal and run ./reviews. nothing happened. The issue here is that dataTask is an asynchronous operation and our script will exit immediately without waiting for the completion to be called. Modifying the code to call dispatchMain() at the end resolves this: #!/usr/bin/swift import Foundation let url = URL(string: \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\")! URLSession.shared.dataTask(with: url, completionHandler: { data, response, error in print(response as Any) }).resume() dispatchMain() Heading back to terminal and running ./reviews we should get some output like Optional(41678 bytes) but weve also introduced a new problem - the programme didnt terminate. Lets fix this and then we can crack on with the rest of our tasks: 1 2 3 4 5 6 7 8 9 10 11 #!/usr/bin/swift import Foundation let url = URL(string: \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\")! URLSession.shared.dataTask(with: url, completionHandler: { data, response, error in print(response as Any) exit(EXIT_SUCCESS) }).resume() dispatchMain() On line 8 Ive added an exit, well provide different exit codes later on depending on whether the tool succeeded or not. To prepare for the next steps well just add some error handling: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 #!/usr/bin/swift import Foundation let url = URL(string: \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\")! URLSession.shared.dataTask(with: url, completionHandler: { data, response, error in if let error = error { print(error.localizedDescription) exit(EXIT_FAILURE) } guard let httpResponse = response as? HTTPURLResponse, 200..<300 ~= httpResponse.statusCode else { print(\"Invalid response \\(String(describing: response))\") exit(EXIT_FAILURE) } if let data = data, data.count > 0 { print(data as Any) exit(EXIT_SUCCESS) } else { print(\"No data!!\") exit(EXIT_FAILURE) } }).resume() dispatchMain() Lines 7-10 are covering cases where there is a failure at the task level. Lines 12-15 are covering errors at the http level. Lines 20-23 are covering cases where there is no data returned. The happy path is hidden in lines 17-20. Side note: Depending on the usage of your scripts you may choose to tailor the level of error reporting and decide if things like force unwraps are acceptable. I tend to find its worth putting error handling in as Ill rarely look at this code, so when it goes wrong it will be a pain to debug without some guidance. Parsing the data We can look at the jq filter we created earlier to guide us on what we need to build. jq .feed.entry[] | {title: .title.label, rating: .\"im:rating\".label, comment: .content.label} We need to dive into the JSON through feed and entry - we can do this by mirroring this structure and using Swifts Decodable: struct Response: Decodable { let feed: Feed struct Feed: Decodable { let entry: [Entry] struct Entry: Decodable { } } } In order to decode an Entry well provide a custom implementation of init(from:) - this will allow us to flatten the data e.g. instead of having entry.title.label we end up with just entry.title. We can do this with the following: struct Entry: Decodable { let comment: String let rating: Int let title: String init(from decoder: Decoder) throws { let container = try decoder.container(keyedBy: CodingKeys.self) comment = try container.nestedContainer(keyedBy: CodingKeys.self, forKey: .comment).decode(String.self, forKey: .label) rating = Int(try container.nestedContainer(keyedBy: CodingKeys.self, forKey: .rating).decode(String.self, forKey: .label))! title = try container.nestedContainer(keyedBy: CodingKeys.self, forKey: .title).decode(String.self, forKey: .label) } private enum CodingKeys: String, CodingKey { case comment = \"content\" case rating = \"im:rating\" case title case label } } With this done we can wire it all up - well go back to the happy path and add: do { print(try JSONDecoder().decode(Response.self, from: data)) exit(EXIT_SUCCESS) } catch { print(\"Failed to decode - \\(error.localizedDescription)\") exit(EXIT_FAILURE) } Thats the complicated stuff out of the way - the next part is the data manipulation that makes the tool actually useful. Processing the data Lets start by printing a summary of the different star ratings. The high level approach will be to loop over all the reviews and keep a track of how many times each star rating was used. Well then return a string that shows the rating number and then an asterisk to represent the number of ratings. func ratings(entries: [Response.Feed.Entry]) -> String { let countedSet = NSCountedSet() entries.forEach { countedSet.add($0.rating) } return (countedSet.allObjects as! [Int]) .sorted(by: >) .reduce(into: \"\") { result, key in result.append(\"\\(key): \\(String(repeating: \"*\", count: countedSet.count(for: key)))\\n\") } } This will yield output like: 5: ***************** 4: ** 3: * 2: **** 1: ************************** The other task we wanted to do was print all the comments that had a rating of 3 or less. This is the simpler of the two tasks as we just need to filter the entries and then format for printing: func reviews(entries: [Response.Feed.Entry]) -> String { return entries .filter { $0.rating <= 3 } .map({ \"\"\" (\\($0.rating)) - \\($0.title) > \\($0.comment) \"\"\" }).joined(separator: \"\\n\\n-\\n\\n\") } This will yield output like: (3) - Love it > This is my favourite app. Putting it all together we end up with: #!/usr/bin/swift import Foundation struct Response: Decodable { let feed: Feed struct Feed: Decodable { let entry: [Entry] struct Entry: Decodable { let comment: String let rating: Int let title: String init(from decoder: Decoder) throws { let container = try decoder.container(keyedBy: CodingKeys.self) comment = try container.nestedContainer(keyedBy: CodingKeys.self, forKey: .comment).decode(String.self, forKey: .label) rating = Int(try container.nestedContainer(keyedBy: CodingKeys.self, forKey: .rating).decode(String.self, forKey: .label))! title = try container.nestedContainer(keyedBy: CodingKeys.self, forKey: .title).decode(String.self, forKey: .label) } private enum CodingKeys: String, CodingKey { case comment = \"content\" case rating = \"im:rating\" case title case label } } } } func ratings(entries: [Response.Feed.Entry]) -> String { let countedSet = NSCountedSet() entries.forEach { countedSet.add($0.rating) } return (countedSet.allObjects as! [Int]) .sorted(by: >) .reduce(into: \"\") { result, key in result.append(\"\\(key): \\(String(repeating: \"*\", count: countedSet.count(for: key)))\\n\") } } func reviews(entries: [Response.Feed.Entry]) -> String { return entries .filter { $0.rating <= 3 } .map({ \"\"\" (\\($0.rating)) - \\($0.title) > \\($0.comment) \"\"\" }).joined(separator: \"\\n\\n-\\n\\n\") } let url = URL(string: \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\")! URLSession.shared.dataTask(with: url, completionHandler: { data, response, error in if let error = error { print(error.localizedDescription) exit(EXIT_FAILURE) } guard let httpResponse = response as? HTTPURLResponse, 200..<300 ~= httpResponse.statusCode else { print(\"Invalid response \\(String(describing: response))\") exit(EXIT_FAILURE) } if let data = data, data.count > 0 { do { let entries = try JSONDecoder().decode(Response.self, from: data).feed.entry print(ratings(entries: entries)) print() print(reviews(entries: entries)) exit(EXIT_SUCCESS) } catch { print(\"Failed to decode - \\(error.localizedDescription)\") exit(EXIT_FAILURE) } } else { print(\"No data!!\") exit(EXIT_FAILURE) } }).resume() dispatchMain() Conclusion Creating tools is a lot of fun and isnt as scary as it might seem at first. Weve done networking, data parsing and some data munging all in one file with not too much effort, which is very rewarding. The single file approach is probably best for shorter tasks. In the example above its already becoming unwieldy and it would be worth considering moving to a Swift Package Manager tool (maybe thats a future post). ",
        "_version_":1718938137657344002},
      {
        "story_id":19214387,
        "story_author":"cmsimike",
        "story_descendants":196,
        "story_score":598,
        "story_time":"2019-02-21T03:53:23Z",
        "story_title":"Simdjson – Parsing Gigabytes of JSON per Second",
        "search":["Simdjson – Parsing Gigabytes of JSON per Second",
          "Normal",
          "https://github.com/lemire/simdjson",
          "JSON is everywhere on the Internet. Servers spend a *lot* of time parsing it. We need a fresh approach. The simdjson library uses commonly available SIMD instructions and microparallel algorithms to parse JSON 4x faster than RapidJSON and 25x faster than JSON for Modern C++. Fast: Over 4x faster than commonly used production-grade JSON parsers. Record Breaking Features: Minify JSON at 6 GB/s, validate UTF-8 at 13 GB/s, NDJSON at 3.5 GB/s. Easy: First-class, easy to use and carefully documented APIs. Strict: Full JSON and UTF-8 validation, lossless parsing. Performance with no compromises. Automatic: Selects a CPU-tailored parser at runtime. No configuration needed. Reliable: From memory allocation to error handling, simdjson's design avoids surprises. Peer Reviewed: Our research appears in venues like VLDB Journal, Software: Practice and Experience. This library is part of the Awesome Modern C++ list. Table of Contents Quick Start Documentation Performance results Real-world usage Bindings and Ports of simdjson About simdjson Funding Contributing to simdjson License Quick Start The simdjson library is easily consumable with a single .h and .cpp file. Prerequisites: g++ (version 7 or better) or clang++ (version 6 or better), and a 64-bit system with a command-line shell (e.g., Linux, macOS, freeBSD). We also support programming environments like Visual Studio and Xcode, but different steps are needed. Pull simdjson.h and simdjson.cpp into a directory, along with the sample file twitter.json. wget https://raw.githubusercontent.com/simdjson/simdjson/master/singleheader/simdjson.h https://raw.githubusercontent.com/simdjson/simdjson/master/singleheader/simdjson.cpp https://raw.githubusercontent.com/simdjson/simdjson/master/jsonexamples/twitter.json Create quickstart.cpp: #include <iostream> #include \"simdjson.h\" using namespace simdjson; int main(void) { ondemand::parser parser; padded_string json = padded_string::load(\"twitter.json\"); ondemand::document tweets = parser.iterate(json); std::cout << uint64_t(tweets[\"search_metadata\"][\"count\"]) << \" results.\" << std::endl; } c++ -o quickstart quickstart.cpp simdjson.cpp ./quickstart Documentation Usage documentation is available: Basics is an overview of how to use simdjson and its APIs. Performance shows some more advanced scenarios and how to tune for them. Implementation Selection describes runtime CPU detection and how you can work with it. API contains the automatically generated API documentation. Performance results The simdjson library uses three-quarters less instructions than state-of-the-art parser RapidJSON. To our knowledge, simdjson is the first fully-validating JSON parser to run at gigabytes per second (GB/s) on commodity processors. It can parse millions of JSON documents per second on a single core. The following figure represents parsing speed in GB/s for parsing various files on an Intel Skylake processor (3.4 GHz) using the GNU GCC 10 compiler (with the -O3 flag). We compare against the best and fastest C++ libraries on benchmarks that load and process the data. The simdjson library offers full unicode (UTF-8) validation and exact number parsing. The simdjson library offers high speed whether it processes tiny files (e.g., 300 bytes) or larger files (e.g., 3MB). The following plot presents parsing speed for synthetic files over various sizes generated with a script on a 3.4 GHz Skylake processor (GNU GCC 9, -O3). All our experiments are reproducible. For NDJSON files, we can exceed 3 GB/s with our multithreaded parsing functions. Real-world usage Microsoft FishStore Yandex ClickHouse Clang Build Analyzer Shopify HeapProfiler If you are planning to use simdjson in a product, please work from one of our releases. Bindings and Ports of simdjson We distinguish between \"bindings\" (which just wrap the C++ code) and a port to another programming language (which reimplements everything). ZippyJSON: Swift bindings for the simdjson project. libpy_simdjson: high-speed Python bindings for simdjson using libpy. pysimdjson: Python bindings for the simdjson project. cysimdjson: high-speed Python bindings for the simdjson project. simdjson-rs: Rust port. simdjson-rust: Rust wrapper (bindings). SimdJsonSharp: C# version for .NET Core (bindings and full port). simdjson_nodejs: Node.js bindings for the simdjson project. simdjson_php: PHP bindings for the simdjson project. simdjson_ruby: Ruby bindings for the simdjson project. fast_jsonparser: Ruby bindings for the simdjson project. simdjson-go: Go port using Golang assembly. rcppsimdjson: R bindings. simdjson_erlang: erlang bindings. lua-simdjson: lua bindings. About simdjson The simdjson library takes advantage of modern microarchitectures, parallelizing with SIMD vector instructions, reducing branch misprediction, and reducing data dependency to take advantage of each CPU's multiple execution cores. Some people enjoy reading our paper: A description of the design and implementation of simdjson is in our research article: Geoff Langdale, Daniel Lemire, Parsing Gigabytes of JSON per Second, VLDB Journal 28 (6), 2019. We have an in-depth paper focused on the UTF-8 validation: John Keiser, Daniel Lemire, Validating UTF-8 In Less Than One Instruction Per Byte, Software: Practice & Experience 51 (5), 2021. We also have an informal blog post providing some background and context. For the video inclined, (It was the best voted talk, we're kinda proud of it.) Funding The work is supported by the Natural Sciences and Engineering Research Council of Canada under grant number RGPIN-2017-03910. Contributing to simdjson Head over to CONTRIBUTING.md for information on contributing to simdjson, and HACKING.md for information on source, building, and architecture/design. License This code is made available under the Apache License 2.0. Under Windows, we build some tools using the windows/dirent_portable.h file (which is outside our library code): it under the liberal (business-friendly) MIT license. For compilers that do not support C++17, we bundle the string-view library which is published under the Boost license. Like the Apache license, the Boost license is a permissive license allowing commercial redistribution. For efficient number serialization, we bundle Florian Loitsch's implementation of the Grisu2 algorithm for binary to decimal floating-point numbers. The implementation was slightly modified by JSON for Modern C++ library. Both Florian Loitsch's implementation and JSON for Modern C++ are provided under the MIT license. For runtime dispatching, we use some code from the PyTorch project licensed under 3-clause BSD. ",
          "This is very cool. Meanwhile, in the xi-editor project, we're struggling with the fact that Swift JSON parsing is very slow. My benchmarking clocked in at 0.00089GB/s for Swift 4, and things don't seem to have improved much with Swift 5. I'm encouraging people on that issue to do a blog post.<p>[1]: <a href=\"https://github.com/xi-editor/xi-mac/issues/102\" rel=\"nofollow\">https://github.com/xi-editor/xi-mac/issues/102</a>",
          "One of the two authors here. Happy to answer questions.<p>The intent was to open things but not publicize them at this stage but Hacker News seems to find stuff. Wouldn't surprise me if plenty of folks follow Daniel Lemire on Github as his stuff is always interesting."],
        "story_type":"Normal",
        "url_raw":"https://github.com/lemire/simdjson",
        "comments.comment_id":[19214577,
          19214731],
        "comments.comment_author":["raphlinus",
          "glangdale"],
        "comments.comment_descendants":[7,
          11],
        "comments.comment_time":["2019-02-21T04:38:09Z",
          "2019-02-21T05:14:08Z"],
        "comments.comment_text":["This is very cool. Meanwhile, in the xi-editor project, we're struggling with the fact that Swift JSON parsing is very slow. My benchmarking clocked in at 0.00089GB/s for Swift 4, and things don't seem to have improved much with Swift 5. I'm encouraging people on that issue to do a blog post.<p>[1]: <a href=\"https://github.com/xi-editor/xi-mac/issues/102\" rel=\"nofollow\">https://github.com/xi-editor/xi-mac/issues/102</a>",
          "One of the two authors here. Happy to answer questions.<p>The intent was to open things but not publicize them at this stage but Hacker News seems to find stuff. Wouldn't surprise me if plenty of folks follow Daniel Lemire on Github as his stuff is always interesting."],
        "id":"c4f714b0-0dcd-4747-9002-369ede81d6d8",
        "url_text":"JSON is everywhere on the Internet. Servers spend a *lot* of time parsing it. We need a fresh approach. The simdjson library uses commonly available SIMD instructions and microparallel algorithms to parse JSON 4x faster than RapidJSON and 25x faster than JSON for Modern C++. Fast: Over 4x faster than commonly used production-grade JSON parsers. Record Breaking Features: Minify JSON at 6 GB/s, validate UTF-8 at 13 GB/s, NDJSON at 3.5 GB/s. Easy: First-class, easy to use and carefully documented APIs. Strict: Full JSON and UTF-8 validation, lossless parsing. Performance with no compromises. Automatic: Selects a CPU-tailored parser at runtime. No configuration needed. Reliable: From memory allocation to error handling, simdjson's design avoids surprises. Peer Reviewed: Our research appears in venues like VLDB Journal, Software: Practice and Experience. This library is part of the Awesome Modern C++ list. Table of Contents Quick Start Documentation Performance results Real-world usage Bindings and Ports of simdjson About simdjson Funding Contributing to simdjson License Quick Start The simdjson library is easily consumable with a single .h and .cpp file. Prerequisites: g++ (version 7 or better) or clang++ (version 6 or better), and a 64-bit system with a command-line shell (e.g., Linux, macOS, freeBSD). We also support programming environments like Visual Studio and Xcode, but different steps are needed. Pull simdjson.h and simdjson.cpp into a directory, along with the sample file twitter.json. wget https://raw.githubusercontent.com/simdjson/simdjson/master/singleheader/simdjson.h https://raw.githubusercontent.com/simdjson/simdjson/master/singleheader/simdjson.cpp https://raw.githubusercontent.com/simdjson/simdjson/master/jsonexamples/twitter.json Create quickstart.cpp: #include <iostream> #include \"simdjson.h\" using namespace simdjson; int main(void) { ondemand::parser parser; padded_string json = padded_string::load(\"twitter.json\"); ondemand::document tweets = parser.iterate(json); std::cout << uint64_t(tweets[\"search_metadata\"][\"count\"]) << \" results.\" << std::endl; } c++ -o quickstart quickstart.cpp simdjson.cpp ./quickstart Documentation Usage documentation is available: Basics is an overview of how to use simdjson and its APIs. Performance shows some more advanced scenarios and how to tune for them. Implementation Selection describes runtime CPU detection and how you can work with it. API contains the automatically generated API documentation. Performance results The simdjson library uses three-quarters less instructions than state-of-the-art parser RapidJSON. To our knowledge, simdjson is the first fully-validating JSON parser to run at gigabytes per second (GB/s) on commodity processors. It can parse millions of JSON documents per second on a single core. The following figure represents parsing speed in GB/s for parsing various files on an Intel Skylake processor (3.4 GHz) using the GNU GCC 10 compiler (with the -O3 flag). We compare against the best and fastest C++ libraries on benchmarks that load and process the data. The simdjson library offers full unicode (UTF-8) validation and exact number parsing. The simdjson library offers high speed whether it processes tiny files (e.g., 300 bytes) or larger files (e.g., 3MB). The following plot presents parsing speed for synthetic files over various sizes generated with a script on a 3.4 GHz Skylake processor (GNU GCC 9, -O3). All our experiments are reproducible. For NDJSON files, we can exceed 3 GB/s with our multithreaded parsing functions. Real-world usage Microsoft FishStore Yandex ClickHouse Clang Build Analyzer Shopify HeapProfiler If you are planning to use simdjson in a product, please work from one of our releases. Bindings and Ports of simdjson We distinguish between \"bindings\" (which just wrap the C++ code) and a port to another programming language (which reimplements everything). ZippyJSON: Swift bindings for the simdjson project. libpy_simdjson: high-speed Python bindings for simdjson using libpy. pysimdjson: Python bindings for the simdjson project. cysimdjson: high-speed Python bindings for the simdjson project. simdjson-rs: Rust port. simdjson-rust: Rust wrapper (bindings). SimdJsonSharp: C# version for .NET Core (bindings and full port). simdjson_nodejs: Node.js bindings for the simdjson project. simdjson_php: PHP bindings for the simdjson project. simdjson_ruby: Ruby bindings for the simdjson project. fast_jsonparser: Ruby bindings for the simdjson project. simdjson-go: Go port using Golang assembly. rcppsimdjson: R bindings. simdjson_erlang: erlang bindings. lua-simdjson: lua bindings. About simdjson The simdjson library takes advantage of modern microarchitectures, parallelizing with SIMD vector instructions, reducing branch misprediction, and reducing data dependency to take advantage of each CPU's multiple execution cores. Some people enjoy reading our paper: A description of the design and implementation of simdjson is in our research article: Geoff Langdale, Daniel Lemire, Parsing Gigabytes of JSON per Second, VLDB Journal 28 (6), 2019. We have an in-depth paper focused on the UTF-8 validation: John Keiser, Daniel Lemire, Validating UTF-8 In Less Than One Instruction Per Byte, Software: Practice & Experience 51 (5), 2021. We also have an informal blog post providing some background and context. For the video inclined, (It was the best voted talk, we're kinda proud of it.) Funding The work is supported by the Natural Sciences and Engineering Research Council of Canada under grant number RGPIN-2017-03910. Contributing to simdjson Head over to CONTRIBUTING.md for information on contributing to simdjson, and HACKING.md for information on source, building, and architecture/design. License This code is made available under the Apache License 2.0. Under Windows, we build some tools using the windows/dirent_portable.h file (which is outside our library code): it under the liberal (business-friendly) MIT license. For compilers that do not support C++17, we bundle the string-view library which is published under the Boost license. Like the Apache license, the Boost license is a permissive license allowing commercial redistribution. For efficient number serialization, we bundle Florian Loitsch's implementation of the Grisu2 algorithm for binary to decimal floating-point numbers. The implementation was slightly modified by JSON for Modern C++ library. Both Florian Loitsch's implementation and JSON for Modern C++ are provided under the MIT license. For runtime dispatching, we use some code from the PyTorch project licensed under 3-clause BSD. ",
        "_version_":1718938149115133952},
      {
        "story_id":20627137,
        "story_author":"cyrieu",
        "story_descendants":46,
        "story_score":128,
        "story_time":"2019-08-06T17:04:09Z",
        "story_title":"Launch HN: Lang (YC S19) – Internationalization Built for Devs",
        "search":["Launch HN: Lang (YC S19) – Internationalization Built for Devs",
          "Hey HN! We’re Eric, Peter, and Abhi, founders of Lang (<a href=\"https://www.langapi.co\" rel=\"nofollow\">https://www.langapi.co</a>). We help developers quickly translate their apps into foreign languages by combining internationalization SDKs with a command-line interface that integrates directly with human translators.<p>Previously, we all worked on building internationalization and localization tooling for companies. In our experience, companies don’t think about translation until too late, and the tech debt builds up very fast. It’s a nightmare to receive a task that says “translate app into Spanish.” Choosing the right open-source framework, refactoring the entire codebase, and integrating with human translators is a massive effort. As engineers, we wanted to work on features - not putting every string in our codebase into a translations.json file. In our months of internationalization work, we couldn’t find a good all-in-one toolkit. So we built Lang.<p>Like other internationalization libraries, Lang gives you a tr() function. Wrap your strings with tr(), and we’ll show your users translations that correspond to their language settings at run-time. But how do you actually get the translations? Open-source frameworks like Polyglot.js stop here, but Lang doesn’t. Run “push,” and our command-line tool will parse your code files, find tr() calls, collect newly added strings, and send them to human translators for you. For JavaScript, we use Babel to construct an Abstract Syntax Tree (AST) of your code, and traverse the tree to find tr()’d strings. For a developer, this makes it simple to add/remove/update strings: just run “push” in your terminal. You can track the status of your translations on our dashboard, and when they’re done just run “pull.” We’ll generate a translation file for you, and connect it with our tr() function. You own the file - Lang doesn’t make any network requests for translations at run-time, and your translations always load, even if our service is down.<p>This works for static strings in the code, but what about dynamic content in the backend or database? We expose a function called liveTr(), which takes a string argument. The first time liveTr() sees an untranslated string, it will make a request to Lang to translate it and return the string in its original language. But the next time, it will fetch the translation on-demand. We’ve shipped liveTr() with built-in caching functionality to reduce the number of network requests. We also have self-hosted solutions for users with high uptime requirements. This is a common in-house feature companies build for internationalization, and we want to make it available to all devs.<p>Lang currently supports JavaScript and Typescript apps (React, React Native, Vue etc.) with closed betas for Django, Android, and iOS. Give us a try at <a href=\"https://www.langapi.co/signup\" rel=\"nofollow\">https://www.langapi.co/signup</a> - machine translations are free, so you can see your app in another language in minutes. If you use human translations, we charge $99 / month for our tooling, and 6-8 cents per word translated. A lot of our work is inspired by open-source, and we want to give back - if you’re building an open-source project or non-profit, ping us at eric@langapi.co. We’ll drop the monthly fee :)<p>The HN community builds amazing products, and we’re sure there are plenty of people here who have translated their apps - we’d love to hear your experiences in this area and your feedback on how we can improve!",
          "LanchHN",
          "I like the idea, but you are putting more weight behind the tooling than I would. I don't find translation tooling to be cumbersome, so especially if machine translations are free, I don't find the price point for human translations to be compelling.<p>What would be compelling is if you could pro-actively call out the bigger gotchas in translation - grammatical differences that make you change word orders, different mechanism for handling plurals, etc. If you could preemptively warn us, even before a \"push\" that we may hit a problem, I'd take a closer look. For example, flagging a line saying, \"Hey, it looks like you are using phrasing that will be problematic in <Italian/Hindi/Russian/etc.> Here's why...\"",
          "We needed recently translate our chrome extension (languagelearningwithnetflix.com) into 18 languages. We are poor and had less than $1000 for the job.<p>Here's our approach.<p>1. Move all strings into a Google doc. Takes about 8 hours.<p>2. Organise strings into groups with screenshots, think carefully, split strings, look for reused strings, reword things to make them simpler and easier to translate, add notes to some strings to make the meaning more explicit. Very tedious part of job, 2-3 days work.<p>3. Put the doc, editable with link, onto upwork with a fixed payment, somewhat generous for translation wordcount. Check translator is a native in the target language, had some good feedback and ideally some IT/programming experience. Order translations for the languages we can check ourselves (5-6 languages).<p>4. Check the translations received for issues. Translators typically misinterpret the same things, as the source was not clear enough. Fix these issues. Maybe 4 hours work.<p>5. Now send to 10+ other translators for the languages we don't understand. Cross fingers that these will be ok.<p>6. Check translations of labels for homogenous usage of semicolons, capitals, fullstops etc. Struggle with zh/ja/ko.<p>7. Use a small JS script to transform CSV output from sheets to JSON for chrome.i18n.<p>8. Cycle through all locales and for overflowing text or other issues.<p>9. For any extra strings that we might need later, can try Microsoft UI translations database, or else, Google translate (which is mostly ok, can check the reverse translation).<p>Honestly this all was quite a lot of boring work, but we probably ended up with reasonable translations at a good price, and managed to pay translators reasonable money."],
        "story_text":"Hey HN! We’re Eric, Peter, and Abhi, founders of Lang (<a href=\"https://www.langapi.co\" rel=\"nofollow\">https://www.langapi.co</a>). We help developers quickly translate their apps into foreign languages by combining internationalization SDKs with a command-line interface that integrates directly with human translators.<p>Previously, we all worked on building internationalization and localization tooling for companies. In our experience, companies don’t think about translation until too late, and the tech debt builds up very fast. It’s a nightmare to receive a task that says “translate app into Spanish.” Choosing the right open-source framework, refactoring the entire codebase, and integrating with human translators is a massive effort. As engineers, we wanted to work on features - not putting every string in our codebase into a translations.json file. In our months of internationalization work, we couldn’t find a good all-in-one toolkit. So we built Lang.<p>Like other internationalization libraries, Lang gives you a tr() function. Wrap your strings with tr(), and we’ll show your users translations that correspond to their language settings at run-time. But how do you actually get the translations? Open-source frameworks like Polyglot.js stop here, but Lang doesn’t. Run “push,” and our command-line tool will parse your code files, find tr() calls, collect newly added strings, and send them to human translators for you. For JavaScript, we use Babel to construct an Abstract Syntax Tree (AST) of your code, and traverse the tree to find tr()’d strings. For a developer, this makes it simple to add/remove/update strings: just run “push” in your terminal. You can track the status of your translations on our dashboard, and when they’re done just run “pull.” We’ll generate a translation file for you, and connect it with our tr() function. You own the file - Lang doesn’t make any network requests for translations at run-time, and your translations always load, even if our service is down.<p>This works for static strings in the code, but what about dynamic content in the backend or database? We expose a function called liveTr(), which takes a string argument. The first time liveTr() sees an untranslated string, it will make a request to Lang to translate it and return the string in its original language. But the next time, it will fetch the translation on-demand. We’ve shipped liveTr() with built-in caching functionality to reduce the number of network requests. We also have self-hosted solutions for users with high uptime requirements. This is a common in-house feature companies build for internationalization, and we want to make it available to all devs.<p>Lang currently supports JavaScript and Typescript apps (React, React Native, Vue etc.) with closed betas for Django, Android, and iOS. Give us a try at <a href=\"https://www.langapi.co/signup\" rel=\"nofollow\">https://www.langapi.co/signup</a> - machine translations are free, so you can see your app in another language in minutes. If you use human translations, we charge $99 / month for our tooling, and 6-8 cents per word translated. A lot of our work is inspired by open-source, and we want to give back - if you’re building an open-source project or non-profit, ping us at eric@langapi.co. We’ll drop the monthly fee :)<p>The HN community builds amazing products, and we’re sure there are plenty of people here who have translated their apps - we’d love to hear your experiences in this area and your feedback on how we can improve!",
        "story_type":"LanchHN",
        "comments.comment_id":[20628097,
          20631231],
        "comments.comment_author":["codingdave",
          "davidzweig"],
        "comments.comment_descendants":[2,
          2],
        "comments.comment_time":["2019-08-06T18:36:28Z",
          "2019-08-07T01:21:20Z"],
        "comments.comment_text":["I like the idea, but you are putting more weight behind the tooling than I would. I don't find translation tooling to be cumbersome, so especially if machine translations are free, I don't find the price point for human translations to be compelling.<p>What would be compelling is if you could pro-actively call out the bigger gotchas in translation - grammatical differences that make you change word orders, different mechanism for handling plurals, etc. If you could preemptively warn us, even before a \"push\" that we may hit a problem, I'd take a closer look. For example, flagging a line saying, \"Hey, it looks like you are using phrasing that will be problematic in <Italian/Hindi/Russian/etc.> Here's why...\"",
          "We needed recently translate our chrome extension (languagelearningwithnetflix.com) into 18 languages. We are poor and had less than $1000 for the job.<p>Here's our approach.<p>1. Move all strings into a Google doc. Takes about 8 hours.<p>2. Organise strings into groups with screenshots, think carefully, split strings, look for reused strings, reword things to make them simpler and easier to translate, add notes to some strings to make the meaning more explicit. Very tedious part of job, 2-3 days work.<p>3. Put the doc, editable with link, onto upwork with a fixed payment, somewhat generous for translation wordcount. Check translator is a native in the target language, had some good feedback and ideally some IT/programming experience. Order translations for the languages we can check ourselves (5-6 languages).<p>4. Check the translations received for issues. Translators typically misinterpret the same things, as the source was not clear enough. Fix these issues. Maybe 4 hours work.<p>5. Now send to 10+ other translators for the languages we don't understand. Cross fingers that these will be ok.<p>6. Check translations of labels for homogenous usage of semicolons, capitals, fullstops etc. Struggle with zh/ja/ko.<p>7. Use a small JS script to transform CSV output from sheets to JSON for chrome.i18n.<p>8. Cycle through all locales and for overflowing text or other issues.<p>9. For any extra strings that we might need later, can try Microsoft UI translations database, or else, Google translate (which is mostly ok, can check the reverse translation).<p>Honestly this all was quite a lot of boring work, but we probably ended up with reasonable translations at a good price, and managed to pay translators reasonable money."],
        "id":"98a320fd-8609-4ea4-a310-6e39c539e31a",
        "_version_":1718938205911252992},
      {
        "story_id":20993747,
        "story_author":"arzzen",
        "story_descendants":9,
        "story_score":120,
        "story_time":"2019-09-17T11:29:15Z",
        "story_title":"Show HN: Statistical tool for analyzing a Git repository",
        "search":["Show HN: Statistical tool for analyzing a Git repository",
          "ShowHN",
          "https://github.com/arzzen/git-quick-stats/",
          "GIT quick statistics git-quick-stats is a simple and efficient way to access various statistics in a git repository. Any git repository may contain tons of information about commits, contributors, and files. Extracting this information is not always trivial, mostly because there are a gadzillion options to a gadzillion git commands I dont think there is a single person alive who knows them all. Probably not even Linus Torvalds himself :). Table of Contents Screenshots Usage Interactive Non-interactive Command-line arguments Git log since and until Git log limit Git log options Git pathspec Git merge view strategy Color themes Installation UNIX and Linux macOS Windows Docker System requirements Dependencies FAQ Contribution Code reviews Some tips for good pull requests Formatting Tests Licensing Contributors Backers Sponsors Screenshots Usage Interactive git-quick-stats has a built-in interactive menu that can be executed as such: Or Non-interactive For those who prefer to utilize command-line options, git-quick-stats also has a non-interactive mode supporting both short and long options: git-quick-stats <optional-command-to-execute-directly> Or git quick-stats <optional-command-to-execute-directly> Command-line arguments Possible arguments in short and long form: GENERATE OPTIONS -T, --detailed-git-stats give a detailed list of git stats -R, --git-stats-by-branch see detailed list of git stats by branch -c, --changelogs see changelogs -L, --changelogs-by-author see changelogs by author -S, --my-daily-stats see your current daily stats -V, --csv-output-by-branch output daily stats by branch in CSV format -j, --json-output save git log as a JSON formatted file to a specified area LIST OPTIONS -b, --branch-tree show an ASCII graph of the git repo branch history -D, --branches-by-date show branches by date -C, --contributors see a list of everyone who contributed to the repo -a, --commits-per-author displays a list of commits per author -d, --commits-per-day displays a list of commits per day -m, --commits-by-month displays a list of commits per month -w, --commits-by-weekday displays a list of commits per weekday -o, --commits-by-hour displays a list of commits per hour -A, --commits-by-author-by-hour displays a list of commits per hour by author -z, --commits-by-timezone displays a list of commits per timezone -Z, --commits-by-author-by-timezone displays a list of commits per timezone by author SUGGEST OPTIONS -r, --suggest-reviewers show the best people to contact to review code -h, -?, --help display this help text in the terminal Git log since and until You can set the variables _GIT_SINCE and/or _GIT_UNTIL before running git-quick-stats to limit the git log. These work similar to git's built-in --since and --until log options. export _GIT_SINCE=\"2017-01-20\" export _GIT_UNTIL=\"2017-01-22\" Once set, run git quick-stats as normal. Note that this affects all stats that parse the git log history until unset. Git log limit You can set variable _GIT_LIMIT for limited output. It will affect the \"changelogs\" and \"branch tree\" options. Git log options You can set _GIT_LOG_OPTIONS for git log options: export _GIT_LOG_OPTIONS=\"--ignore-all-space --ignore-blank-lines\" Git pathspec You can exclude a directory from the stats by using pathspec export _GIT_PATHSPEC=':!directory' You can also exclude files from the stats. Note that it works with any alphanumeric, glob, or regex that git respects. export _GIT_PATHSPEC=':!package-lock.json' Git merge view strategy You can set the variable _GIT_MERGE_VIEW to enable merge commits to be part of the stats by setting _GIT_MERGE_VIEW to enable. You can also choose to only show merge commits by setting _GIT_MERGE_VIEW to exclusive. Default is to not show merge commits. These work similar to git's built-in --merges and --no-merges log options. export _GIT_MERGE_VIEW=\"enable\" export _GIT_MERGE_VIEW=\"exclusive\" Git branch You can set the variable _GIT_BRANCH to set the branch of the stats. Works with commands --git-stats-by-branch and --csv-output-by-branch. export _GIT_BRANCH=\"master\" Color themes You can change to the legacy color scheme by toggling the variable _MENU_THEME between default and legacy export _MENU_THEME=\"legacy\" Installation Debian and Ubuntu If you are on at least Debian Bullseye or Ubuntu Focal you can use apt for installation: apt install git-quick-stats UNIX and Linux git clone https://github.com/arzzen/git-quick-stats.git && cd git-quick-stats sudo make install For uninstalling, open up the cloned directory and run For update/reinstall macOS (homebrew) brew install git-quick-stats Or you can follow the UNIX and Linux instructions if you wish. Windows If you are installing with Cygwin, use these scripts: installer uninstaller If you are wishing to use this with WSL, follow the UNIX and Linux instructions. Docker You can use the Docker image provided: Build: docker build -t arzzen/git-quick-stats . Run interactive menu: docker run --rm -it -v $(pwd):/git arzzen/git-quick-stats Docker pull command: docker pull arzzen/git-quick-stats docker repository System requirements An OS with a Bash shell Tools we use: awk basename cat column echo git grep head printf seq sort tput tr uniq wc Dependencies bsdmainutils apt install bsdmainutils FAQ Q: I get some errors after run git-quick-stats in cygwin like /usr/local/bin/git-quick-stats: line 2: $'\\r': command not found A: You can run the dos2unix app in cygwin as follows: /bin/dos2unix.exe /usr/local/bin/git-quick-stats. This will convert the script from the CR-LF convention that Microsoft uses to the LF convention that UNIX, OS X, and Linux use. You should then should be able to run it as normal. Q: How they could be used in a project with many git projects and statistics would show a summary of all git projects? A: If you want to include submodule logs, you can try using the following: export _GIT_LOG_OPTIONS=\"-p --submodule=log\" (more info about git log --submodule) Contribution Want to contribute? Great! First, read this page. Code reviews All submissions, including submissions by project members, require review. We use GitHub pull requests for this purpose. Some tips for good pull requests Use our code When in doubt, try to stay true to the existing code of the project. Write a descriptive commit message. What problem are you solving and what are the consequences? Where and what did you test? Some good tips: here and here. If your PR consists of multiple commits which are successive improvements / fixes to your first commit, consider squashing them into a single commit (git rebase -i) such that your PR is a single commit on top of the current HEAD. This make reviewing the code so much easier, and our history more readable. Formatting This documentation is written using standard markdown syntax. Please submit your changes using the same syntax. Tests Licensing MIT see LICENSE for the full license text. Contributors This project exists thanks to all the people who contribute. Backers Thank you to all our backers! [Become a backer] Sponsors Support this project by becoming a sponsor. Your logo will show up here with a link to your website. [Become a sponsor] ",
          "Pull Panda [0] is another tool we've been using that is offered as a SaaS and is now free since it was acquired by Github this year. It tells you average PR review time, average PR diff size, who is most requested for review, review-comment ratio, etc. I can't believe it's taken Github so long to make progress on dashboards like this for engineering managers, but looking forward to the time Pull Panda is fully integrated.<p>[0] <a href=\"https://pullreminders.com\" rel=\"nofollow\">https://pullreminders.com</a>",
          "I made a tool myself [0] with slightly different tools. It answers two questions: Who are the relevant coders and what parts of the code are the hotspots?<p>Here is an example run on sqlite:<p><pre><code>    Top Committers (of 28 authors):\n    D. Richard Hipp      13359 commits during 19 years until 2019-09-17\n    Dan Kennedy          5813 commits during 17 years until 2019-09-16\n     together these authors have 80+% of the commits (19172/20987)\n\n    Files with most commits:\n    1143 commits: src/sqlite.h.in      during 19 years until 2019-09-16\n    1331 commits: src/where.c          during 19 years until 2019-09-03\n    1360 commits: src/btree.c          during 18 years until 2019-08-24\n    1650 commits: src/vdbe.c           during 19 years until 2019-09-16\n    1893 commits: src/sqliteInt.h      during 19 years until 2019-09-14\n\n    Files with most authors:\n    11 authors: src/main.c          \n    11 authors: src/sqliteInt.h     \n    12 authors: configure.ac        \n    12 authors: src/shell.c         \n    15 authors: Makefile.in         \n\n    By file extension:\n    .test: 1333 files\n       .c: 379 files\n     together these make up 80+% of the files (1712/2138)\n</code></pre>\n[0] <a href=\"https://github.com/qznc/dot/blob/master/bin/git-overview\" rel=\"nofollow\">https://github.com/qznc/dot/blob/master/bin/git-overview</a>"],
        "story_type":"ShowHN",
        "url_raw":"https://github.com/arzzen/git-quick-stats/",
        "comments.comment_id":[20994452,
          20999219],
        "comments.comment_author":["eatonphil",
          "qznc"],
        "comments.comment_descendants":[0,
          0],
        "comments.comment_time":["2019-09-17T12:59:08Z",
          "2019-09-17T19:36:32Z"],
        "comments.comment_text":["Pull Panda [0] is another tool we've been using that is offered as a SaaS and is now free since it was acquired by Github this year. It tells you average PR review time, average PR diff size, who is most requested for review, review-comment ratio, etc. I can't believe it's taken Github so long to make progress on dashboards like this for engineering managers, but looking forward to the time Pull Panda is fully integrated.<p>[0] <a href=\"https://pullreminders.com\" rel=\"nofollow\">https://pullreminders.com</a>",
          "I made a tool myself [0] with slightly different tools. It answers two questions: Who are the relevant coders and what parts of the code are the hotspots?<p>Here is an example run on sqlite:<p><pre><code>    Top Committers (of 28 authors):\n    D. Richard Hipp      13359 commits during 19 years until 2019-09-17\n    Dan Kennedy          5813 commits during 17 years until 2019-09-16\n     together these authors have 80+% of the commits (19172/20987)\n\n    Files with most commits:\n    1143 commits: src/sqlite.h.in      during 19 years until 2019-09-16\n    1331 commits: src/where.c          during 19 years until 2019-09-03\n    1360 commits: src/btree.c          during 18 years until 2019-08-24\n    1650 commits: src/vdbe.c           during 19 years until 2019-09-16\n    1893 commits: src/sqliteInt.h      during 19 years until 2019-09-14\n\n    Files with most authors:\n    11 authors: src/main.c          \n    11 authors: src/sqliteInt.h     \n    12 authors: configure.ac        \n    12 authors: src/shell.c         \n    15 authors: Makefile.in         \n\n    By file extension:\n    .test: 1333 files\n       .c: 379 files\n     together these make up 80+% of the files (1712/2138)\n</code></pre>\n[0] <a href=\"https://github.com/qznc/dot/blob/master/bin/git-overview\" rel=\"nofollow\">https://github.com/qznc/dot/blob/master/bin/git-overview</a>"],
        "id":"985667d2-c5c4-4000-98b5-c7b9c52d0ba1",
        "url_text":"GIT quick statistics git-quick-stats is a simple and efficient way to access various statistics in a git repository. Any git repository may contain tons of information about commits, contributors, and files. Extracting this information is not always trivial, mostly because there are a gadzillion options to a gadzillion git commands I dont think there is a single person alive who knows them all. Probably not even Linus Torvalds himself :). Table of Contents Screenshots Usage Interactive Non-interactive Command-line arguments Git log since and until Git log limit Git log options Git pathspec Git merge view strategy Color themes Installation UNIX and Linux macOS Windows Docker System requirements Dependencies FAQ Contribution Code reviews Some tips for good pull requests Formatting Tests Licensing Contributors Backers Sponsors Screenshots Usage Interactive git-quick-stats has a built-in interactive menu that can be executed as such: Or Non-interactive For those who prefer to utilize command-line options, git-quick-stats also has a non-interactive mode supporting both short and long options: git-quick-stats <optional-command-to-execute-directly> Or git quick-stats <optional-command-to-execute-directly> Command-line arguments Possible arguments in short and long form: GENERATE OPTIONS -T, --detailed-git-stats give a detailed list of git stats -R, --git-stats-by-branch see detailed list of git stats by branch -c, --changelogs see changelogs -L, --changelogs-by-author see changelogs by author -S, --my-daily-stats see your current daily stats -V, --csv-output-by-branch output daily stats by branch in CSV format -j, --json-output save git log as a JSON formatted file to a specified area LIST OPTIONS -b, --branch-tree show an ASCII graph of the git repo branch history -D, --branches-by-date show branches by date -C, --contributors see a list of everyone who contributed to the repo -a, --commits-per-author displays a list of commits per author -d, --commits-per-day displays a list of commits per day -m, --commits-by-month displays a list of commits per month -w, --commits-by-weekday displays a list of commits per weekday -o, --commits-by-hour displays a list of commits per hour -A, --commits-by-author-by-hour displays a list of commits per hour by author -z, --commits-by-timezone displays a list of commits per timezone -Z, --commits-by-author-by-timezone displays a list of commits per timezone by author SUGGEST OPTIONS -r, --suggest-reviewers show the best people to contact to review code -h, -?, --help display this help text in the terminal Git log since and until You can set the variables _GIT_SINCE and/or _GIT_UNTIL before running git-quick-stats to limit the git log. These work similar to git's built-in --since and --until log options. export _GIT_SINCE=\"2017-01-20\" export _GIT_UNTIL=\"2017-01-22\" Once set, run git quick-stats as normal. Note that this affects all stats that parse the git log history until unset. Git log limit You can set variable _GIT_LIMIT for limited output. It will affect the \"changelogs\" and \"branch tree\" options. Git log options You can set _GIT_LOG_OPTIONS for git log options: export _GIT_LOG_OPTIONS=\"--ignore-all-space --ignore-blank-lines\" Git pathspec You can exclude a directory from the stats by using pathspec export _GIT_PATHSPEC=':!directory' You can also exclude files from the stats. Note that it works with any alphanumeric, glob, or regex that git respects. export _GIT_PATHSPEC=':!package-lock.json' Git merge view strategy You can set the variable _GIT_MERGE_VIEW to enable merge commits to be part of the stats by setting _GIT_MERGE_VIEW to enable. You can also choose to only show merge commits by setting _GIT_MERGE_VIEW to exclusive. Default is to not show merge commits. These work similar to git's built-in --merges and --no-merges log options. export _GIT_MERGE_VIEW=\"enable\" export _GIT_MERGE_VIEW=\"exclusive\" Git branch You can set the variable _GIT_BRANCH to set the branch of the stats. Works with commands --git-stats-by-branch and --csv-output-by-branch. export _GIT_BRANCH=\"master\" Color themes You can change to the legacy color scheme by toggling the variable _MENU_THEME between default and legacy export _MENU_THEME=\"legacy\" Installation Debian and Ubuntu If you are on at least Debian Bullseye or Ubuntu Focal you can use apt for installation: apt install git-quick-stats UNIX and Linux git clone https://github.com/arzzen/git-quick-stats.git && cd git-quick-stats sudo make install For uninstalling, open up the cloned directory and run For update/reinstall macOS (homebrew) brew install git-quick-stats Or you can follow the UNIX and Linux instructions if you wish. Windows If you are installing with Cygwin, use these scripts: installer uninstaller If you are wishing to use this with WSL, follow the UNIX and Linux instructions. Docker You can use the Docker image provided: Build: docker build -t arzzen/git-quick-stats . Run interactive menu: docker run --rm -it -v $(pwd):/git arzzen/git-quick-stats Docker pull command: docker pull arzzen/git-quick-stats docker repository System requirements An OS with a Bash shell Tools we use: awk basename cat column echo git grep head printf seq sort tput tr uniq wc Dependencies bsdmainutils apt install bsdmainutils FAQ Q: I get some errors after run git-quick-stats in cygwin like /usr/local/bin/git-quick-stats: line 2: $'\\r': command not found A: You can run the dos2unix app in cygwin as follows: /bin/dos2unix.exe /usr/local/bin/git-quick-stats. This will convert the script from the CR-LF convention that Microsoft uses to the LF convention that UNIX, OS X, and Linux use. You should then should be able to run it as normal. Q: How they could be used in a project with many git projects and statistics would show a summary of all git projects? A: If you want to include submodule logs, you can try using the following: export _GIT_LOG_OPTIONS=\"-p --submodule=log\" (more info about git log --submodule) Contribution Want to contribute? Great! First, read this page. Code reviews All submissions, including submissions by project members, require review. We use GitHub pull requests for this purpose. Some tips for good pull requests Use our code When in doubt, try to stay true to the existing code of the project. Write a descriptive commit message. What problem are you solving and what are the consequences? Where and what did you test? Some good tips: here and here. If your PR consists of multiple commits which are successive improvements / fixes to your first commit, consider squashing them into a single commit (git rebase -i) such that your PR is a single commit on top of the current HEAD. This make reviewing the code so much easier, and our history more readable. Formatting This documentation is written using standard markdown syntax. Please submit your changes using the same syntax. Tests Licensing MIT see LICENSE for the full license text. Contributors This project exists thanks to all the people who contribute. Backers Thank you to all our backers! [Become a backer] Sponsors Support this project by becoming a sponsor. Your logo will show up here with a link to your website. [Become a sponsor] ",
        "_version_":1718938220544131072},
      {
        "story_id":19074170,
        "story_author":"nosarthur",
        "story_descendants":36,
        "story_score":79,
        "story_time":"2019-02-04T05:22:20Z",
        "story_title":"Show HN: Gita – a CLI tool to manage multiple Git repos",
        "search":["Show HN: Gita – a CLI tool to manage multiple Git repos",
          "ShowHN",
          "https://github.com/nosarthur/gita",
          "_______________________________ ( ____ \\__ __|__ __( ___ ) | ( \\/ ) ( ) ( | ( ) | | | | | | | | (___) | | | ____ | | | | | ___ | | | \\_ ) | | | | | ( ) | | (___) |__) (___ | | | ) ( | (_______)_______/ )_( |/ \\| v0.15 Gita: a command-line tool to manage multiple git repos This tool does two things display the status of multiple git repos such as branch, modification, commit message side by side (batch) delegate git commands/aliases from any working directory If several repos are related, it helps to see their status together. I also hate to change directories to execute git commands. In this screenshot, the gita ll command displays the status of all repos. The gita remote dotfiles command translates to git remote -v for the dotfiles repo, even though we are not in the repo. The gita fetch command fetches from all repos and two of them have updates. To see the pre-defined commands, run gita -h or take a look at cmds.json. To add your own commands, see the customization section. To run arbitrary git command, see the superman mode section. To run arbitrary shell command, see the shell mode section. The branch color distinguishes 5 situations between local and remote branches: color meaning white local has no remote green local is the same as remote red local has diverged from remote purple local is ahead of remote (good for push) yellow local is behind remote (good for merge) The choice of purple for ahead and yellow for behind is motivated by blueshift and redshift, using green as baseline. You can change the color scheme using the gita color command. See the customization section. The additional status symbols denote symbol meaning + staged changes * unstaged changes _ untracked files/folders The bookkeeping sub-commands are gita add <repo-path(s)> [-g <groupname>]: add repo(s) to gita, optionally into an existing group gita add -a <repo-parent-path(s)>: add repo(s) in <repo-parent-path(s)> recursively and automatically generate hierarchical groups. See the customization section for more details. gita add -b <bare-repo-path(s)>: add bare repo(s) to gita. See the customization section for more details on setting custom worktree. gita add -r <repo-parent-path(s)>: add repo(s) in <repo-parent-path(s)> recursively gita clone <config-file>: clone repos in config-file (generated by gita freeze) to current directory. gita clone -p <config-file>: clone repos in config-file to prescribed paths. gita context: context sub-command gita context: show current context gita context <group-name>: set context to group-name, all operations then only apply to repos in this group gita context auto: set context automatically according to the current working directory gita context none: remove context gita color: color sub-command gita color [ll]: Show available colors and the current coloring scheme gita color reset: Reset to the default coloring scheme gita color set <situation> <color>: Use the specified color for the local-remote situation gita flags: flags sub-command gita flags set <repo-name> <flags>: add custom flags to repo gita flags [ll]: display repos with custom flags gita freeze: print information of all repos such as URL, name, and path. Use with gita clone. gita group: group sub-command gita group add <repo-name(s)> -n <group-name>: add repo(s) to a new or existing group gita group [ll]: display existing groups with repos gita group ls: display existing group names gita group rename <group-name> <new-name>: change group name gita group rm <group-name(s)>: delete group(s) gita group rmrepo <repo-name(s)> -n <group-name>: remove repo(s) from existing group gita info: info sub-command gita info [ll]: display the used and unused information items gita info add <info-item>: enable information item gita info rm <info-item>: disable information item gita ll: display the status of all repos gita ll <group-name>: display the status of repos in a group gita ll -g: display the repo summaries by groups gita ls: display the names of all repos gita ls <repo-name>: display the absolute path of one repo gita rename <repo-name> <new-name>: rename a repo gita rm <repo-name(s)>: remove repo(s) from gita (won't remove files on disk) gita -v: display gita version The git delegating sub-commands are of two formats gita <sub-command> [repo-name(s) or group-name(s)]: optional repo or group input, and no input means all repos. gita <sub-command> <repo-name(s) or groups-name(s)>: required repo name(s) or group name(s) input They translate to git <sub-command> for the corresponding repos. By default, only fetch and pull take optional input. In other words, gita fetch and gita pull apply to all repos. To see the pre-defined sub-commands, run gita -h or take a look at cmds.json. To add your own sub-commands or override the default behaviors, see the customization section. To run arbitrary git command, see the superman mode section. If more than one repos are specified, the git command runs asynchronously, with the exception of log, difftool and mergetool, which require non-trivial user input. Repo configuration is saved in $XDG_CONFIG_HOME/gita/repos.csv (most likely ~/.config/gita/repos.csv). Installation To install the latest version, run If you prefer development mode, download the source code and run pip3 install -e <gita-source-folder> In either case, calling gita in terminal may not work, then put the following line in the .bashrc file. alias gita=\"python3 -m gita\" Windows users may need to enable the ANSI escape sequence in terminal for the branch color to work. See this stackoverflow post for details. Auto-completion Download .gita-completion.bash or .gita-completion.zsh and source it in shell. Superman mode The superman mode delegates any git command or alias. Usage: gita super [repo-name(s) or group-name(s)] <any-git-command-with-or-without-options> Here repo-name(s) or group-name(s) are optional, and their absence means all repos. For example, gita super checkout master puts all repos on the master branch gita super frontend-repo backend-repo commit -am 'implement a new feature' executes git commit -am 'implement a new feature' for frontend-repo and backend-repo Shell mode The shell mode delegates any shell command. Usage: gita shell [repo-name(s) or group-name(s)] <any-shell-command> Here repo-name(s) or group-name(s) are optional, and their absence means all repos. For example, gita shell ll lists contents for all repos gita shell repo1 repo2 mkdir docs create a new directory docs in repo1 and repo2 gita shell \"git describe --abbrev=0 --tags | xargs git checkout\": check out the latest tag for all repos Customization define repo group and context When the project contains several independent but related repos, we can define a group and execute gita command on this group. For example, gita group add repo1 repo2 -n my-group gita ll my-group gita pull my-group To save more typing, one can set a group as context, then any gita command is scoped to the group gita context my-group gita ll gita pull The most useful context maybe auto. In this mode, the context is automatically determined from the current working directory (CWD): the context is the group whose member repo's path contains CWD. To set it, run To remove the context, run It is also possible to recursively add repos within a directory and generate hierarchical groups automatically. For example, running on the following folder structure src project1 repo1 repo2 repo3 project2 repo4 repo5 repo6 gives rise to 3 groups: src:repo1,repo2,repo3,repo4,repo5,repo6 src-project1:repo1,repo2 src-project2:repo4,repo5 add user-defined sub-command using json file Custom delegating sub-commands can be defined in $XDG_CONFIG_HOME/gita/cmds.json (most likely ~/.config/gita/cmds.json) And they shadow the default ones if name collisions exist. Default delegating sub-commands are defined in cmds.json. For example, gita stat <repo-name(s)> is registered as \"stat\":{ \"cmd\": \"git diff --stat\", \"help\": \"show edit statistics\" } which executes git diff --stat for the specified repo(s). To disable asynchronous execution, set disable_async to be true. See the difftool example: \"difftool\":{ \"cmd\": \"git difftool\", \"disable_async\": true, \"help\": \"show differences using a tool\" } If you want a custom command to behave like gita fetch, i.e., to apply to all repos when no repo is specified, set allow_all to be true. For example, the following snippet creates a new command gita comaster [repo-name(s)] with optional repo name input. \"comaster\":{ \"cmd\": \"checkout master\", \"allow_all\": true, \"help\": \"checkout the master branch\" } Any command that runs in the superman mode mode or the shell mode can be defined in this json format. For example, the following command runs in shell mode and fetches only the current branch from upstream. \"fetchcrt\":{ \"cmd\": \"git rev-parse --abbrev-ref HEAD | xargs git fetch --prune upstream\", \"allow_all\": true, \"shell\": true, \"help\": \"fetch current branch only\" } customize the local/remote relationship coloring displayed by the gita ll command You can see the default color scheme and the available colors via gita color. To change the color coding, use gita color set <situation> <color>. The configuration is saved in $XDG_CONFIG_HOME/gita/color.csv. customize information displayed by the gita ll command You can customize the information displayed by gita ll. The used and unused information items are shown with gita info, and the configuration is saved in $XDG_CONFIG_HOME/gita/info.csv. For example, the default setting corresponds to branch,commit_msg,commit_time customize git command flags One can set custom flags to run git commands. For example, with gita flags set my-repo --git-dir=`gita ls dotfiles` --work-tree=$HOME any git command/alias triggered from gita on dotfiles will use these flags. Note that the flags are applied immediately after git. For example, gita st dotfiles translates to git --git-dir=$HOME/somefolder --work-tree=$HOME status running from the dotfiles directory. This feature was originally added to deal with bare repo dotfiles. Requirements Gita requires Python 3.6 or higher, due to the use of f-string and asyncio module. Under the hood, gita uses subprocess to run git commands/aliases. Thus the installed git version may matter. I have git 1.8.3.1, 2.17.2, and 2.20.1 on my machines, and their results agree. Tips effect shell command enter <repo> directory cd `gita ls <repo>` delete repos in <group> gita group ll <group> | xargs gita rm Contributing To contribute, you can report/fix bugs request/implement features star/recommend this project Read this article if you have never contribute code to open source project before. Chat room is available on To run tests locally, simply pytest in the source code folder. Note that context should be set as none. More implementation details are in design.md. A step-by-step guide to reproduce this project is here. You can also sponsor me on GitHub. Any amount is appreciated! Other multi-repo tools I haven't tried them but I heard good things about them. myrepos repo ",
          "One thing this project does really well is to start the readme with a screenshot. I open the link, scroll down to the readme, and I immediately see what sort of user interface/experience I will get. Some commenters have noted that Gita is similar to other multi-repo tools, but both Repo and wstool are more effort to evaluate, because their readmes don't have pictures.",
          "Nice, but is there a way to just run any command? I.e. just `gita <optional repo names/paths> <pass entire command line git -C repodir>`. This has advantages in that you don't have to go round via a command file, don't have to keep it synced across machines, don't have to remember what you put in the file etc, and can just use the git syntax which already took long enough to learn by heart :P<p>I've used multiple multiple repository tools and in the end all I happen to use is one (usually versioned) file to store a list of repositories and then a command which just loops over all repos and applies anything to it. If I need custom commands I use git aliases so that works both for normal git and whatever tool used."],
        "story_type":"ShowHN",
        "url_raw":"https://github.com/nosarthur/gita",
        "comments.comment_id":[19075272,
          19075476],
        "comments.comment_author":["fyhn",
          "stinos"],
        "comments.comment_descendants":[1,
          3],
        "comments.comment_time":["2019-02-04T10:43:55Z",
          "2019-02-04T11:34:37Z"],
        "comments.comment_text":["One thing this project does really well is to start the readme with a screenshot. I open the link, scroll down to the readme, and I immediately see what sort of user interface/experience I will get. Some commenters have noted that Gita is similar to other multi-repo tools, but both Repo and wstool are more effort to evaluate, because their readmes don't have pictures.",
          "Nice, but is there a way to just run any command? I.e. just `gita <optional repo names/paths> <pass entire command line git -C repodir>`. This has advantages in that you don't have to go round via a command file, don't have to keep it synced across machines, don't have to remember what you put in the file etc, and can just use the git syntax which already took long enough to learn by heart :P<p>I've used multiple multiple repository tools and in the end all I happen to use is one (usually versioned) file to store a list of repositories and then a command which just loops over all repos and applies anything to it. If I need custom commands I use git aliases so that works both for normal git and whatever tool used."],
        "id":"ed8d9197-eaf1-480e-a6a4-986f10ba95fd",
        "url_text":"_______________________________ ( ____ \\__ __|__ __( ___ ) | ( \\/ ) ( ) ( | ( ) | | | | | | | | (___) | | | ____ | | | | | ___ | | | \\_ ) | | | | | ( ) | | (___) |__) (___ | | | ) ( | (_______)_______/ )_( |/ \\| v0.15 Gita: a command-line tool to manage multiple git repos This tool does two things display the status of multiple git repos such as branch, modification, commit message side by side (batch) delegate git commands/aliases from any working directory If several repos are related, it helps to see their status together. I also hate to change directories to execute git commands. In this screenshot, the gita ll command displays the status of all repos. The gita remote dotfiles command translates to git remote -v for the dotfiles repo, even though we are not in the repo. The gita fetch command fetches from all repos and two of them have updates. To see the pre-defined commands, run gita -h or take a look at cmds.json. To add your own commands, see the customization section. To run arbitrary git command, see the superman mode section. To run arbitrary shell command, see the shell mode section. The branch color distinguishes 5 situations between local and remote branches: color meaning white local has no remote green local is the same as remote red local has diverged from remote purple local is ahead of remote (good for push) yellow local is behind remote (good for merge) The choice of purple for ahead and yellow for behind is motivated by blueshift and redshift, using green as baseline. You can change the color scheme using the gita color command. See the customization section. The additional status symbols denote symbol meaning + staged changes * unstaged changes _ untracked files/folders The bookkeeping sub-commands are gita add <repo-path(s)> [-g <groupname>]: add repo(s) to gita, optionally into an existing group gita add -a <repo-parent-path(s)>: add repo(s) in <repo-parent-path(s)> recursively and automatically generate hierarchical groups. See the customization section for more details. gita add -b <bare-repo-path(s)>: add bare repo(s) to gita. See the customization section for more details on setting custom worktree. gita add -r <repo-parent-path(s)>: add repo(s) in <repo-parent-path(s)> recursively gita clone <config-file>: clone repos in config-file (generated by gita freeze) to current directory. gita clone -p <config-file>: clone repos in config-file to prescribed paths. gita context: context sub-command gita context: show current context gita context <group-name>: set context to group-name, all operations then only apply to repos in this group gita context auto: set context automatically according to the current working directory gita context none: remove context gita color: color sub-command gita color [ll]: Show available colors and the current coloring scheme gita color reset: Reset to the default coloring scheme gita color set <situation> <color>: Use the specified color for the local-remote situation gita flags: flags sub-command gita flags set <repo-name> <flags>: add custom flags to repo gita flags [ll]: display repos with custom flags gita freeze: print information of all repos such as URL, name, and path. Use with gita clone. gita group: group sub-command gita group add <repo-name(s)> -n <group-name>: add repo(s) to a new or existing group gita group [ll]: display existing groups with repos gita group ls: display existing group names gita group rename <group-name> <new-name>: change group name gita group rm <group-name(s)>: delete group(s) gita group rmrepo <repo-name(s)> -n <group-name>: remove repo(s) from existing group gita info: info sub-command gita info [ll]: display the used and unused information items gita info add <info-item>: enable information item gita info rm <info-item>: disable information item gita ll: display the status of all repos gita ll <group-name>: display the status of repos in a group gita ll -g: display the repo summaries by groups gita ls: display the names of all repos gita ls <repo-name>: display the absolute path of one repo gita rename <repo-name> <new-name>: rename a repo gita rm <repo-name(s)>: remove repo(s) from gita (won't remove files on disk) gita -v: display gita version The git delegating sub-commands are of two formats gita <sub-command> [repo-name(s) or group-name(s)]: optional repo or group input, and no input means all repos. gita <sub-command> <repo-name(s) or groups-name(s)>: required repo name(s) or group name(s) input They translate to git <sub-command> for the corresponding repos. By default, only fetch and pull take optional input. In other words, gita fetch and gita pull apply to all repos. To see the pre-defined sub-commands, run gita -h or take a look at cmds.json. To add your own sub-commands or override the default behaviors, see the customization section. To run arbitrary git command, see the superman mode section. If more than one repos are specified, the git command runs asynchronously, with the exception of log, difftool and mergetool, which require non-trivial user input. Repo configuration is saved in $XDG_CONFIG_HOME/gita/repos.csv (most likely ~/.config/gita/repos.csv). Installation To install the latest version, run If you prefer development mode, download the source code and run pip3 install -e <gita-source-folder> In either case, calling gita in terminal may not work, then put the following line in the .bashrc file. alias gita=\"python3 -m gita\" Windows users may need to enable the ANSI escape sequence in terminal for the branch color to work. See this stackoverflow post for details. Auto-completion Download .gita-completion.bash or .gita-completion.zsh and source it in shell. Superman mode The superman mode delegates any git command or alias. Usage: gita super [repo-name(s) or group-name(s)] <any-git-command-with-or-without-options> Here repo-name(s) or group-name(s) are optional, and their absence means all repos. For example, gita super checkout master puts all repos on the master branch gita super frontend-repo backend-repo commit -am 'implement a new feature' executes git commit -am 'implement a new feature' for frontend-repo and backend-repo Shell mode The shell mode delegates any shell command. Usage: gita shell [repo-name(s) or group-name(s)] <any-shell-command> Here repo-name(s) or group-name(s) are optional, and their absence means all repos. For example, gita shell ll lists contents for all repos gita shell repo1 repo2 mkdir docs create a new directory docs in repo1 and repo2 gita shell \"git describe --abbrev=0 --tags | xargs git checkout\": check out the latest tag for all repos Customization define repo group and context When the project contains several independent but related repos, we can define a group and execute gita command on this group. For example, gita group add repo1 repo2 -n my-group gita ll my-group gita pull my-group To save more typing, one can set a group as context, then any gita command is scoped to the group gita context my-group gita ll gita pull The most useful context maybe auto. In this mode, the context is automatically determined from the current working directory (CWD): the context is the group whose member repo's path contains CWD. To set it, run To remove the context, run It is also possible to recursively add repos within a directory and generate hierarchical groups automatically. For example, running on the following folder structure src project1 repo1 repo2 repo3 project2 repo4 repo5 repo6 gives rise to 3 groups: src:repo1,repo2,repo3,repo4,repo5,repo6 src-project1:repo1,repo2 src-project2:repo4,repo5 add user-defined sub-command using json file Custom delegating sub-commands can be defined in $XDG_CONFIG_HOME/gita/cmds.json (most likely ~/.config/gita/cmds.json) And they shadow the default ones if name collisions exist. Default delegating sub-commands are defined in cmds.json. For example, gita stat <repo-name(s)> is registered as \"stat\":{ \"cmd\": \"git diff --stat\", \"help\": \"show edit statistics\" } which executes git diff --stat for the specified repo(s). To disable asynchronous execution, set disable_async to be true. See the difftool example: \"difftool\":{ \"cmd\": \"git difftool\", \"disable_async\": true, \"help\": \"show differences using a tool\" } If you want a custom command to behave like gita fetch, i.e., to apply to all repos when no repo is specified, set allow_all to be true. For example, the following snippet creates a new command gita comaster [repo-name(s)] with optional repo name input. \"comaster\":{ \"cmd\": \"checkout master\", \"allow_all\": true, \"help\": \"checkout the master branch\" } Any command that runs in the superman mode mode or the shell mode can be defined in this json format. For example, the following command runs in shell mode and fetches only the current branch from upstream. \"fetchcrt\":{ \"cmd\": \"git rev-parse --abbrev-ref HEAD | xargs git fetch --prune upstream\", \"allow_all\": true, \"shell\": true, \"help\": \"fetch current branch only\" } customize the local/remote relationship coloring displayed by the gita ll command You can see the default color scheme and the available colors via gita color. To change the color coding, use gita color set <situation> <color>. The configuration is saved in $XDG_CONFIG_HOME/gita/color.csv. customize information displayed by the gita ll command You can customize the information displayed by gita ll. The used and unused information items are shown with gita info, and the configuration is saved in $XDG_CONFIG_HOME/gita/info.csv. For example, the default setting corresponds to branch,commit_msg,commit_time customize git command flags One can set custom flags to run git commands. For example, with gita flags set my-repo --git-dir=`gita ls dotfiles` --work-tree=$HOME any git command/alias triggered from gita on dotfiles will use these flags. Note that the flags are applied immediately after git. For example, gita st dotfiles translates to git --git-dir=$HOME/somefolder --work-tree=$HOME status running from the dotfiles directory. This feature was originally added to deal with bare repo dotfiles. Requirements Gita requires Python 3.6 or higher, due to the use of f-string and asyncio module. Under the hood, gita uses subprocess to run git commands/aliases. Thus the installed git version may matter. I have git 1.8.3.1, 2.17.2, and 2.20.1 on my machines, and their results agree. Tips effect shell command enter <repo> directory cd `gita ls <repo>` delete repos in <group> gita group ll <group> | xargs gita rm Contributing To contribute, you can report/fix bugs request/implement features star/recommend this project Read this article if you have never contribute code to open source project before. Chat room is available on To run tests locally, simply pytest in the source code folder. Note that context should be set as none. More implementation details are in design.md. A step-by-step guide to reproduce this project is here. You can also sponsor me on GitHub. Any amount is appreciated! Other multi-repo tools I haven't tried them but I heard good things about them. myrepos repo ",
        "_version_":1718938144242401281},
      {
        "story_id":19855350,
        "story_author":"sandreas",
        "story_descendants":12,
        "story_score":48,
        "story_time":"2019-05-08T02:16:23Z",
        "story_title":"Show HN: M4b-tool – a tool to merge, split and chapterize audiobooks",
        "search":["Show HN: M4b-tool – a tool to merge, split and chapterize audiobooks",
          "ShowHN",
          "https://github.com/sandreas/m4b-tool",
          "m4b-tool m4b-tool is a is a wrapper for ffmpeg and mp4v2 to merge, split or and manipulate audiobook files with chapters. Although m4b-tool is designed to handle m4b files, nearly all audio formats should be supported, e.g. mp3, aac, ogg, alac and flac. Important Note Unfortunately I am pretty busy at the moment, so m4b-tool 0.4.2 is very old. Since it is not planned to release a newer version without having complete documentation, there is only the latest pre-release getting bug fixes. It is already pretty stable, so if you are experiencing bugs with v0.4.2, please try the latest pre-release, if it has been already fixed there. Thank you, sandreas https://pilabor.com Features merge a set of audio files (e.g. MP3 or AAC) into a single m4b file split a single m4b file into several output files by chapters or a flac encoded album into single tracks via cue sheet Add or adjust chapters for an existing m4b file via silence detection or musicbrainz TL;DR - examples for the most common tasks Merge multiple files merge all audio files in directory data/my-audio-book into file data/merged.m4b (tags are retained and data/my-audio-book/cover.jpg and data/my-audio-book/description.txt are embedded, if available) m4b-tool merge \"data/my-audio-book/\" --output-file=\"data/merged.m4b\" Split one file by chapters split one big m4b file by chapters into multiple mp3 files at data/my-audio-book_splitted/ (tags are retained, data/my-audio-book_splitted/cover.jpg is created, if m4b contains a cover) m4b-tool split --audio-format mp3 --audio-bitrate 96k --audio-channels 1 --audio-samplerate 22050 \"data/my-audio-book.m4b\" Chapters adjustment of a file via silence detection chapters can try to adjust existing chapters of an m4b by silence detection m4b-tool chapters --adjust-by-silence -o \"data/destination-with-adjusted-chapters.m4b\" \"data/source-with-misplaced-chapters.m4b\" Best practices Since the most used subcommand of m4b-tool seems to be merge, lets talk about best practice... Step 0 - Take a look at the docker image Unfortunately m4b-tool has many dependencies. Not only one-liners, if you would like to get the best quality and tagging support, many dependencies have to be compiled manually with extra options. Thats why you should take a look at the docker image, which comes with all the bells and whistles of top audio quality, top tagging support and easy installation and has almost no disadvantages. Note: If you are on windows, it might be difficult to make it work Step 1 - Organizing your audiobooks in directories When merging audiobooks, you should prepare them - the following directory structure helps a lot, even if you only merge one single audiobook: input/<main genre>/<author>/<title> or if it is a series input/<main genre>/<author>/<series>/<series-part> - <title> Examples: input/Fantasy/J.K. Rowling/Quidditch Through the Ages/ input/Fantasy/J.K. Rowling/Harry Potter/1 - Harry Potter and the Philosopher's Stone/ Note: If your audiobook title contains invalid path characters like /, just replace them with a dash -. Step 2 - add cover and a description Now, because you almost always want a cover and a description for your audiobook, you should add the following files in the main directory: cover.jpg description.txt (Be sure to use UTF-8 text file encoding for the contents) Examples: input/Fantasy/J.K. Rowling/Quidditch Through the Ages/cover.jpg input/Fantasy/J.K. Rowling/Quidditch Through the Ages/description.txt Note: m4b-tool will find and embed these files automatically but does not fail, if they are not present Step 3 - chapters Chapters are nice to add waypoints for your audiobook. They help to remember the last position and improve the experience in general. fixed chapters If you would like to adjust chapters manually, you can add a chapters.txt (same location as cover.jpg) with following contents (<chapter-start> <chapter-title>): 00:00:00.000 Intro 00:04:19.153 This is 00:09:24.078 A way to add 00:14:34.500 Chapters manually by tag If your input files are tagged, these tags will be used to create the chapter metadata by its title. So if you tag your input files with valid chapter names as track title, this will result in a nice and clean m4b-file with valid chapter names. by length Another great feature since m4b-tool v.0.4.0 is the --max-chapter-length parameter. Often the individual input files are too big which results in chapters with a very long duration. This can be annoying, if you would like to jump to a certain point, since you have to rewind or fast-forward and hold the button for a long time, instead of just tipping previous or next a few times. To automatically add sub-chapters, you could provide: --max-chapter-length=300,900 This will cause m4b-tool Trying to preserve original chapters as long as they are not longer than 15 minutes (900 seconds) If a track is longer than 15 minutes Perform a silence detection and try to add sub-chapters at every silence between 5 minutes (300 seconds) and 15 minutes (900 seconds) If no silence is detected, add a hard cut sub-chapter every 5 minutes Sub-chapters are named like the original and get an additional index. This is a nice way to keep the real names but not having chapters with a too long duration. Step 4 (optional) - for iPod owners If you own an iPod, there might be a problem with too long audiobooks, since iPods only support 32bit sampling rates. If your audiobook is longer than 27 hours with 22050Hz sampling rate, you could provide --adjust-for-ipod, to automatically downsample your audiobook, which results in lower quality, but at least its working on your good old iPod... Step 5 (optional) - more cpu cores, faster conversion m4b-tool supports multiple conversion tasks in parallel with the --jobs parameter (e.g. --jobs=2). If you have to convert more than one file, which is the common case, you nearly double the merge speed by providing the --jobs=2 parameter (or quadruplicate with --jobs=4, if you have a quad core system, etc.). Don't provide a number higher than the number of cores on your system - this will slow down the merge... Note: If you run the conversion on all your cores, it will result in almost 100% CPU usage, which may lead to slower system performance Step 6 - Use the --batch-pattern feature In m4b-tool v.0.4.0 the --batch-pattern feature was added. It can be used to batch-convert multiple audiobooks at once, but also to just convert one single audiobook - because you can create tags from an existing directory structure. Hint: The output-file parameter has to be a directory, when using --batch-pattern. Even multiple --batch-pattern parameters are supported, while the first match will be used first. So if you created the directory structure as described above, the final command to merge input/Fantasy/Harry Potter/1 - Harry Potter and the Philosopher's Stone/ to output/Fantasy/Harry Potter/1 - Harry Potter and the Philosopher's Stone.m4b would look like this: m4b-tool merge -v --jobs=2 --output-file=\"output/\" --max-chapter-length=300,900 --adjust-for-ipod --batch-pattern=\"input/%g/%a/%s/%p - %n/\" --batch-pattern=\"input/%g/%a/%n/\" \"input/\" In --batch-pattern mode, existing files are skipped by default Result If you performed the above steps with the docker image or installed and compiled all dependencies, you should get the following result: Top quality audio by using libfdk_aac encoder Series and single audiobooks have valid tags for genre, author, title, sorttitle, etc. from --batch-pattern usage If the files cover.jpg and description.txt exist in the main directories, a cover, a description and a longdesc are embedded If you tagged the input files, real chapter names should appear in your player No more chapters longer than 15 minutes Working iPod versions for audiobooks longer than 27 hours Installation Docker To use docker with m4b-tool, you first have to build a custom image located in the docker directory. Since this image is compiling every third party library from scratch to get the best possible audio quality, it can take a long time for the first build. Note: You should know that build does not mean that m4b-tool is being compiled from source. That indeed is strange, but unlike other projects, the m4b-tool docker image only downloads the latest binary release unless you do some extra work (see below). # clone m4b-tool repository git clone https://github.com/sandreas/m4b-tool.git # change directory cd m4b-tool # build docker image - this will take a while docker build . -t m4b-tool # create an alias for m4b-tool running docker alias m4b-tool='docker run -it --rm -u $(id -u):$(id -g) -v \"$(pwd)\":/mnt m4b-tool' # testing the command m4b-tool --version Note: If you use the alias above, keep in mind that you cannot use absolute paths (e.g. /tmp/data/audiobooks/harry potter 1) or symlinks. You must change into the directory and use relative paths (e.g. cd /tmp/data && m4b-tool merge \"audiobooks/harry potter 1\" --output-file harry.m4b) Dockerize a Pre-Release or an older release version To build a docker container using a Pre-Release or an older m4b-tool release, it is required to provide an extra parameter for downloading a specific version into the image, e.g. for v.0.4.1: docker build . --build-arg M4B_TOOL_DOWNLOAD_LINK=https://github.com/sandreas/m4b-tool/releases/download/v.0.4.1/m4b-tool.tar.gz -t m4b-tool Note: You could also just edit the according variable in the Dockerfile. Dockerize a custom build, that is not available via download link Developers or experts might want to run a complete custom build of m4b-tool or build the code themselves (e.g. if you forked the repository and applied some patches). If that is the case, you can store the custom build to dist/m4b-tool.phar relative to the Dockerfile and then do a default build. # dist/m4b-tool.phar is available docker build . -t m4b-tool After this the custom build should be integrated into the docker image. MacOS On MacOS you may use the awesome package manager brew to install m4b-tool. Recommended: High audio quality, sort tagging Getting best audio quality requires some additional effort. You have to recompile ffmpeg with the non-free libfdk_aac codec. This requires uninstalling the default ffmpeg package if installed, since brew dropped the possibility for extra options. There is no official ffmpeg-with-options repository, but a pretty decent tap, that you could use to save time. # FIRST INSTALL ONLY: if not already done, remove existing ffmpeg with default audio quality options # check for ffmpeg with libfdk and uninstall if libfdk is not already available [ -x \"$(which ffmpeg)\" ] && (ffmpeg -hide_banner -codecs 2>&1 | grep libfdk || brew uninstall ffmpeg) # tap required repositories brew tap sandreas/tap brew tap homebrew-ffmpeg/ffmpeg # check available ffmpeg options and which you would like to use brew options homebrew-ffmpeg/ffmpeg/ffmpeg # install ffmpeg with at least libfdk_aac for best audio quality brew install homebrew-ffmpeg/ffmpeg/ffmpeg --with-fdk-aac # install m4b-tool brew install sandreas/tap/m4b-tool # check installed m4b-tool version m4b-tool --version Stick to defaults (acceptable audio quality, no sort tagging) If the above did not work for you or you would just to checkout m4b-tool before using it in production, you might want to try the quick and easy way. It will work, but you get lower audio quality and there is no support for sort tagging. # tap m4b-tool repository brew tap sandreas/tap # install dependencies brew install ffmpeg fdk-aac-encoder mp4v2 # install m4b-tool with acceptable audio quality and no sort tagging brew install --ignore-dependencies sandreas/tap/m4b-tool Ubuntu # install all dependencies sudo apt install ffmpeg mp4v2-utils fdkaac php-cli php-intl php-json php-mbstring php-xml # install / upgrade m4b-tool sudo wget https://github.com/sandreas/m4b-tool/releases/download/v.0.4.2/m4b-tool.phar -O /usr/local/bin/m4b-tool && sudo chmod +x /usr/local/bin/m4b-tool # check installed m4b-tool version m4b-tool --version Note: If you would like to get the best possible audio quality, you have to compile ffmpeg with the high quality encoder fdk-aac (--enable-libfdk_aac) - see https://trac.ffmpeg.org/wiki/CompilationGuide/Ubuntu for a step-by-step guide to compile ffmpeg. Manual installation (only recommended on Windows systems) m4b-tool is written in PHP and uses ffmpeg, mp4v2 and optionally fdkaac for high efficiency codecs to perform conversions. Therefore you will need the following tools in your %PATH%: php >= 7.1 with mbstring extension enabled (https://php.net) ffmpeg (https://www.ffmpeg.org) mp4v2 (mp4chaps, mp4art, etc. https://github.com/sandreas/m4b-tool/releases/download/v0.2/mp4v2-windows.zip) fdkaac (optional, only if you need high efficiency for low bitrates <= 32k, http://wlc.io/2015/06/20/fdk-aac/ - caution: not official!) To check the dependencies, running following commands via command line should show similar output: $ php -v Copyright (c) 1997-2018 The PHP Group [...] $ ffmpeg -version ffmpeg version 4.1.1 Copyright (c) 2000-2019 the FFmpeg developers [...] $ mp4chaps --version mp4chaps - MP4v2 2.0.0 $ fdkaac fdkaac 1.0.0 [...] If you are sure, all dependencies are installed, the next step is to download the latest release of m4b-tool from https://github.com/sandreas/m4b-tool/releases Depending on the operating system, you can rename m4b-tool.phar to m4b-tool and run m4b-tool --version directly from the command line. If you are not sure, you can always use the command php m4b-tool.phar --version to check if the installation was successful. This should work on every system. If you would like to use the latest source code with all new features and fixes, you could also build from source. The current build might be in unstable and should only be used for testing purposes or if you need a specific feature that has not been released. Custom mp4v2 for accurate sorting order Most audiobooks are not released in alphabetical order. A prominent example is Harry Potter. So if you have all the Harry Potter audiobooks, it depends on your player, but probably they are not listed in the correct order... let's see, what the alphabetical order would be: Harry Potter and the Chamber of Secrets (Part 2) Harry Potter and the Philosopher's Stone (Part 1) Harry Potter and the Prisoner of Azkaban (Part 3) And the correct order would have been: Harry Potter and the Philosopher's Stone (Part 1) Harry Potter and the Chamber of Secrets (Part 2) Harry Potter and the Prisoner of Azkaban (Part 3) Well, there is a solution for this. You have to tag the audiobook with a custom sortname and / or sortalbum. If your player supports these tags, the order is now correct, even when the title is still the original title. To achieve this, i had to build a custom version of mp4v2 (more accurate mp4tags), to add options for these tags and add the pseudo tags --series and --series-part. So if you do the following: m4b-tool merge --name=\"Harry Potter and the Chamber of Secrets\" --series=\"Harry Potter\" --series-part=\"2\" --output-file=\"output/Harry Potter and the Chamber of Secrets.m4b\" \"input/Harry Potter and the Chamber of Secrets\" It would result in: Name: Harry Potter and the Chamber of Secrets Sortname: Harry Potter 2 - Harry Potter and the Chamber of Secrets Install custom mp4v2 In the docker image, the custom version is already installed git clone https://github.com/sandreas/mp4v2 cd mp4v2 ./configure make && sudo make install About audio quality In m4b-tool all audio conversions are performed with ffmpeg resulting in pretty descent audio quality using its free encoders. However, best quality takes some extra effort, so if you are using the free encoders, m4b-tool might show the following hint: Your ffmpeg version cannot produce top quality aac using encoder aac instead of libfdk_aac That's not really a problem, because the difference between the aac and libfdk_aac encoder is hardly noticeable in most cases. But to overcome the hint and get the best audio quality possible, you have to use a non-free encoder, that is not integrated in ffmpeg by default (licensing reasons). Depending on the operating system you are using, installing the non-free encoder may require a little extra skills, effort and time (see the notes for your operating system above). You have to decide, if it is worth the additional effort for getting the slightly better quality. If you are using the docker image, you should get the best quality by default. If you are using very low bitrates (<= 32k), you could also use high efficiency profiles to further improve audio quality (e.g. --audio-profile=aac_he for mono). Unfortunately, ffmpeg's high efficiency implementation produces audio files, that are incompatible with many players (including iTunes). To produce high efficiency files, that are compatible with at least most common players, you will need to install fdkaac for now. More Details: https://github.com/sandreas/m4b-tool/issues/19 https://trac.ffmpeg.org/wiki/Encode/AAC https://trac.ffmpeg.org/wiki/Encode/HighQualityAudio Submitting issues You think there is an issue with m4b-tool? First take a look at the Known Issues below. If this does not help, please provide the following information when adding an issue: the operating system you use the exact command, that you tried, e.g. m4b-tool merge my-audio-book/ --output-file merged.m4b the error message, that occured or the circumstances, e.g. the resulting file merged.m4b is only 5kb other relevant information, e.g. sample files if needed Example: Title: m4b-tool does not embed covers If i run m4b-tool with a folder containing a cover.png, it does not embed the cover and shows an error message. OS: Ubuntu 16.04 LTS Command: `m4b-tool merge my-audio-book/ ---output-file merged.m4b` Error: Cannot embed cover, cover is not a valid image file Attached files: cover.png Known issues If you are getting PHP Exceptions, it is a configuration issue with PHP in most cases. If are not familiar with PHP configuration, you could follow these instructions, to fix a few known issues: Exception Charset not supported [Exception] charset windows-1252 is not supported - use one of these instead: utf-8 This mostly happens on windows, because the mbstring-Extension is used to internally convert charsets, so that special chars like german umlauts are supported on every platform. To fix this, you need to enable the mbstring-extension: Run php --ini on the command line: C:\\>php --ini ... Loaded Configuration File: C:\\Program Files\\php\\php.ini Open the configuration file (e.g. C:\\Program Files\\php\\php.ini) in a text editor and search for extension=. On Windows there should be an item like this: ;extension=php_mbstring.dll remove the ; to enable the extension: extension=php_mbstring.dll Now everything should work as expected. m4b-tool commands The following list contains all possible commands including merge, split and chapters accompanied by the reference of parameters available in every command. merge With m4b-tool you can merge a set of audio files to one single m4b audiobook file. Example: m4b-tool merge \"data/my-audio-book\" --output-file=\"data/my-audio-book.m4b\" This merges all Audio-Files in folder data/my-audio-book into my-audio-book.m4b, using the tag-title of every file for generating chapters. If there is a file data/my-audio-book/cover.jpg, it will be used as cover for the resulting m4b file. Note: If you use untagged audio files, you could provide a musicbrainz id to get the correct chapter names, see command chapter for more info. Reference For all options, see m4b-tool merge --help: Description: Merges a set of files to one single file Usage: merge [options] [--] <input> [<more-input-files>...] Arguments: input Input file or folder more-input-files Other Input files or folders Options: --logfile[=LOGFILE] file to log all output [default: \"\"] --debug enable debug mode - sets verbosity to debug, logfile to m4b-tool.log and temporary encoded files are not deleted -f, --force force overwrite of existing files --no-cache clear cache completely before doing anything --ffmpeg-threads[=FFMPEG-THREADS] specify -threads parameter for ffmpeg - you should also consider --jobs when merge is used [default: \"\"] --platform-charset[=PLATFORM-CHARSET] Convert from this filesystem charset to utf-8, when tagging files (e.g. Windows-1252, mainly used on Windows Systems) [default: \"\"] --ffmpeg-param[=FFMPEG-PARAM] Add argument to every ffmpeg call, append after all other ffmpeg parameters (e.g. --ffmpeg-param=\"-max_muxing_queue_size\" --ffmpeg-param=\"1000\" for ffmpeg [...] -max_muxing_queue_size 1000) (multiple values allowed) -a, --silence-min-length[=SILENCE-MIN-LENGTH] silence minimum length in milliseconds [default: 1750] -b, --silence-max-length[=SILENCE-MAX-LENGTH] silence maximum length in milliseconds [default: 0] --max-chapter-length[=MAX-CHAPTER-LENGTH] maximum chapter length in seconds - its also possible to provide a desired chapter length in form of 300,900 where 300 is desired and 900 is max - if the max chapter length is exceeded, the chapter is placed on the first silence between desired and max chapter length [default: \"0\"] --name[=NAME] custom name, otherwise the existing metadata will be used --sortname[=SORTNAME] custom sortname, that is used only for sorting --album[=ALBUM] custom album, otherwise the existing metadata for name will be used --sortalbum[=SORTALBUM] custom sortalbum, that is used only for sorting --artist[=ARTIST] custom artist, otherwise the existing metadata will be used --sortartist[=SORTARTIST] custom sortartist, that is used only for sorting --genre[=GENRE] custom genre, otherwise the existing metadata will be used --writer[=WRITER] custom writer, otherwise the existing metadata will be used --albumartist[=ALBUMARTIST] custom albumartist, otherwise the existing metadata will be used --year[=YEAR] custom year, otherwise the existing metadata will be used --description[=DESCRIPTION] custom short description, otherwise the existing metadata will be used --longdesc[=LONGDESC] custom long description, otherwise the existing metadata will be used --comment[=COMMENT] custom comment, otherwise the existing metadata will be used --copyright[=COPYRIGHT] custom copyright, otherwise the existing metadata will be used --encoded-by[=ENCODED-BY] custom encoded-by, otherwise the existing metadata will be used --cover[=COVER] custom cover, otherwise the existing metadata will be used --skip-cover skip extracting and embedding covers --series[=SERIES] custom series, this pseudo tag will be used to auto create sort order (e.g. Harry Potter or The Kingkiller Chronicles) --series-part[=SERIES-PART] custom series part, this pseudo tag will be used to auto create sort order (e.g. 1 or 2.5) --audio-format[=AUDIO-FORMAT] output format, that ffmpeg will use to create files [default: \"m4b\"] --audio-channels[=AUDIO-CHANNELS] audio channels, e.g. 1, 2 [default: \"\"] --audio-bitrate[=AUDIO-BITRATE] audio bitrate, e.g. 64k, 128k, ... [default: \"\"] --audio-samplerate[=AUDIO-SAMPLERATE] audio samplerate, e.g. 22050, 44100, ... [default: \"\"] --audio-codec[=AUDIO-CODEC] audio codec, e.g. libmp3lame, aac, ... [default: \"\"] --audio-profile[=AUDIO-PROFILE] audio profile, when using extra low bitrate - valid values: aac_he, aac_he_v2 [default: \"\"] --adjust-for-ipod auto adjust bitrate and sampling rate for ipod, if track is too long (may result in low audio quality) --fix-mime-type try to fix MIME-type (e.g. from video/mp4 to audio/mp4) - this is needed for some players to prevent an empty video window -o, --output-file=OUTPUT-FILE output file --include-extensions[=INCLUDE-EXTENSIONS] comma separated list of file extensions to include (others are skipped) [default: \"aac,alac,flac,m4a,m4b,mp3,oga,ogg,wav,wma,mp4\"] -m, --musicbrainz-id=MUSICBRAINZ-ID musicbrainz id so load chapters from --no-conversion skip conversion (destination file uses same encoding as source - all encoding specific options will be ignored) --batch-pattern[=BATCH-PATTERN] multiple batch patterns that can be used to merge all audio books in a directory matching the given patterns (e.g. %a/%t for author/title) - parameter --output-file must be a directory (multiple values allowed) --dry-run perform a dry run without converting all the files in batch mode (requires --batch-pattern) --jobs[=JOBS] Specifies the number of jobs (commands) to run simultaneously [default: 1] --use-filenames-as-chapters Use filenames for chapter titles instead of tag contents --no-chapter-reindexing Do not perform any reindexing for index-only chapter names (by default m4b-tool will try to detect index-only chapters like Chapter 1, Chapter 2 and reindex it with its numbers only) -h, --help Display this help message -q, --quiet Do not output any message -V, --version Display this application version --ansi Force ANSI output --no-ansi Disable ANSI output -n, --no-interaction Do not ask any interactive question -v|vv|vvv, --verbose Increase the verbosity of messages: 1 for normal output, 2 for more verbose output and 3 for debug Placeholder reference for --batch-pattern If you use the --batch-pattern parameter, the following placeholders are supported title / name: %n sort_name: %N album: %m, sort_album: %M, artist: %a, sort_artist: %a, genre: %g, writer: %w, album_artist: %t, year: %y, description: %d, long_description: %d, comment: %c, copyright: %c, encoded_by: %e, series: %s, series_part: %p, split m4b-tool can be used to split a single m4b into a file per chapter or a flac encoded album into single tracks via cue sheet. Example: m4b-tool split --audio-format mp3 --audio-bitrate 96k --audio-channels 1 --audio-samplerate 22050 \"data/my-audio-book.m4b\" This splits the file data/my-audio-book.m4b into an mp3 file for each chapter, writing the files into data/my-audio-book_splitted/. Cue sheet splitting (experimental) If you would like to split a flac file containing multiple tracks, a cue sheet with the exact filename of the flac is required (my-album.flac requires my-album.cue): # my-album.cue is automatically found and used for splitting m4b-tool split --audio-format=mp3 --audio-bitrate=192k --audio-channels=2 --audio-samplerate=48000 \"data/my-album.flac\" Reference For all options, see m4b-tool split --help: Description: Splits an m4b file into parts Usage: split [options] [--] <input> Arguments: input Input file or folder Options: --logfile[=LOGFILE] file to dump all output [default: \"\"] --debug enable debug mode - sets verbosity to debug, logfile to m4b-tool.log and temporary files are not deleted -f, --force force overwrite of existing files --no-cache do not use cached values and clear cache completely --ffmpeg-threads[=FFMPEG-THREADS] specify -threads parameter for ffmpeg [default: \"\"] --platform-charset[=PLATFORM-CHARSET] Convert from this filesystem charset to utf-8, when tagging files (e.g. Windows-1252, mainly used on Windows Systems) [default: \"\"] --ffmpeg-param[=FFMPEG-PARAM] Add argument to every ffmpeg call, append after all other ffmpeg parameters (e.g. --ffmpeg-param=\"-max_muxing_queue_size\" --ffmpeg-param=\"1000\" for ffmpeg [...] -max_muxing_queue_size 1000) (multiple values allowed) -a, --silence-min-length[=SILENCE-MIN-LENGTH] silence minimum length in milliseconds [default: 1750] -b, --silence-max-length[=SILENCE-MAX-LENGTH] silence maximum length in milliseconds [default: 0] --max-chapter-length[=MAX-CHAPTER-LENGTH] maximum chapter length in seconds - its also possible to provide a desired chapter length in form of 300,900 where 300 is desired and 900 is max - if the max chapter length is exceeded, the chapter is placed on the first silence between desired and max chapter length [default: \"0\"] --audio-format[=AUDIO-FORMAT] output format, that ffmpeg will use to create files [default: \"m4b\"] --audio-channels[=AUDIO-CHANNELS] audio channels, e.g. 1, 2 [default: \"\"] --audio-bitrate[=AUDIO-BITRATE] audio bitrate, e.g. 64k, 128k, ... [default: \"\"] --audio-samplerate[=AUDIO-SAMPLERATE] audio samplerate, e.g. 22050, 44100, ... [default: \"\"] --audio-codec[=AUDIO-CODEC] audio codec, e.g. libmp3lame, aac, ... [default: \"\"] --audio-profile[=AUDIO-PROFILE] audio profile, when using extra low bitrate - valid values (mono, stereo): aac_he, aac_he_v2 [default: \"\"] --adjust-for-ipod auto adjust bitrate and sampling rate for ipod, if track is to long (may lead to poor quality) --name[=NAME] provide a custom audiobook name, otherwise the existing metadata will be used [default: \"\"] --sortname[=SORTNAME] provide a custom audiobook name, that is used only for sorting purposes [default: \"\"] --album[=ALBUM] provide a custom audiobook album, otherwise the existing metadata for name will be used [default: \"\"] --sortalbum[=SORTALBUM] provide a custom audiobook album, that is used only for sorting purposes [default: \"\"] --artist[=ARTIST] provide a custom audiobook artist, otherwise the existing metadata will be used [default: \"\"] --sortartist[=SORTARTIST] provide a custom audiobook artist, that is used only for sorting purposes [default: \"\"] --genre[=GENRE] provide a custom audiobook genre, otherwise the existing metadata will be used [default: \"\"] --writer[=WRITER] provide a custom audiobook writer, otherwise the existing metadata will be used [default: \"\"] --albumartist[=ALBUMARTIST] provide a custom audiobook albumartist, otherwise the existing metadata will be used [default: \"\"] --year[=YEAR] provide a custom audiobook year, otherwise the existing metadata will be used [default: \"\"] --cover[=COVER] provide a custom audiobook cover, otherwise the existing metadata will be used --description[=DESCRIPTION] provide a custom audiobook short description, otherwise the existing metadata will be used --longdesc[=LONGDESC] provide a custom audiobook long description, otherwise the existing metadata will be used --comment[=COMMENT] provide a custom audiobook comment, otherwise the existing metadata will be used --copyright[=COPYRIGHT] provide a custom audiobook copyright, otherwise the existing metadata will be used --encoded-by[=ENCODED-BY] provide a custom audiobook encoded-by, otherwise the existing metadata will be used --series[=SERIES] provide a custom audiobook series, this pseudo tag will be used to auto create sort order (e.g. Harry Potter or The Kingkiller Chronicles) --series-part[=SERIES-PART] provide a custom audiobook series part, this pseudo tag will be used to auto create sort order (e.g. 1 or 2.5) --skip-cover skip extracting and embedding covers --fix-mime-type try to fix MIME-type (e.g. from video/mp4 to audio/mp4) - this is needed for some players to prevent video window -o, --output-dir[=OUTPUT-DIR] output directory [default: \"\"] -p, --filename-template[=FILENAME-TEMPLATE] filename twig-template for output file naming [default: \"{{\\\"%03d\\\"|format(track)}}-{{title|raw}}\"] --use-existing-chapters-file use an existing manually edited chapters file <audiobook-name>.chapters.txt instead of embedded chapters for splitting -h, --help Display this help message -q, --quiet Do not output any message -V, --version Display this application version --ansi Force ANSI output --no-ansi Disable ANSI output -n, --no-interaction Do not ask any interactive question -v|vv|vvv, --verbose Increase the verbosity of messages: 1 for normal output, 2 for more verbose output and 3 for debug Help: Split an m4b into multiple m4b or mp3 files by chapter filename-template reference If you would like to use a custom filename template, the Twig template engine is provided. The following variables are available: {{encoder}} {{title}} {{artist}} {{genre}} {{writer}} {{album}} {{disk}} {{disks}} {{albumArtist}} {{year}} {{track}} {{tracks}} {{cover}} {{description}} {{longDescription}} {{comment}} {{copyright}} {{encodedBy}} You can also use some Twig specific template extensions to pad or reformat these values. The default template is {{\\\"%03d\\\"|format(track)}}-{{title}}, which results in filenames like 001-mychapter Slashes are interpreted as directory separators, so if you use a template {{year}}/{{artist}}/{{title}} the resulting directory and file is 2018/Joanne K. Rowling/Harry Potter 1 It is not recommended to use {{description}} or {{longdescription}} for filenames but they are also provided, if the field contains other information than intended Special chars, that are forbidden in filenames are removed automatically chapters Many m4b audiobook files do not contain valid chapters for different reasons. m4b-tool can handle two cases: Correct misplaced chapters by silence detection Add chapters from an internet source (mostly for well known titles) Misplaced chapters In some cases there is a shift between the chapter mark and the real beginning of a chapter. m4b-tool could try to correct that by detecting silences and relocating the chapter to the nearest silence: m4b-tool chapters --adjust-by-silence -o \"data/destination-with-adjusted-chapters.m4b\" \"data/source-with-misplaced-chapters.m4b\" It won't work, if the shift is to large or if the chapters are strongly misplaced, but since everything is done automatically, it's worth a try, isn't it? Too long chapters Sometimes you have a file that contains valid chapters, but they are too long, so you would like to split them into sub-chapters. This is tricky, because the chapters command relays only on metadata and not on track length - so it won't work. BUT: There might be a workaround. In the latest pre-release since July 2020 you can do the following: Put the source file into an empty directory, e.g. input/my-file.m4b (this is important, don't skip this step!) Run m4b-tool merge -v --no-conversion --max-chapter-length=300,900 \"input/\" -o \"output/my-rechaptered-file.m4b\" Because of --no-conversion the chaptering process is lossless, but it takes the existing chapters as input and recalculates it based on the --max-chapter-length parameter and a new silence detection. No chapters at all If you have a well known audiobook, like Harry Potter and the Philosophers Stone, you might be lucky that it is on musicbrainz. In this case m4b-tool can try to correct the chapter information using silence detection and the musicbrainz data. Since this is not a trivial task and prone to error, m4b-tool offers some parameters to correct misplaced chapter positions manually. A typical workflow Getting the musicbrainz id You have to find the exact musicbrainz id: An easy way to find the book is to use the authors name or the readers name to search for it Once you found the book of interest, click on the list entry to show further information To get the musicbrainz id, open the details page and find the MBID (e.g. 8669da33-bf9c-47fe-adc9-23798a37b096) Example: https://musicbrainz.org/work/8669da33-bf9c-47fe-adc9-23798a37b096 MBID: 8669da33-bf9c-47fe-adc9-23798a37b096 Finding main chapters After getting the MBID you should find the main chapter points (where the name of the current chapter name is read aloud by the author). m4b-tool chapters --merge-similar --first-chapter-offset 4000 --last-chapter-offset 3500 -m 8669da33-bf9c-47fe-adc9-23798a37b096 \"../data/harry-potter-1.m4b\" Explanation: --merge-similar: merges all similar chapters (e.g. The Boy Who Lived, Part 1 and The Boy Who Lived, Part 2 will be merged to The Boy Who Lived) --first-chapter-offset: creates an start offset chapter called Offset First Chapter with a length of 4 seconds for skipping intros (e.g. audible, etc.) --last-chapter-offset: creates an end offset chapter called Offset Last Chapter with a length of 3,5 seconds for skipping outros (e.g. audible, etc.) -m: MBID Finding misplaced main chapters Now listen to the audiobook an go through the chapters. Lets assume, all but 2 chapters were detected correctly. The two misplaced chapters are chapter number 6 and 9. To find the real position of chapters 6 and 9 invoke: m4b-tool chapter --find-misplaced-chapters 5,8 --merge-similar --first-chapter-offset 4000 --last-chapter-offset 3500 -m 8669da33-bf9c-47fe-adc9-23798a37b096 \"../data/harry-potter-1.m4b\" Explanation: --find-misplaced-chapters: Comma separated list of chapter numbers, that were not detected correctly. Now m4b-tool will generate a potential chapter for every silence around the used chapter mark to find the right chapter position. Listen to the audiobook again and find the right chapter position. Note them down. Manually adjust misplaced chapters Next run the full chapter detection with the --no-chapter-import option, which prevents writing the chapters directly to the file. m4b-tool chapter --no-chapter-import --first-chapter-offset 4000 --last-chapter-offset 3500 -m 8669da33-bf9c-47fe-adc9-23798a37b096 \"../data/harry-potter-1.m4b\" To Adjust misplaced chapters, do the following: Change the start position of all misplaced chapters manually in the file ../data/harry-potter-1.chapters.txt Import the corrected chapters with mp4chaps -i ../data/harry-potter-1.m4b Listen to harry-potter-1.m4b again, now the chapters should be at the correct position. Troubleshooting If none of the chapters are detected correctly, this can have different reasons: The silence parts of this audiobook are too short for detection. To adjust the minimum silence length, use --silence-min-length 1000 setting the silence length to 1 second. Caution: To low values can lead to misplaced chapters and increased detection time. You provided the wrong MBID There is too much background noise in this specific audiobook, so that silences cannot be detected Reference For all options, see m4b-tool chapters --help: Description: Adds chapters to m4b file Usage: chapters [options] [--] <input> Arguments: input Input file or folder Options: --logfile[=LOGFILE] file to dump all output [default: \"\"] --debug enable debug mode - sets verbosity to debug, logfile to m4b-tool.log and temporary files are not deleted -f, --force force overwrite of existing files --no-cache do not use cached values and clear cache completely --ffmpeg-threads[=FFMPEG-THREADS] specify -threads parameter for ffmpeg [default: \"\"] --platform-charset[=PLATFORM-CHARSET] Convert from this filesystem charset to utf-8, when tagging files (e.g. Windows-1252, mainly used on Windows Systems) [default: \"\"] --ffmpeg-param[=FFMPEG-PARAM] Add argument to every ffmpeg call, append after all other ffmpeg parameters (e.g. --ffmpeg-param=\"-max_muxing_queue_size\" --ffmpeg-param=\"1000\" for ffmpeg [...] -max_muxing_queue_size 1000) (multiple values allowed) -a, --silence-min-length[=SILENCE-MIN-LENGTH] silence minimum length in milliseconds [default: 1750] -b, --silence-max-length[=SILENCE-MAX-LENGTH] silence maximum length in milliseconds [default: 0] --max-chapter-length[=MAX-CHAPTER-LENGTH] maximum chapter length in seconds - its also possible to provide a desired chapter length in form of 300,900 where 300 is desired and 900 is max - if the max chapter length is exceeded, the chapter is placed on the first silence between desired and max chapter length [default: \"0\"] -m, --musicbrainz-id=MUSICBRAINZ-ID musicbrainz id so load chapters from -s, --merge-similar merge similar chapter names -o, --output-file[=OUTPUT-FILE] write chapters to this output file [default: \"\"] --adjust-by-silence will try to adjust chapters of a file by silence detection and existing chapter marks --find-misplaced-chapters[=FIND-MISPLACED-CHAPTERS] mark silence around chapter numbers that where not detected correctly, e.g. 8,15,18 [default: \"\"] --find-misplaced-offset[=FIND-MISPLACED-OFFSET] mark silence around chapter numbers with this offset seconds maximum [default: 120] --find-misplaced-tolerance[=FIND-MISPLACED-TOLERANCE] mark another chapter with this offset before each silence to compensate ffmpeg mismatches [default: -4000] --no-chapter-numbering do not append chapter number after name, e.g. My Chapter (1) --no-chapter-import do not import chapters into m4b-file, just create chapters.txt --chapter-pattern[=CHAPTER-PATTERN] regular expression for matching chapter name [default: \"/^[^:]+[1-9][0-9]*:[\\s]*(.*),.*[1-9][0-9]*[\\s]*$/i\"] --chapter-replacement[=CHAPTER-REPLACEMENT] regular expression replacement for matching chapter name [default: \"$1\"] --chapter-remove-chars[=CHAPTER-REMOVE-CHARS] remove these chars from chapter name [default: \"\"] --first-chapter-offset[=FIRST-CHAPTER-OFFSET] milliseconds to add after silence on chapter start [default: 0] --last-chapter-offset[=LAST-CHAPTER-OFFSET] milliseconds to add after silence on chapter start [default: 0] -h, --help Display this help message -q, --quiet Do not output any message -V, --version Display this application version --ansi Force ANSI output --no-ansi Disable ANSI output -n, --no-interaction Do not ask any interactive question -v|vv|vvv, --verbose Increase the verbosity of messages: 1 for normal output, 2 for more verbose output and 3 for debug Help: Can add Chapters to m4b files via different types of inputs Latest release m4b-tool is a one-man-project, so sometimes it evolves quickly and often nothing happens. If you have reported an issue and it is marked as fixed, there might be no stable release for a long time. That's why now there is a latest tag in combination with a Pre-Release for testing purposes. These releases always contain the most recent builds with all available fixes and new features. Mostly untested, there may be new bugs, non-functional features or - pretty unlikely - critical issues with the risk of data loss. Feedback is always welcome, but don't expect that these are fixed quickly. To get the Pre-Release, go to https://github.com/sandreas/m4b-tool/releases/tag/latest and download the m4b-tool.tar.gz or if using docker rebuild the image with: docker build . --build-arg M4B_TOOL_DOWNLOAD_LINK=<link-to-pre-release> -t m4b-tool Building from source m4b-tool contains a build script, which will create an executable m4b-tool.phar in the dist folder. Composer for PHP is required, so after installing composer, run following commands in project root folder: Linux / Unix Install Dependencies (Ubuntu) sudo apt install ffmpeg mp4v2-utils fdkaac php-cli composer phpunit php-mbstring Build macOS Install Dependencies (brew) brew update brew install php@7.4 phpunit brew link php@7.4 Build Windows Request for help - especially german users Right now, I'm experimenting with speech recognition and speech to text using this project This is for a feature to automatically add chapter names by speech recognition. I'm not sure this will be ever working as expected, but right now I'm pretty confident, it is possible to do the following, if there are enough speech samples in a specific language: Extract chapter names and first sentences of a chapter from an ebook Detect all silences in the audiobook Perform a speech to text for the first 30 seconds after the silence Compare it with the text parts of the ebook, mark the chapter positions and add real chapters names To do that and improve the german speech recognition, I would really appreciate YOUR help on: https://voice.mozilla.org/de (german) No account is needed to help You can support mozilla DeepSpeech to better support german speech recognition by just verifying sentences after listening or, even more important, reading out loud and uploading sentences. I try to add a few ones every day, its really easy and quite fun. At the moment the german speech recognition is not good enough for the algorithm, but I will check out every now and then - as soon the recognition is working good enough, I'll go on with this feature. ",
          "I'm curious: what players do people use so that they feel the need to <i>merge</i> an audiobook into a single file? With VLC, the accuracy of my aim on the time bar would become nonexistent. Just two days ago I've finally whipped up a script to <i>split</i> books, and not by chapters but into 10-minute chunks.",
          "Another poster cited \"a cover per chapter, etc\" as an advantage, but I don't see that capability in the linked tool (and I don't think it's part of the m4b format, which is a shame - the ability to change art per chapter or even more so, at points in the timeline, would really open some new uses e.g. saved lectures with slides)."],
        "story_type":"ShowHN",
        "url_raw":"https://github.com/sandreas/m4b-tool",
        "comments.comment_id":[19856487,
          19859685],
        "comments.comment_author":["aasasd",
          "schemathings"],
        "comments.comment_descendants":[2,
          1],
        "comments.comment_time":["2019-05-08T06:54:39Z",
          "2019-05-08T15:24:49Z"],
        "comments.comment_text":["I'm curious: what players do people use so that they feel the need to <i>merge</i> an audiobook into a single file? With VLC, the accuracy of my aim on the time bar would become nonexistent. Just two days ago I've finally whipped up a script to <i>split</i> books, and not by chapters but into 10-minute chunks.",
          "Another poster cited \"a cover per chapter, etc\" as an advantage, but I don't see that capability in the linked tool (and I don't think it's part of the m4b format, which is a shame - the ability to change art per chapter or even more so, at points in the timeline, would really open some new uses e.g. saved lectures with slides)."],
        "id":"15ee210e-826c-4394-aadf-72dd8a51f2f8",
        "url_text":"m4b-tool m4b-tool is a is a wrapper for ffmpeg and mp4v2 to merge, split or and manipulate audiobook files with chapters. Although m4b-tool is designed to handle m4b files, nearly all audio formats should be supported, e.g. mp3, aac, ogg, alac and flac. Important Note Unfortunately I am pretty busy at the moment, so m4b-tool 0.4.2 is very old. Since it is not planned to release a newer version without having complete documentation, there is only the latest pre-release getting bug fixes. It is already pretty stable, so if you are experiencing bugs with v0.4.2, please try the latest pre-release, if it has been already fixed there. Thank you, sandreas https://pilabor.com Features merge a set of audio files (e.g. MP3 or AAC) into a single m4b file split a single m4b file into several output files by chapters or a flac encoded album into single tracks via cue sheet Add or adjust chapters for an existing m4b file via silence detection or musicbrainz TL;DR - examples for the most common tasks Merge multiple files merge all audio files in directory data/my-audio-book into file data/merged.m4b (tags are retained and data/my-audio-book/cover.jpg and data/my-audio-book/description.txt are embedded, if available) m4b-tool merge \"data/my-audio-book/\" --output-file=\"data/merged.m4b\" Split one file by chapters split one big m4b file by chapters into multiple mp3 files at data/my-audio-book_splitted/ (tags are retained, data/my-audio-book_splitted/cover.jpg is created, if m4b contains a cover) m4b-tool split --audio-format mp3 --audio-bitrate 96k --audio-channels 1 --audio-samplerate 22050 \"data/my-audio-book.m4b\" Chapters adjustment of a file via silence detection chapters can try to adjust existing chapters of an m4b by silence detection m4b-tool chapters --adjust-by-silence -o \"data/destination-with-adjusted-chapters.m4b\" \"data/source-with-misplaced-chapters.m4b\" Best practices Since the most used subcommand of m4b-tool seems to be merge, lets talk about best practice... Step 0 - Take a look at the docker image Unfortunately m4b-tool has many dependencies. Not only one-liners, if you would like to get the best quality and tagging support, many dependencies have to be compiled manually with extra options. Thats why you should take a look at the docker image, which comes with all the bells and whistles of top audio quality, top tagging support and easy installation and has almost no disadvantages. Note: If you are on windows, it might be difficult to make it work Step 1 - Organizing your audiobooks in directories When merging audiobooks, you should prepare them - the following directory structure helps a lot, even if you only merge one single audiobook: input/<main genre>/<author>/<title> or if it is a series input/<main genre>/<author>/<series>/<series-part> - <title> Examples: input/Fantasy/J.K. Rowling/Quidditch Through the Ages/ input/Fantasy/J.K. Rowling/Harry Potter/1 - Harry Potter and the Philosopher's Stone/ Note: If your audiobook title contains invalid path characters like /, just replace them with a dash -. Step 2 - add cover and a description Now, because you almost always want a cover and a description for your audiobook, you should add the following files in the main directory: cover.jpg description.txt (Be sure to use UTF-8 text file encoding for the contents) Examples: input/Fantasy/J.K. Rowling/Quidditch Through the Ages/cover.jpg input/Fantasy/J.K. Rowling/Quidditch Through the Ages/description.txt Note: m4b-tool will find and embed these files automatically but does not fail, if they are not present Step 3 - chapters Chapters are nice to add waypoints for your audiobook. They help to remember the last position and improve the experience in general. fixed chapters If you would like to adjust chapters manually, you can add a chapters.txt (same location as cover.jpg) with following contents (<chapter-start> <chapter-title>): 00:00:00.000 Intro 00:04:19.153 This is 00:09:24.078 A way to add 00:14:34.500 Chapters manually by tag If your input files are tagged, these tags will be used to create the chapter metadata by its title. So if you tag your input files with valid chapter names as track title, this will result in a nice and clean m4b-file with valid chapter names. by length Another great feature since m4b-tool v.0.4.0 is the --max-chapter-length parameter. Often the individual input files are too big which results in chapters with a very long duration. This can be annoying, if you would like to jump to a certain point, since you have to rewind or fast-forward and hold the button for a long time, instead of just tipping previous or next a few times. To automatically add sub-chapters, you could provide: --max-chapter-length=300,900 This will cause m4b-tool Trying to preserve original chapters as long as they are not longer than 15 minutes (900 seconds) If a track is longer than 15 minutes Perform a silence detection and try to add sub-chapters at every silence between 5 minutes (300 seconds) and 15 minutes (900 seconds) If no silence is detected, add a hard cut sub-chapter every 5 minutes Sub-chapters are named like the original and get an additional index. This is a nice way to keep the real names but not having chapters with a too long duration. Step 4 (optional) - for iPod owners If you own an iPod, there might be a problem with too long audiobooks, since iPods only support 32bit sampling rates. If your audiobook is longer than 27 hours with 22050Hz sampling rate, you could provide --adjust-for-ipod, to automatically downsample your audiobook, which results in lower quality, but at least its working on your good old iPod... Step 5 (optional) - more cpu cores, faster conversion m4b-tool supports multiple conversion tasks in parallel with the --jobs parameter (e.g. --jobs=2). If you have to convert more than one file, which is the common case, you nearly double the merge speed by providing the --jobs=2 parameter (or quadruplicate with --jobs=4, if you have a quad core system, etc.). Don't provide a number higher than the number of cores on your system - this will slow down the merge... Note: If you run the conversion on all your cores, it will result in almost 100% CPU usage, which may lead to slower system performance Step 6 - Use the --batch-pattern feature In m4b-tool v.0.4.0 the --batch-pattern feature was added. It can be used to batch-convert multiple audiobooks at once, but also to just convert one single audiobook - because you can create tags from an existing directory structure. Hint: The output-file parameter has to be a directory, when using --batch-pattern. Even multiple --batch-pattern parameters are supported, while the first match will be used first. So if you created the directory structure as described above, the final command to merge input/Fantasy/Harry Potter/1 - Harry Potter and the Philosopher's Stone/ to output/Fantasy/Harry Potter/1 - Harry Potter and the Philosopher's Stone.m4b would look like this: m4b-tool merge -v --jobs=2 --output-file=\"output/\" --max-chapter-length=300,900 --adjust-for-ipod --batch-pattern=\"input/%g/%a/%s/%p - %n/\" --batch-pattern=\"input/%g/%a/%n/\" \"input/\" In --batch-pattern mode, existing files are skipped by default Result If you performed the above steps with the docker image or installed and compiled all dependencies, you should get the following result: Top quality audio by using libfdk_aac encoder Series and single audiobooks have valid tags for genre, author, title, sorttitle, etc. from --batch-pattern usage If the files cover.jpg and description.txt exist in the main directories, a cover, a description and a longdesc are embedded If you tagged the input files, real chapter names should appear in your player No more chapters longer than 15 minutes Working iPod versions for audiobooks longer than 27 hours Installation Docker To use docker with m4b-tool, you first have to build a custom image located in the docker directory. Since this image is compiling every third party library from scratch to get the best possible audio quality, it can take a long time for the first build. Note: You should know that build does not mean that m4b-tool is being compiled from source. That indeed is strange, but unlike other projects, the m4b-tool docker image only downloads the latest binary release unless you do some extra work (see below). # clone m4b-tool repository git clone https://github.com/sandreas/m4b-tool.git # change directory cd m4b-tool # build docker image - this will take a while docker build . -t m4b-tool # create an alias for m4b-tool running docker alias m4b-tool='docker run -it --rm -u $(id -u):$(id -g) -v \"$(pwd)\":/mnt m4b-tool' # testing the command m4b-tool --version Note: If you use the alias above, keep in mind that you cannot use absolute paths (e.g. /tmp/data/audiobooks/harry potter 1) or symlinks. You must change into the directory and use relative paths (e.g. cd /tmp/data && m4b-tool merge \"audiobooks/harry potter 1\" --output-file harry.m4b) Dockerize a Pre-Release or an older release version To build a docker container using a Pre-Release or an older m4b-tool release, it is required to provide an extra parameter for downloading a specific version into the image, e.g. for v.0.4.1: docker build . --build-arg M4B_TOOL_DOWNLOAD_LINK=https://github.com/sandreas/m4b-tool/releases/download/v.0.4.1/m4b-tool.tar.gz -t m4b-tool Note: You could also just edit the according variable in the Dockerfile. Dockerize a custom build, that is not available via download link Developers or experts might want to run a complete custom build of m4b-tool or build the code themselves (e.g. if you forked the repository and applied some patches). If that is the case, you can store the custom build to dist/m4b-tool.phar relative to the Dockerfile and then do a default build. # dist/m4b-tool.phar is available docker build . -t m4b-tool After this the custom build should be integrated into the docker image. MacOS On MacOS you may use the awesome package manager brew to install m4b-tool. Recommended: High audio quality, sort tagging Getting best audio quality requires some additional effort. You have to recompile ffmpeg with the non-free libfdk_aac codec. This requires uninstalling the default ffmpeg package if installed, since brew dropped the possibility for extra options. There is no official ffmpeg-with-options repository, but a pretty decent tap, that you could use to save time. # FIRST INSTALL ONLY: if not already done, remove existing ffmpeg with default audio quality options # check for ffmpeg with libfdk and uninstall if libfdk is not already available [ -x \"$(which ffmpeg)\" ] && (ffmpeg -hide_banner -codecs 2>&1 | grep libfdk || brew uninstall ffmpeg) # tap required repositories brew tap sandreas/tap brew tap homebrew-ffmpeg/ffmpeg # check available ffmpeg options and which you would like to use brew options homebrew-ffmpeg/ffmpeg/ffmpeg # install ffmpeg with at least libfdk_aac for best audio quality brew install homebrew-ffmpeg/ffmpeg/ffmpeg --with-fdk-aac # install m4b-tool brew install sandreas/tap/m4b-tool # check installed m4b-tool version m4b-tool --version Stick to defaults (acceptable audio quality, no sort tagging) If the above did not work for you or you would just to checkout m4b-tool before using it in production, you might want to try the quick and easy way. It will work, but you get lower audio quality and there is no support for sort tagging. # tap m4b-tool repository brew tap sandreas/tap # install dependencies brew install ffmpeg fdk-aac-encoder mp4v2 # install m4b-tool with acceptable audio quality and no sort tagging brew install --ignore-dependencies sandreas/tap/m4b-tool Ubuntu # install all dependencies sudo apt install ffmpeg mp4v2-utils fdkaac php-cli php-intl php-json php-mbstring php-xml # install / upgrade m4b-tool sudo wget https://github.com/sandreas/m4b-tool/releases/download/v.0.4.2/m4b-tool.phar -O /usr/local/bin/m4b-tool && sudo chmod +x /usr/local/bin/m4b-tool # check installed m4b-tool version m4b-tool --version Note: If you would like to get the best possible audio quality, you have to compile ffmpeg with the high quality encoder fdk-aac (--enable-libfdk_aac) - see https://trac.ffmpeg.org/wiki/CompilationGuide/Ubuntu for a step-by-step guide to compile ffmpeg. Manual installation (only recommended on Windows systems) m4b-tool is written in PHP and uses ffmpeg, mp4v2 and optionally fdkaac for high efficiency codecs to perform conversions. Therefore you will need the following tools in your %PATH%: php >= 7.1 with mbstring extension enabled (https://php.net) ffmpeg (https://www.ffmpeg.org) mp4v2 (mp4chaps, mp4art, etc. https://github.com/sandreas/m4b-tool/releases/download/v0.2/mp4v2-windows.zip) fdkaac (optional, only if you need high efficiency for low bitrates <= 32k, http://wlc.io/2015/06/20/fdk-aac/ - caution: not official!) To check the dependencies, running following commands via command line should show similar output: $ php -v Copyright (c) 1997-2018 The PHP Group [...] $ ffmpeg -version ffmpeg version 4.1.1 Copyright (c) 2000-2019 the FFmpeg developers [...] $ mp4chaps --version mp4chaps - MP4v2 2.0.0 $ fdkaac fdkaac 1.0.0 [...] If you are sure, all dependencies are installed, the next step is to download the latest release of m4b-tool from https://github.com/sandreas/m4b-tool/releases Depending on the operating system, you can rename m4b-tool.phar to m4b-tool and run m4b-tool --version directly from the command line. If you are not sure, you can always use the command php m4b-tool.phar --version to check if the installation was successful. This should work on every system. If you would like to use the latest source code with all new features and fixes, you could also build from source. The current build might be in unstable and should only be used for testing purposes or if you need a specific feature that has not been released. Custom mp4v2 for accurate sorting order Most audiobooks are not released in alphabetical order. A prominent example is Harry Potter. So if you have all the Harry Potter audiobooks, it depends on your player, but probably they are not listed in the correct order... let's see, what the alphabetical order would be: Harry Potter and the Chamber of Secrets (Part 2) Harry Potter and the Philosopher's Stone (Part 1) Harry Potter and the Prisoner of Azkaban (Part 3) And the correct order would have been: Harry Potter and the Philosopher's Stone (Part 1) Harry Potter and the Chamber of Secrets (Part 2) Harry Potter and the Prisoner of Azkaban (Part 3) Well, there is a solution for this. You have to tag the audiobook with a custom sortname and / or sortalbum. If your player supports these tags, the order is now correct, even when the title is still the original title. To achieve this, i had to build a custom version of mp4v2 (more accurate mp4tags), to add options for these tags and add the pseudo tags --series and --series-part. So if you do the following: m4b-tool merge --name=\"Harry Potter and the Chamber of Secrets\" --series=\"Harry Potter\" --series-part=\"2\" --output-file=\"output/Harry Potter and the Chamber of Secrets.m4b\" \"input/Harry Potter and the Chamber of Secrets\" It would result in: Name: Harry Potter and the Chamber of Secrets Sortname: Harry Potter 2 - Harry Potter and the Chamber of Secrets Install custom mp4v2 In the docker image, the custom version is already installed git clone https://github.com/sandreas/mp4v2 cd mp4v2 ./configure make && sudo make install About audio quality In m4b-tool all audio conversions are performed with ffmpeg resulting in pretty descent audio quality using its free encoders. However, best quality takes some extra effort, so if you are using the free encoders, m4b-tool might show the following hint: Your ffmpeg version cannot produce top quality aac using encoder aac instead of libfdk_aac That's not really a problem, because the difference between the aac and libfdk_aac encoder is hardly noticeable in most cases. But to overcome the hint and get the best audio quality possible, you have to use a non-free encoder, that is not integrated in ffmpeg by default (licensing reasons). Depending on the operating system you are using, installing the non-free encoder may require a little extra skills, effort and time (see the notes for your operating system above). You have to decide, if it is worth the additional effort for getting the slightly better quality. If you are using the docker image, you should get the best quality by default. If you are using very low bitrates (<= 32k), you could also use high efficiency profiles to further improve audio quality (e.g. --audio-profile=aac_he for mono). Unfortunately, ffmpeg's high efficiency implementation produces audio files, that are incompatible with many players (including iTunes). To produce high efficiency files, that are compatible with at least most common players, you will need to install fdkaac for now. More Details: https://github.com/sandreas/m4b-tool/issues/19 https://trac.ffmpeg.org/wiki/Encode/AAC https://trac.ffmpeg.org/wiki/Encode/HighQualityAudio Submitting issues You think there is an issue with m4b-tool? First take a look at the Known Issues below. If this does not help, please provide the following information when adding an issue: the operating system you use the exact command, that you tried, e.g. m4b-tool merge my-audio-book/ --output-file merged.m4b the error message, that occured or the circumstances, e.g. the resulting file merged.m4b is only 5kb other relevant information, e.g. sample files if needed Example: Title: m4b-tool does not embed covers If i run m4b-tool with a folder containing a cover.png, it does not embed the cover and shows an error message. OS: Ubuntu 16.04 LTS Command: `m4b-tool merge my-audio-book/ ---output-file merged.m4b` Error: Cannot embed cover, cover is not a valid image file Attached files: cover.png Known issues If you are getting PHP Exceptions, it is a configuration issue with PHP in most cases. If are not familiar with PHP configuration, you could follow these instructions, to fix a few known issues: Exception Charset not supported [Exception] charset windows-1252 is not supported - use one of these instead: utf-8 This mostly happens on windows, because the mbstring-Extension is used to internally convert charsets, so that special chars like german umlauts are supported on every platform. To fix this, you need to enable the mbstring-extension: Run php --ini on the command line: C:\\>php --ini ... Loaded Configuration File: C:\\Program Files\\php\\php.ini Open the configuration file (e.g. C:\\Program Files\\php\\php.ini) in a text editor and search for extension=. On Windows there should be an item like this: ;extension=php_mbstring.dll remove the ; to enable the extension: extension=php_mbstring.dll Now everything should work as expected. m4b-tool commands The following list contains all possible commands including merge, split and chapters accompanied by the reference of parameters available in every command. merge With m4b-tool you can merge a set of audio files to one single m4b audiobook file. Example: m4b-tool merge \"data/my-audio-book\" --output-file=\"data/my-audio-book.m4b\" This merges all Audio-Files in folder data/my-audio-book into my-audio-book.m4b, using the tag-title of every file for generating chapters. If there is a file data/my-audio-book/cover.jpg, it will be used as cover for the resulting m4b file. Note: If you use untagged audio files, you could provide a musicbrainz id to get the correct chapter names, see command chapter for more info. Reference For all options, see m4b-tool merge --help: Description: Merges a set of files to one single file Usage: merge [options] [--] <input> [<more-input-files>...] Arguments: input Input file or folder more-input-files Other Input files or folders Options: --logfile[=LOGFILE] file to log all output [default: \"\"] --debug enable debug mode - sets verbosity to debug, logfile to m4b-tool.log and temporary encoded files are not deleted -f, --force force overwrite of existing files --no-cache clear cache completely before doing anything --ffmpeg-threads[=FFMPEG-THREADS] specify -threads parameter for ffmpeg - you should also consider --jobs when merge is used [default: \"\"] --platform-charset[=PLATFORM-CHARSET] Convert from this filesystem charset to utf-8, when tagging files (e.g. Windows-1252, mainly used on Windows Systems) [default: \"\"] --ffmpeg-param[=FFMPEG-PARAM] Add argument to every ffmpeg call, append after all other ffmpeg parameters (e.g. --ffmpeg-param=\"-max_muxing_queue_size\" --ffmpeg-param=\"1000\" for ffmpeg [...] -max_muxing_queue_size 1000) (multiple values allowed) -a, --silence-min-length[=SILENCE-MIN-LENGTH] silence minimum length in milliseconds [default: 1750] -b, --silence-max-length[=SILENCE-MAX-LENGTH] silence maximum length in milliseconds [default: 0] --max-chapter-length[=MAX-CHAPTER-LENGTH] maximum chapter length in seconds - its also possible to provide a desired chapter length in form of 300,900 where 300 is desired and 900 is max - if the max chapter length is exceeded, the chapter is placed on the first silence between desired and max chapter length [default: \"0\"] --name[=NAME] custom name, otherwise the existing metadata will be used --sortname[=SORTNAME] custom sortname, that is used only for sorting --album[=ALBUM] custom album, otherwise the existing metadata for name will be used --sortalbum[=SORTALBUM] custom sortalbum, that is used only for sorting --artist[=ARTIST] custom artist, otherwise the existing metadata will be used --sortartist[=SORTARTIST] custom sortartist, that is used only for sorting --genre[=GENRE] custom genre, otherwise the existing metadata will be used --writer[=WRITER] custom writer, otherwise the existing metadata will be used --albumartist[=ALBUMARTIST] custom albumartist, otherwise the existing metadata will be used --year[=YEAR] custom year, otherwise the existing metadata will be used --description[=DESCRIPTION] custom short description, otherwise the existing metadata will be used --longdesc[=LONGDESC] custom long description, otherwise the existing metadata will be used --comment[=COMMENT] custom comment, otherwise the existing metadata will be used --copyright[=COPYRIGHT] custom copyright, otherwise the existing metadata will be used --encoded-by[=ENCODED-BY] custom encoded-by, otherwise the existing metadata will be used --cover[=COVER] custom cover, otherwise the existing metadata will be used --skip-cover skip extracting and embedding covers --series[=SERIES] custom series, this pseudo tag will be used to auto create sort order (e.g. Harry Potter or The Kingkiller Chronicles) --series-part[=SERIES-PART] custom series part, this pseudo tag will be used to auto create sort order (e.g. 1 or 2.5) --audio-format[=AUDIO-FORMAT] output format, that ffmpeg will use to create files [default: \"m4b\"] --audio-channels[=AUDIO-CHANNELS] audio channels, e.g. 1, 2 [default: \"\"] --audio-bitrate[=AUDIO-BITRATE] audio bitrate, e.g. 64k, 128k, ... [default: \"\"] --audio-samplerate[=AUDIO-SAMPLERATE] audio samplerate, e.g. 22050, 44100, ... [default: \"\"] --audio-codec[=AUDIO-CODEC] audio codec, e.g. libmp3lame, aac, ... [default: \"\"] --audio-profile[=AUDIO-PROFILE] audio profile, when using extra low bitrate - valid values: aac_he, aac_he_v2 [default: \"\"] --adjust-for-ipod auto adjust bitrate and sampling rate for ipod, if track is too long (may result in low audio quality) --fix-mime-type try to fix MIME-type (e.g. from video/mp4 to audio/mp4) - this is needed for some players to prevent an empty video window -o, --output-file=OUTPUT-FILE output file --include-extensions[=INCLUDE-EXTENSIONS] comma separated list of file extensions to include (others are skipped) [default: \"aac,alac,flac,m4a,m4b,mp3,oga,ogg,wav,wma,mp4\"] -m, --musicbrainz-id=MUSICBRAINZ-ID musicbrainz id so load chapters from --no-conversion skip conversion (destination file uses same encoding as source - all encoding specific options will be ignored) --batch-pattern[=BATCH-PATTERN] multiple batch patterns that can be used to merge all audio books in a directory matching the given patterns (e.g. %a/%t for author/title) - parameter --output-file must be a directory (multiple values allowed) --dry-run perform a dry run without converting all the files in batch mode (requires --batch-pattern) --jobs[=JOBS] Specifies the number of jobs (commands) to run simultaneously [default: 1] --use-filenames-as-chapters Use filenames for chapter titles instead of tag contents --no-chapter-reindexing Do not perform any reindexing for index-only chapter names (by default m4b-tool will try to detect index-only chapters like Chapter 1, Chapter 2 and reindex it with its numbers only) -h, --help Display this help message -q, --quiet Do not output any message -V, --version Display this application version --ansi Force ANSI output --no-ansi Disable ANSI output -n, --no-interaction Do not ask any interactive question -v|vv|vvv, --verbose Increase the verbosity of messages: 1 for normal output, 2 for more verbose output and 3 for debug Placeholder reference for --batch-pattern If you use the --batch-pattern parameter, the following placeholders are supported title / name: %n sort_name: %N album: %m, sort_album: %M, artist: %a, sort_artist: %a, genre: %g, writer: %w, album_artist: %t, year: %y, description: %d, long_description: %d, comment: %c, copyright: %c, encoded_by: %e, series: %s, series_part: %p, split m4b-tool can be used to split a single m4b into a file per chapter or a flac encoded album into single tracks via cue sheet. Example: m4b-tool split --audio-format mp3 --audio-bitrate 96k --audio-channels 1 --audio-samplerate 22050 \"data/my-audio-book.m4b\" This splits the file data/my-audio-book.m4b into an mp3 file for each chapter, writing the files into data/my-audio-book_splitted/. Cue sheet splitting (experimental) If you would like to split a flac file containing multiple tracks, a cue sheet with the exact filename of the flac is required (my-album.flac requires my-album.cue): # my-album.cue is automatically found and used for splitting m4b-tool split --audio-format=mp3 --audio-bitrate=192k --audio-channels=2 --audio-samplerate=48000 \"data/my-album.flac\" Reference For all options, see m4b-tool split --help: Description: Splits an m4b file into parts Usage: split [options] [--] <input> Arguments: input Input file or folder Options: --logfile[=LOGFILE] file to dump all output [default: \"\"] --debug enable debug mode - sets verbosity to debug, logfile to m4b-tool.log and temporary files are not deleted -f, --force force overwrite of existing files --no-cache do not use cached values and clear cache completely --ffmpeg-threads[=FFMPEG-THREADS] specify -threads parameter for ffmpeg [default: \"\"] --platform-charset[=PLATFORM-CHARSET] Convert from this filesystem charset to utf-8, when tagging files (e.g. Windows-1252, mainly used on Windows Systems) [default: \"\"] --ffmpeg-param[=FFMPEG-PARAM] Add argument to every ffmpeg call, append after all other ffmpeg parameters (e.g. --ffmpeg-param=\"-max_muxing_queue_size\" --ffmpeg-param=\"1000\" for ffmpeg [...] -max_muxing_queue_size 1000) (multiple values allowed) -a, --silence-min-length[=SILENCE-MIN-LENGTH] silence minimum length in milliseconds [default: 1750] -b, --silence-max-length[=SILENCE-MAX-LENGTH] silence maximum length in milliseconds [default: 0] --max-chapter-length[=MAX-CHAPTER-LENGTH] maximum chapter length in seconds - its also possible to provide a desired chapter length in form of 300,900 where 300 is desired and 900 is max - if the max chapter length is exceeded, the chapter is placed on the first silence between desired and max chapter length [default: \"0\"] --audio-format[=AUDIO-FORMAT] output format, that ffmpeg will use to create files [default: \"m4b\"] --audio-channels[=AUDIO-CHANNELS] audio channels, e.g. 1, 2 [default: \"\"] --audio-bitrate[=AUDIO-BITRATE] audio bitrate, e.g. 64k, 128k, ... [default: \"\"] --audio-samplerate[=AUDIO-SAMPLERATE] audio samplerate, e.g. 22050, 44100, ... [default: \"\"] --audio-codec[=AUDIO-CODEC] audio codec, e.g. libmp3lame, aac, ... [default: \"\"] --audio-profile[=AUDIO-PROFILE] audio profile, when using extra low bitrate - valid values (mono, stereo): aac_he, aac_he_v2 [default: \"\"] --adjust-for-ipod auto adjust bitrate and sampling rate for ipod, if track is to long (may lead to poor quality) --name[=NAME] provide a custom audiobook name, otherwise the existing metadata will be used [default: \"\"] --sortname[=SORTNAME] provide a custom audiobook name, that is used only for sorting purposes [default: \"\"] --album[=ALBUM] provide a custom audiobook album, otherwise the existing metadata for name will be used [default: \"\"] --sortalbum[=SORTALBUM] provide a custom audiobook album, that is used only for sorting purposes [default: \"\"] --artist[=ARTIST] provide a custom audiobook artist, otherwise the existing metadata will be used [default: \"\"] --sortartist[=SORTARTIST] provide a custom audiobook artist, that is used only for sorting purposes [default: \"\"] --genre[=GENRE] provide a custom audiobook genre, otherwise the existing metadata will be used [default: \"\"] --writer[=WRITER] provide a custom audiobook writer, otherwise the existing metadata will be used [default: \"\"] --albumartist[=ALBUMARTIST] provide a custom audiobook albumartist, otherwise the existing metadata will be used [default: \"\"] --year[=YEAR] provide a custom audiobook year, otherwise the existing metadata will be used [default: \"\"] --cover[=COVER] provide a custom audiobook cover, otherwise the existing metadata will be used --description[=DESCRIPTION] provide a custom audiobook short description, otherwise the existing metadata will be used --longdesc[=LONGDESC] provide a custom audiobook long description, otherwise the existing metadata will be used --comment[=COMMENT] provide a custom audiobook comment, otherwise the existing metadata will be used --copyright[=COPYRIGHT] provide a custom audiobook copyright, otherwise the existing metadata will be used --encoded-by[=ENCODED-BY] provide a custom audiobook encoded-by, otherwise the existing metadata will be used --series[=SERIES] provide a custom audiobook series, this pseudo tag will be used to auto create sort order (e.g. Harry Potter or The Kingkiller Chronicles) --series-part[=SERIES-PART] provide a custom audiobook series part, this pseudo tag will be used to auto create sort order (e.g. 1 or 2.5) --skip-cover skip extracting and embedding covers --fix-mime-type try to fix MIME-type (e.g. from video/mp4 to audio/mp4) - this is needed for some players to prevent video window -o, --output-dir[=OUTPUT-DIR] output directory [default: \"\"] -p, --filename-template[=FILENAME-TEMPLATE] filename twig-template for output file naming [default: \"{{\\\"%03d\\\"|format(track)}}-{{title|raw}}\"] --use-existing-chapters-file use an existing manually edited chapters file <audiobook-name>.chapters.txt instead of embedded chapters for splitting -h, --help Display this help message -q, --quiet Do not output any message -V, --version Display this application version --ansi Force ANSI output --no-ansi Disable ANSI output -n, --no-interaction Do not ask any interactive question -v|vv|vvv, --verbose Increase the verbosity of messages: 1 for normal output, 2 for more verbose output and 3 for debug Help: Split an m4b into multiple m4b or mp3 files by chapter filename-template reference If you would like to use a custom filename template, the Twig template engine is provided. The following variables are available: {{encoder}} {{title}} {{artist}} {{genre}} {{writer}} {{album}} {{disk}} {{disks}} {{albumArtist}} {{year}} {{track}} {{tracks}} {{cover}} {{description}} {{longDescription}} {{comment}} {{copyright}} {{encodedBy}} You can also use some Twig specific template extensions to pad or reformat these values. The default template is {{\\\"%03d\\\"|format(track)}}-{{title}}, which results in filenames like 001-mychapter Slashes are interpreted as directory separators, so if you use a template {{year}}/{{artist}}/{{title}} the resulting directory and file is 2018/Joanne K. Rowling/Harry Potter 1 It is not recommended to use {{description}} or {{longdescription}} for filenames but they are also provided, if the field contains other information than intended Special chars, that are forbidden in filenames are removed automatically chapters Many m4b audiobook files do not contain valid chapters for different reasons. m4b-tool can handle two cases: Correct misplaced chapters by silence detection Add chapters from an internet source (mostly for well known titles) Misplaced chapters In some cases there is a shift between the chapter mark and the real beginning of a chapter. m4b-tool could try to correct that by detecting silences and relocating the chapter to the nearest silence: m4b-tool chapters --adjust-by-silence -o \"data/destination-with-adjusted-chapters.m4b\" \"data/source-with-misplaced-chapters.m4b\" It won't work, if the shift is to large or if the chapters are strongly misplaced, but since everything is done automatically, it's worth a try, isn't it? Too long chapters Sometimes you have a file that contains valid chapters, but they are too long, so you would like to split them into sub-chapters. This is tricky, because the chapters command relays only on metadata and not on track length - so it won't work. BUT: There might be a workaround. In the latest pre-release since July 2020 you can do the following: Put the source file into an empty directory, e.g. input/my-file.m4b (this is important, don't skip this step!) Run m4b-tool merge -v --no-conversion --max-chapter-length=300,900 \"input/\" -o \"output/my-rechaptered-file.m4b\" Because of --no-conversion the chaptering process is lossless, but it takes the existing chapters as input and recalculates it based on the --max-chapter-length parameter and a new silence detection. No chapters at all If you have a well known audiobook, like Harry Potter and the Philosophers Stone, you might be lucky that it is on musicbrainz. In this case m4b-tool can try to correct the chapter information using silence detection and the musicbrainz data. Since this is not a trivial task and prone to error, m4b-tool offers some parameters to correct misplaced chapter positions manually. A typical workflow Getting the musicbrainz id You have to find the exact musicbrainz id: An easy way to find the book is to use the authors name or the readers name to search for it Once you found the book of interest, click on the list entry to show further information To get the musicbrainz id, open the details page and find the MBID (e.g. 8669da33-bf9c-47fe-adc9-23798a37b096) Example: https://musicbrainz.org/work/8669da33-bf9c-47fe-adc9-23798a37b096 MBID: 8669da33-bf9c-47fe-adc9-23798a37b096 Finding main chapters After getting the MBID you should find the main chapter points (where the name of the current chapter name is read aloud by the author). m4b-tool chapters --merge-similar --first-chapter-offset 4000 --last-chapter-offset 3500 -m 8669da33-bf9c-47fe-adc9-23798a37b096 \"../data/harry-potter-1.m4b\" Explanation: --merge-similar: merges all similar chapters (e.g. The Boy Who Lived, Part 1 and The Boy Who Lived, Part 2 will be merged to The Boy Who Lived) --first-chapter-offset: creates an start offset chapter called Offset First Chapter with a length of 4 seconds for skipping intros (e.g. audible, etc.) --last-chapter-offset: creates an end offset chapter called Offset Last Chapter with a length of 3,5 seconds for skipping outros (e.g. audible, etc.) -m: MBID Finding misplaced main chapters Now listen to the audiobook an go through the chapters. Lets assume, all but 2 chapters were detected correctly. The two misplaced chapters are chapter number 6 and 9. To find the real position of chapters 6 and 9 invoke: m4b-tool chapter --find-misplaced-chapters 5,8 --merge-similar --first-chapter-offset 4000 --last-chapter-offset 3500 -m 8669da33-bf9c-47fe-adc9-23798a37b096 \"../data/harry-potter-1.m4b\" Explanation: --find-misplaced-chapters: Comma separated list of chapter numbers, that were not detected correctly. Now m4b-tool will generate a potential chapter for every silence around the used chapter mark to find the right chapter position. Listen to the audiobook again and find the right chapter position. Note them down. Manually adjust misplaced chapters Next run the full chapter detection with the --no-chapter-import option, which prevents writing the chapters directly to the file. m4b-tool chapter --no-chapter-import --first-chapter-offset 4000 --last-chapter-offset 3500 -m 8669da33-bf9c-47fe-adc9-23798a37b096 \"../data/harry-potter-1.m4b\" To Adjust misplaced chapters, do the following: Change the start position of all misplaced chapters manually in the file ../data/harry-potter-1.chapters.txt Import the corrected chapters with mp4chaps -i ../data/harry-potter-1.m4b Listen to harry-potter-1.m4b again, now the chapters should be at the correct position. Troubleshooting If none of the chapters are detected correctly, this can have different reasons: The silence parts of this audiobook are too short for detection. To adjust the minimum silence length, use --silence-min-length 1000 setting the silence length to 1 second. Caution: To low values can lead to misplaced chapters and increased detection time. You provided the wrong MBID There is too much background noise in this specific audiobook, so that silences cannot be detected Reference For all options, see m4b-tool chapters --help: Description: Adds chapters to m4b file Usage: chapters [options] [--] <input> Arguments: input Input file or folder Options: --logfile[=LOGFILE] file to dump all output [default: \"\"] --debug enable debug mode - sets verbosity to debug, logfile to m4b-tool.log and temporary files are not deleted -f, --force force overwrite of existing files --no-cache do not use cached values and clear cache completely --ffmpeg-threads[=FFMPEG-THREADS] specify -threads parameter for ffmpeg [default: \"\"] --platform-charset[=PLATFORM-CHARSET] Convert from this filesystem charset to utf-8, when tagging files (e.g. Windows-1252, mainly used on Windows Systems) [default: \"\"] --ffmpeg-param[=FFMPEG-PARAM] Add argument to every ffmpeg call, append after all other ffmpeg parameters (e.g. --ffmpeg-param=\"-max_muxing_queue_size\" --ffmpeg-param=\"1000\" for ffmpeg [...] -max_muxing_queue_size 1000) (multiple values allowed) -a, --silence-min-length[=SILENCE-MIN-LENGTH] silence minimum length in milliseconds [default: 1750] -b, --silence-max-length[=SILENCE-MAX-LENGTH] silence maximum length in milliseconds [default: 0] --max-chapter-length[=MAX-CHAPTER-LENGTH] maximum chapter length in seconds - its also possible to provide a desired chapter length in form of 300,900 where 300 is desired and 900 is max - if the max chapter length is exceeded, the chapter is placed on the first silence between desired and max chapter length [default: \"0\"] -m, --musicbrainz-id=MUSICBRAINZ-ID musicbrainz id so load chapters from -s, --merge-similar merge similar chapter names -o, --output-file[=OUTPUT-FILE] write chapters to this output file [default: \"\"] --adjust-by-silence will try to adjust chapters of a file by silence detection and existing chapter marks --find-misplaced-chapters[=FIND-MISPLACED-CHAPTERS] mark silence around chapter numbers that where not detected correctly, e.g. 8,15,18 [default: \"\"] --find-misplaced-offset[=FIND-MISPLACED-OFFSET] mark silence around chapter numbers with this offset seconds maximum [default: 120] --find-misplaced-tolerance[=FIND-MISPLACED-TOLERANCE] mark another chapter with this offset before each silence to compensate ffmpeg mismatches [default: -4000] --no-chapter-numbering do not append chapter number after name, e.g. My Chapter (1) --no-chapter-import do not import chapters into m4b-file, just create chapters.txt --chapter-pattern[=CHAPTER-PATTERN] regular expression for matching chapter name [default: \"/^[^:]+[1-9][0-9]*:[\\s]*(.*),.*[1-9][0-9]*[\\s]*$/i\"] --chapter-replacement[=CHAPTER-REPLACEMENT] regular expression replacement for matching chapter name [default: \"$1\"] --chapter-remove-chars[=CHAPTER-REMOVE-CHARS] remove these chars from chapter name [default: \"\"] --first-chapter-offset[=FIRST-CHAPTER-OFFSET] milliseconds to add after silence on chapter start [default: 0] --last-chapter-offset[=LAST-CHAPTER-OFFSET] milliseconds to add after silence on chapter start [default: 0] -h, --help Display this help message -q, --quiet Do not output any message -V, --version Display this application version --ansi Force ANSI output --no-ansi Disable ANSI output -n, --no-interaction Do not ask any interactive question -v|vv|vvv, --verbose Increase the verbosity of messages: 1 for normal output, 2 for more verbose output and 3 for debug Help: Can add Chapters to m4b files via different types of inputs Latest release m4b-tool is a one-man-project, so sometimes it evolves quickly and often nothing happens. If you have reported an issue and it is marked as fixed, there might be no stable release for a long time. That's why now there is a latest tag in combination with a Pre-Release for testing purposes. These releases always contain the most recent builds with all available fixes and new features. Mostly untested, there may be new bugs, non-functional features or - pretty unlikely - critical issues with the risk of data loss. Feedback is always welcome, but don't expect that these are fixed quickly. To get the Pre-Release, go to https://github.com/sandreas/m4b-tool/releases/tag/latest and download the m4b-tool.tar.gz or if using docker rebuild the image with: docker build . --build-arg M4B_TOOL_DOWNLOAD_LINK=<link-to-pre-release> -t m4b-tool Building from source m4b-tool contains a build script, which will create an executable m4b-tool.phar in the dist folder. Composer for PHP is required, so after installing composer, run following commands in project root folder: Linux / Unix Install Dependencies (Ubuntu) sudo apt install ffmpeg mp4v2-utils fdkaac php-cli composer phpunit php-mbstring Build macOS Install Dependencies (brew) brew update brew install php@7.4 phpunit brew link php@7.4 Build Windows Request for help - especially german users Right now, I'm experimenting with speech recognition and speech to text using this project This is for a feature to automatically add chapter names by speech recognition. I'm not sure this will be ever working as expected, but right now I'm pretty confident, it is possible to do the following, if there are enough speech samples in a specific language: Extract chapter names and first sentences of a chapter from an ebook Detect all silences in the audiobook Perform a speech to text for the first 30 seconds after the silence Compare it with the text parts of the ebook, mark the chapter positions and add real chapters names To do that and improve the german speech recognition, I would really appreciate YOUR help on: https://voice.mozilla.org/de (german) No account is needed to help You can support mozilla DeepSpeech to better support german speech recognition by just verifying sentences after listening or, even more important, reading out loud and uploading sentences. I try to add a few ones every day, its really easy and quite fun. At the moment the german speech recognition is not good enough for the algorithm, but I will check out every now and then - as soon the recognition is working good enough, I'll go on with this feature. ",
        "_version_":1718938178297004032},
      {
        "story_id":20189603,
        "story_author":"antixk",
        "story_descendants":14,
        "story_score":88,
        "story_time":"2019-06-15T12:03:52Z",
        "story_title":"Show HN: Squirrel Curve Studio – A simple tool to design spline curves",
        "search":["Show HN: Squirrel Curve Studio – A simple tool to design spline curves",
          "ShowHN",
          "https://github.com/AntixK/Curve-Studio",
          "Squirrel Curve Studio Squirrel Curve Studio is an intuitive easy-to-use app for creating beautiful spline curves (free-form curves). Motivated by the lack of freely available curve designers, Squirrel provides a clean UI for designing, exporting and importing free-form curves. Interface Demo For Developers I welcome pull-requests for any new functionality or improving the app's experience overall. I am currently looking towards decreasing the app size using Neutralino.js. To get started, clone this repository using Git and install npm along with Node.js. From your command line: # Clone this repository git clone https://github.com/AntixK/Curve-Studio # Go into the repository cd Curve-Studio # Install dependencies npm install # Run the app npm start For End-Users See releases for getting pre-built binaries. Built With P5.js - For majority of the UI and curve design Electron - For creating the desktop app Electron Packager - For building the app for all platforms License This project is licensed under the MIT License - see the Curve_Studio_LICENSE.mdd file for details. Acknowledgments Hat tip to Jose Luis Garcia del Castillo for providing the p5js-electron template ",
          "Looks great, but why is it not a web site? Why electron?",
          "Well done for releasing something and hitting the top of HN.<p>It looks like a clean UI, but I'm confused what we're seeing.<p>1. What is W for each point and how is it set? I assume something like 'weight', rather than a third dimension, but neither Bezier nor Catmull-Rom curves have an additional per-vertex parameter [0]. And NURBS are a ratio of two B-splines, but you still have only X, Y & W [1].<p>2. In the demo, each of the curves is listed as 'degree 2' (which suggests quadratic to me). Catmull-Rom curves are cubic (and it appears yours are too), so maybe 'degree 2' means cubic. But then the Bezier: they can be any degree, but the example only interpolates its first and last of five points. Even if the control points are hidden, it is clearly neither cubic nor quadratic, unless new control points are being generated somewhere but not shown.<p>3. NURBS are non-uniform, but here they are locked to fixed sequential integer parameters. With that and the lack of a ratio, is it really just some variant of a b-spline?<p>4. Given the list of XYW triples, in JSON, and the non-standard math, how do you anticipate a user constructing the curve - would they have to use your code and the way it interprets the W parameter?<p>Oh, and<p>> Motivated by the lack of freely available curve designers<p>seems like an extraordinary claim.<p>---<p>Edit to add:<p>[0] Did you mean the cardinal curve / canonical spline, rather than Catmull-Rom? That does have a parameter (tension, usually τ rather than W). It's not very common, but that's the best I can think.<p>[1] To be fair, editors often don't expose the ratio, and expose only a knot vector. But the W values given don't seem to correspond with knots either: there is the wrong number for a start, isn't there?"],
        "story_type":"ShowHN",
        "url_raw":"https://github.com/AntixK/Curve-Studio",
        "url_text":"Squirrel Curve Studio Squirrel Curve Studio is an intuitive easy-to-use app for creating beautiful spline curves (free-form curves). Motivated by the lack of freely available curve designers, Squirrel provides a clean UI for designing, exporting and importing free-form curves. Interface Demo For Developers I welcome pull-requests for any new functionality or improving the app's experience overall. I am currently looking towards decreasing the app size using Neutralino.js. To get started, clone this repository using Git and install npm along with Node.js. From your command line: # Clone this repository git clone https://github.com/AntixK/Curve-Studio # Go into the repository cd Curve-Studio # Install dependencies npm install # Run the app npm start For End-Users See releases for getting pre-built binaries. Built With P5.js - For majority of the UI and curve design Electron - For creating the desktop app Electron Packager - For building the app for all platforms License This project is licensed under the MIT License - see the Curve_Studio_LICENSE.mdd file for details. Acknowledgments Hat tip to Jose Luis Garcia del Castillo for providing the p5js-electron template ",
        "comments.comment_id":[20189880,
          20190269],
        "comments.comment_author":["Tistron",
          "sago"],
        "comments.comment_descendants":[2,
          1],
        "comments.comment_time":["2019-06-15T13:15:16Z",
          "2019-06-15T14:55:01Z"],
        "comments.comment_text":["Looks great, but why is it not a web site? Why electron?",
          "Well done for releasing something and hitting the top of HN.<p>It looks like a clean UI, but I'm confused what we're seeing.<p>1. What is W for each point and how is it set? I assume something like 'weight', rather than a third dimension, but neither Bezier nor Catmull-Rom curves have an additional per-vertex parameter [0]. And NURBS are a ratio of two B-splines, but you still have only X, Y & W [1].<p>2. In the demo, each of the curves is listed as 'degree 2' (which suggests quadratic to me). Catmull-Rom curves are cubic (and it appears yours are too), so maybe 'degree 2' means cubic. But then the Bezier: they can be any degree, but the example only interpolates its first and last of five points. Even if the control points are hidden, it is clearly neither cubic nor quadratic, unless new control points are being generated somewhere but not shown.<p>3. NURBS are non-uniform, but here they are locked to fixed sequential integer parameters. With that and the lack of a ratio, is it really just some variant of a b-spline?<p>4. Given the list of XYW triples, in JSON, and the non-standard math, how do you anticipate a user constructing the curve - would they have to use your code and the way it interprets the W parameter?<p>Oh, and<p>> Motivated by the lack of freely available curve designers<p>seems like an extraordinary claim.<p>---<p>Edit to add:<p>[0] Did you mean the cardinal curve / canonical spline, rather than Catmull-Rom? That does have a parameter (tension, usually τ rather than W). It's not very common, but that's the best I can think.<p>[1] To be fair, editors often don't expose the ratio, and expose only a knot vector. But the W values given don't seem to correspond with knots either: there is the wrong number for a start, isn't there?"],
        "id":"14ad4ae9-5eb9-46ee-9332-166a117f36a7",
        "_version_":1718938190120747009},
      {
        "story_id":20524779,
        "story_author":"snovvcrash",
        "story_descendants":24,
        "story_score":136,
        "story_time":"2019-07-25T12:30:17Z",
        "story_title":"Show HN: CLI forensics tool for tracking USB device artifacts on Linux",
        "search":["Show HN: CLI forensics tool for tracking USB device artifacts on Linux",
          "ShowHN",
          "https://github.com/snovvcrash/usbrip",
          "usbrip (inherited from \"USB Ripper\", not \"USB R.I.P.\") is a simple forensics tool with command line interface that lets you keep track of USB device artifacts (i.e., USB event history) on Linux machines. Table of Contents: Description Quick Start Showcase System Log Structure Dependencies deb pip Manual installation Git Clone install.sh Paths Cron uninstall.sh Usage Synopsis Help Examples ToDo Credits & References Stargazers Chart Post Scriptum Description usbrip is a small piece of software which analyzes Linux log data: journalctl output or contents of /var/log/syslog* (or /var/log/messages*) files. Based on the collected data usbrip can build USB event history tables with the following columns: Connected (date & time) Host VID (vendor ID) PID (product ID) Product Manufacturer Serial Number Port Disconnected (date & time) Besides, it also can: Export collected data as a JSON dump for later use. Generate a list of authorized (trusted) USB devices as a JSON file (call it auth.json). Search for \"violation events\" based on auth.json: discover USB devices that do appear in history and do NOT appear in the auth.json file. *when installed with -s flag* Create protected storages (7-Zip archives) to automatically backup and accumulate USB events with the help of cron scheduler. Search additional details about a specific USB device based on its VID and/or PID. Quick Start Way 1. Install with pip: ~$ sudo -H python3 -m pip install -U usbrip ~$ usbrip -h Way 2. Install bleeding-edge with install.sh (recommended, extra features available): ~$ sudo apt install python3-venv p7zip-full -y ~$ git clone https://github.com/snovvcrash/usbrip && cd usbrip ~/usbrip$ sudo -H installers/install.sh ~/usbrip$ cd ~$ usbrip -h Showcase Docker (*DEMO ONLY!*) ~$ docker run --rm -it snovvcrash/usbrip System Log Structure usbrip supports two types of timestamps to parse within system log files: Non-modified standard syslog structure for GNU/Linux (\"%b %d %H:%M:%S\", ex. \"Jan 1 00:00:00\"). This type of timestamp does not provide the information about the year. Modified (recommended) better syslog structure which provides high precision timestamps including years (\"%Y-%m-%dT%H:%M:%S.%f%z\", ex. \"1970-01-01T00:00:00.000000-00:00\"). If you do have journalctl installed, then there's nothing to worry about as it can convert timestamps on the fly. Otherwise, the desired syslog structure can be achieved by setting RSYSLOG_FileFormat format in rsyslog configuration. Comment out the following line in /etc/rsyslog.conf: $ActionFileDefaultTemplate RSYSLOG_TraditionalFileFormat Add custom .conf file for usbrip: ~$ echo '$ActionFileDefaultTemplate RSYSLOG_FileFormat' | sudo tee /etc/rsyslog.d/usbrip.conf *optional* Delete existing log files: ~$ sudo rm -f /var/log/syslog* /var/log/messages* Restart the service: ~$ sudo systemctl restart rsyslog Firstly, usbrip will check if there is a chance to dump system events using journalctl as the most portable option. If not it will search for and parse /var/log/syslog* or /var/log/messages* system log files. Dependencies deb python3.6 interpreter (or newer) python3-venv p7zip-full (used by storage module) pip terminaltables termcolor tqdm Manual installation Git Clone For simplicity, lets agree that all the commands where ~/usbrip$ prefix is appeared are executed in the ~/usbrip directory which is created as a result of a git clone: ~$ git clone https://github.com/snovvcrash/usbrip ~$ cd usbrip ~/usbrip$ pwd install.sh Besides installing with pip, usbrip can also be installed with custom installers/install.sh script. When using install.sh some extra features become available: The virtual environment is created automatically. You can use the storage module set a cron job to backup USB events on a schedule (example of a cron job can be found in usbrip/cron/usbrip.cron). Warning: if you are using cron scheduling, you want to configure the crontab with sudo crontab -e in order to force the storage update submodule run as root. The storage passwords are kept in /var/opt/usbrip/usbrip.ini and accessible by root only by default. To install usbrip with install.sh use: ~/usbrip$ sudo -H installers/install.sh [-l/--local] [-s/--storages] ~/usbrip$ cd ~$ usbrip -h When -l switch is enabled, Python dependencies are resolved from local .tar packages (3rdPartyTools directory) instead of PyPI. When -s switch is enabled, not only the usbrip project is installed but also the list of trusted USB devices, history and violations storages are created. After the installation completes feel free to remove the ~/usbrip directory. Paths When installed with install.sh, usbrip uses the following paths: /opt/usbrip/ project's main directory. /var/opt/usbrip/log/ usbrip cron logs. /var/opt/usbrip/storage/ USB event storages (history.7z and violations.7z, created during the installation process). /var/opt/usbrip/trusted/ lists of trusted USB devices (auth.json, created during the installation process). /var/opt/usbrip/usbrip.ini usbrip configuration file (contains passwords for 7-Zip storages). /usr/local/bin/usbrip symlink to the /opt/usbrip/venv/bin/usbrip script. Cron Cron jobs can be set as follows: ~/usbrip$ sudo crontab -l > tmpcron && echo \"\" >> tmpcron ~/usbrip$ cat usbrip/cron/usbrip.cron | tee -a tmpcron ~/usbrip$ sudo crontab tmpcron ~/usbrip$ rm tmpcron uninstall.sh The installers/uninstall.sh script removes usbrip and all the installation artifacts from your system. To uninstall usbrip use: ~/usbrip$ sudo installers/uninstall.sh [-a/--all] When -a switch is enabled, not only the usbrip project directory is deleted but also all the storages and usbrip logs are deleted too. Don't forget to remove the cron job if you had set up one. Usage Synopsis # ---------- BANNER ---------- ~$ usbrip banner Get usbrip banner. # ---------- EVENTS ---------- ~$ usbrip events history [-t | -l] [-e] [-n <NUMBER_OF_EVENTS>] [-d <DATE> [<DATE> ...]] [--host <HOST> [<HOST> ...]] [--vid <VID> [<VID> ...]] [--pid <PID> [<PID> ...]] [--prod <PROD> [<PROD> ...]] [--manufact <MANUFACT> [<MANUFACT> ...]] [--serial <SERIAL> [<SERIAL> ...]] [--port <PORT> [<PORT> ...]] [-c <COLUMN> [<COLUMN> ...]] [-f <FILE> [<FILE> ...]] [-q] [--debug] Get USB event history. ~$ usbrip events open <DUMP.JSON> [-t | -l] [-e] [-n <NUMBER_OF_EVENTS>] [-d <DATE> [<DATE> ...]] [--host <HOST> [<HOST> ...]] [--vid <VID> [<VID> ...]] [--pid <PID> [<PID> ...]] [--prod <PROD> [<PROD> ...]] [--manufact <MANUFACT> [<MANUFACT> ...]] [--serial <SERIAL> [<SERIAL> ...]] [--port <PORT> [<PORT> ...]] [-c <COLUMN> [<COLUMN> ...]] [-q] [--debug] Open USB event dump. ~$ sudo usbrip events genauth <OUT_AUTH.JSON> [-a <ATTRIBUTE> [<ATTRIBUTE> ...]] [-e] [-n <NUMBER_OF_EVENTS>] [-d <DATE> [<DATE> ...]] [--host <HOST> [<HOST> ...]] [--vid <VID> [<VID> ...]] [--pid <PID> [<PID> ...]] [--prod <PROD> [<PROD> ...]] [--manufact <MANUFACT> [<MANUFACT> ...]] [--serial <SERIAL> [<SERIAL> ...]] [--port <PORT> [<PORT> ...]] [-f <FILE> [<FILE> ...]] [-q] [--debug] Generate a list of trusted (authorized) USB devices. ~$ sudo usbrip events violations <IN_AUTH.JSON> [-a <ATTRIBUTE> [<ATTRIBUTE> ...]] [-t | -l] [-e] [-n <NUMBER_OF_EVENTS>] [-d <DATE> [<DATE> ...]] [--host <HOST> [<HOST> ...]] [--vid <VID> [<VID> ...]] [--pid <PID> [<PID> ...]] [--prod <PROD> [<PROD> ...]] [--manufact <MANUFACT> [<MANUFACT> ...]] [--serial <SERIAL> [<SERIAL> ...]] [--port <PORT> [<PORT> ...]] [-c <COLUMN> [<COLUMN> ...]] [-f <FILE> [<FILE> ...]] [-q] [--debug] Get USB violation events based on the list of trusted devices. # ---------- STORAGE ---------- ~$ sudo usbrip storage list <STORAGE_TYPE> [-q] [--debug] List contents of the selected storage. STORAGE_TYPE is either \"history\" or \"violations\". ~$ sudo usbrip storage open <STORAGE_TYPE> [-t | -l] [-e] [-n <NUMBER_OF_EVENTS>] [-d <DATE> [<DATE> ...]] [--host <HOST> [<HOST> ...]] [--vid <VID> [<VID> ...]] [--pid <PID> [<PID> ...]] [--prod <PROD> [<PROD> ...]] [--manufact <MANUFACT> [<MANUFACT> ...]] [--serial <SERIAL> [<SERIAL> ...]] [--port <PORT> [<PORT> ...]] [-c <COLUMN> [<COLUMN> ...]] [-q] [--debug] Open selected storage. Behaves similarly to the EVENTS OPEN submodule. ~$ sudo usbrip storage update <STORAGE_TYPE> [IN_AUTH.JSON] [-a <ATTRIBUTE> [<ATTRIBUTE> ...]] [-e] [-n <NUMBER_OF_EVENTS>] [-d <DATE> [<DATE> ...]] [--host <HOST> [<HOST> ...]] [--vid <VID> [<VID> ...]] [--pid <PID> [<PID> ...]] [--prod <PROD> [<PROD> ...]] [--manufact <MANUFACT> [<MANUFACT> ...]] [--serial <SERIAL> [<SERIAL> ...]] [--port <PORT> [<PORT> ...]] [--lvl <COMPRESSION_LEVEL>] [-q] [--debug] Update storage -- add USB events to the existing storage. COMPRESSION_LEVEL is a number in [0..9]. ~$ sudo usbrip storage create <STORAGE_TYPE> [IN_AUTH.JSON] [-a <ATTRIBUTE> [<ATTRIBUTE> ...]] [-e] [-n <NUMBER_OF_EVENTS>] [-d <DATE> [<DATE> ...]] [--host <HOST> [<HOST> ...]] [--vid <VID> [<VID> ...]] [--pid <PID> [<PID> ...]] [--prod <PROD> [<PROD> ...]] [--manufact <MANUFACT> [<MANUFACT> ...]] [--serial <SERIAL> [<SERIAL> ...]] [--port <PORT> [<PORT> ...]] [--lvl <COMPRESSION_LEVEL>] [-q] [--debug] Create storage -- create 7-Zip archive and add USB events to it according to the selected options. ~$ sudo usbrip storage passwd <STORAGE_TYPE> [--lvl <COMPRESSION_LEVEL>] [-q] [--debug] Change password of the existing storage. # ---------- IDs ---------- ~$ usbrip ids search [--vid <VID>] [--pid <PID>] [--offline] [-q] [--debug] Get extra details about a specific USB device by its <VID> and/or <PID> from the USB ID database. ~$ usbrip ids download [-q] [--debug] Update (download) the USB ID database. Help To get a list of module names use: To get a list of submodule names for a specific module use: To get a list of all switches for a specific submodule use: ~$ usbrip <MODULE> <SUBMODULE> -h Examples Show the event history of all USB devices, suppressing banner output, info messages and user interaction (-q, --quiet), represented as a list (-l, --list) with latest 100 entries (-n NUMBER, --number NUMBER): ~$ usbrip events history -ql -n 100 Show the event history of the external USB devices (-e, --external, which were actually disconnected) represented as a table (-t, --table) containing Connected, VID, PID, Disconnected and Serial Number columns (-c COLUMN [COLUMN ...], --column COLUMN [COLUMN ...]) filtered by date (-d DATE [DATE ...], --date DATE [DATE ...]) and PID (--pid <PID> [<PID> ...]) with logs taken from outer files (-f FILE [FILE ...], --file FILE [FILE ...]): ~$ usbrip events history -et -c conn vid pid disconn serial -d '1995-09-15' '2018-07-01' --pid 1337 -f /var/log/syslog.1 /var/log/syslog.2.gz Note: there is a thing to remember when working with filters. There are 4 types of filtering available: only external USB events (devices that can be pulled out easily, -e), by date (-d), by fields (--host, --vid, --pid, --product, --manufact, --serial, --port) and by number of entries you get as the output (-n). When applying different filters simultaneously, you will get the following behavior: firstly, external and by date filters are applied, then usbrip will search for specified field values in the intersection of the last two filters, and in the end it will cut the output to the number you defined with the -n option. So think of it as an intersection for external and by date filtering and union for by fields filtering. Hope it makes sense. Build the event history of all USB devices and redirect the output to a file for further analysis. When the output stream is NOT terminal stdout (| or > for example) there would be no ANSI escape characters (color) in the output so feel free to use it that way. Also notice that usbrip uses some UNICODE symbols so it would be nice to convert the resulting file to UTF-8 encoding (with encov for example) as well as change newline characters to Windows style for portability (with awk for example): ~$ usbrip events history -t | awk '{ sub(\"$\", \"\\r\"); print }' > usbrip.out && enconv -x UTF8 usbrip.out Note: you can always get rid of the escape characters by yourself even if you have already got the output to stdout. To do that just copy the output data to usbrip.out and apply one more awk instruction: ~$ awk '{ sub(\"$\", \"\\r\"); gsub(\"\\\\x1B\\\\[[0-?]*[ -/]*[@-~]\", \"\"); print }' usbrip.out && enconv -x UTF8 usbrip.out Generate a list of trusted USB devices as a JSON file (trusted/auth.json) with VID and PID attributes containing the first three devices connected on November 30, 1984: ~$ sudo usbrip events genauth trusted/auth.json -a vid pid -n 3 -d '1984-11-30' Warning: there are cases when different USB flash drives might have identical serial numbers. This could happen as a result of a manufacturing error or just some black hats were able to rewrite the drive's memory chip which turned out to be non-one-time programmable and so on... Anyways, \"no system is safe\". usbrip does not handle such cases in a smart way so far, namely it will treat a pair of devices with identical SNs (if there exists one) as the same device regarding to the trusted device list and genauth module. Search the event history of the external USB devices for violations based on the list of trusted USB devices (trusted/auth.json) by PID attribute, restrict resulting events to those which have Bob-PC as a hostname, EvilUSBManufacturer as a manufacturer, 0123456789 as a serial number and represent the output as a table with Connected, VID and PID columns: ~$ sudo usbrip events violations trusted/auth.json -a pid -et --host Bob-PC --manufact EvilUSBManufacturer --serial 0123456789 -c conn vid pid Search for details about a specific USB device by its VID (--vid VID) and PID (--pid PID): ~$ usbrip ids search --vid 0781 --pid 5580 Download the latest version of usb.ids database: ToDo Migrate from 7-Zip archives as the protected storages to SQLite DB Credits & References usbrip / Kali Linux , USB Linux / HackWare.ru usbrip: USB- , / Codeby.net Stargazers Chart Post Scriptum If this tool has been useful for you, feel free to buy me a coffee. ",
          "Wouldn't this be more reliable by recording udev events instead of parsing the syslog?",
          "Question:\nFrom the screenshots it seems like no year is shown?\nOnly month, day, time.<p>It would be IMHO advisable to use not the name of the month and <i>somehow</i> fit in the line the year.<p>Note:\nSmall typo: the past of \"shut down\" is \"shut down\" and not \"shutted down\"."],
        "story_type":"ShowHN",
        "url_raw":"https://github.com/snovvcrash/usbrip",
        "comments.comment_id":[20524873,
          20525635],
        "comments.comment_author":["MartijnBraam",
          "jaclaz"],
        "comments.comment_descendants":[3,
          1],
        "comments.comment_time":["2019-07-25T12:47:50Z",
          "2019-07-25T14:28:56Z"],
        "comments.comment_text":["Wouldn't this be more reliable by recording udev events instead of parsing the syslog?",
          "Question:\nFrom the screenshots it seems like no year is shown?\nOnly month, day, time.<p>It would be IMHO advisable to use not the name of the month and <i>somehow</i> fit in the line the year.<p>Note:\nSmall typo: the past of \"shut down\" is \"shut down\" and not \"shutted down\"."],
        "id":"1858e1b3-2812-4dc3-bfcd-419b1fabd7c4",
        "url_text":"usbrip (inherited from \"USB Ripper\", not \"USB R.I.P.\") is a simple forensics tool with command line interface that lets you keep track of USB device artifacts (i.e., USB event history) on Linux machines. Table of Contents: Description Quick Start Showcase System Log Structure Dependencies deb pip Manual installation Git Clone install.sh Paths Cron uninstall.sh Usage Synopsis Help Examples ToDo Credits & References Stargazers Chart Post Scriptum Description usbrip is a small piece of software which analyzes Linux log data: journalctl output or contents of /var/log/syslog* (or /var/log/messages*) files. Based on the collected data usbrip can build USB event history tables with the following columns: Connected (date & time) Host VID (vendor ID) PID (product ID) Product Manufacturer Serial Number Port Disconnected (date & time) Besides, it also can: Export collected data as a JSON dump for later use. Generate a list of authorized (trusted) USB devices as a JSON file (call it auth.json). Search for \"violation events\" based on auth.json: discover USB devices that do appear in history and do NOT appear in the auth.json file. *when installed with -s flag* Create protected storages (7-Zip archives) to automatically backup and accumulate USB events with the help of cron scheduler. Search additional details about a specific USB device based on its VID and/or PID. Quick Start Way 1. Install with pip: ~$ sudo -H python3 -m pip install -U usbrip ~$ usbrip -h Way 2. Install bleeding-edge with install.sh (recommended, extra features available): ~$ sudo apt install python3-venv p7zip-full -y ~$ git clone https://github.com/snovvcrash/usbrip && cd usbrip ~/usbrip$ sudo -H installers/install.sh ~/usbrip$ cd ~$ usbrip -h Showcase Docker (*DEMO ONLY!*) ~$ docker run --rm -it snovvcrash/usbrip System Log Structure usbrip supports two types of timestamps to parse within system log files: Non-modified standard syslog structure for GNU/Linux (\"%b %d %H:%M:%S\", ex. \"Jan 1 00:00:00\"). This type of timestamp does not provide the information about the year. Modified (recommended) better syslog structure which provides high precision timestamps including years (\"%Y-%m-%dT%H:%M:%S.%f%z\", ex. \"1970-01-01T00:00:00.000000-00:00\"). If you do have journalctl installed, then there's nothing to worry about as it can convert timestamps on the fly. Otherwise, the desired syslog structure can be achieved by setting RSYSLOG_FileFormat format in rsyslog configuration. Comment out the following line in /etc/rsyslog.conf: $ActionFileDefaultTemplate RSYSLOG_TraditionalFileFormat Add custom .conf file for usbrip: ~$ echo '$ActionFileDefaultTemplate RSYSLOG_FileFormat' | sudo tee /etc/rsyslog.d/usbrip.conf *optional* Delete existing log files: ~$ sudo rm -f /var/log/syslog* /var/log/messages* Restart the service: ~$ sudo systemctl restart rsyslog Firstly, usbrip will check if there is a chance to dump system events using journalctl as the most portable option. If not it will search for and parse /var/log/syslog* or /var/log/messages* system log files. Dependencies deb python3.6 interpreter (or newer) python3-venv p7zip-full (used by storage module) pip terminaltables termcolor tqdm Manual installation Git Clone For simplicity, lets agree that all the commands where ~/usbrip$ prefix is appeared are executed in the ~/usbrip directory which is created as a result of a git clone: ~$ git clone https://github.com/snovvcrash/usbrip ~$ cd usbrip ~/usbrip$ pwd install.sh Besides installing with pip, usbrip can also be installed with custom installers/install.sh script. When using install.sh some extra features become available: The virtual environment is created automatically. You can use the storage module set a cron job to backup USB events on a schedule (example of a cron job can be found in usbrip/cron/usbrip.cron). Warning: if you are using cron scheduling, you want to configure the crontab with sudo crontab -e in order to force the storage update submodule run as root. The storage passwords are kept in /var/opt/usbrip/usbrip.ini and accessible by root only by default. To install usbrip with install.sh use: ~/usbrip$ sudo -H installers/install.sh [-l/--local] [-s/--storages] ~/usbrip$ cd ~$ usbrip -h When -l switch is enabled, Python dependencies are resolved from local .tar packages (3rdPartyTools directory) instead of PyPI. When -s switch is enabled, not only the usbrip project is installed but also the list of trusted USB devices, history and violations storages are created. After the installation completes feel free to remove the ~/usbrip directory. Paths When installed with install.sh, usbrip uses the following paths: /opt/usbrip/ project's main directory. /var/opt/usbrip/log/ usbrip cron logs. /var/opt/usbrip/storage/ USB event storages (history.7z and violations.7z, created during the installation process). /var/opt/usbrip/trusted/ lists of trusted USB devices (auth.json, created during the installation process). /var/opt/usbrip/usbrip.ini usbrip configuration file (contains passwords for 7-Zip storages). /usr/local/bin/usbrip symlink to the /opt/usbrip/venv/bin/usbrip script. Cron Cron jobs can be set as follows: ~/usbrip$ sudo crontab -l > tmpcron && echo \"\" >> tmpcron ~/usbrip$ cat usbrip/cron/usbrip.cron | tee -a tmpcron ~/usbrip$ sudo crontab tmpcron ~/usbrip$ rm tmpcron uninstall.sh The installers/uninstall.sh script removes usbrip and all the installation artifacts from your system. To uninstall usbrip use: ~/usbrip$ sudo installers/uninstall.sh [-a/--all] When -a switch is enabled, not only the usbrip project directory is deleted but also all the storages and usbrip logs are deleted too. Don't forget to remove the cron job if you had set up one. Usage Synopsis # ---------- BANNER ---------- ~$ usbrip banner Get usbrip banner. # ---------- EVENTS ---------- ~$ usbrip events history [-t | -l] [-e] [-n <NUMBER_OF_EVENTS>] [-d <DATE> [<DATE> ...]] [--host <HOST> [<HOST> ...]] [--vid <VID> [<VID> ...]] [--pid <PID> [<PID> ...]] [--prod <PROD> [<PROD> ...]] [--manufact <MANUFACT> [<MANUFACT> ...]] [--serial <SERIAL> [<SERIAL> ...]] [--port <PORT> [<PORT> ...]] [-c <COLUMN> [<COLUMN> ...]] [-f <FILE> [<FILE> ...]] [-q] [--debug] Get USB event history. ~$ usbrip events open <DUMP.JSON> [-t | -l] [-e] [-n <NUMBER_OF_EVENTS>] [-d <DATE> [<DATE> ...]] [--host <HOST> [<HOST> ...]] [--vid <VID> [<VID> ...]] [--pid <PID> [<PID> ...]] [--prod <PROD> [<PROD> ...]] [--manufact <MANUFACT> [<MANUFACT> ...]] [--serial <SERIAL> [<SERIAL> ...]] [--port <PORT> [<PORT> ...]] [-c <COLUMN> [<COLUMN> ...]] [-q] [--debug] Open USB event dump. ~$ sudo usbrip events genauth <OUT_AUTH.JSON> [-a <ATTRIBUTE> [<ATTRIBUTE> ...]] [-e] [-n <NUMBER_OF_EVENTS>] [-d <DATE> [<DATE> ...]] [--host <HOST> [<HOST> ...]] [--vid <VID> [<VID> ...]] [--pid <PID> [<PID> ...]] [--prod <PROD> [<PROD> ...]] [--manufact <MANUFACT> [<MANUFACT> ...]] [--serial <SERIAL> [<SERIAL> ...]] [--port <PORT> [<PORT> ...]] [-f <FILE> [<FILE> ...]] [-q] [--debug] Generate a list of trusted (authorized) USB devices. ~$ sudo usbrip events violations <IN_AUTH.JSON> [-a <ATTRIBUTE> [<ATTRIBUTE> ...]] [-t | -l] [-e] [-n <NUMBER_OF_EVENTS>] [-d <DATE> [<DATE> ...]] [--host <HOST> [<HOST> ...]] [--vid <VID> [<VID> ...]] [--pid <PID> [<PID> ...]] [--prod <PROD> [<PROD> ...]] [--manufact <MANUFACT> [<MANUFACT> ...]] [--serial <SERIAL> [<SERIAL> ...]] [--port <PORT> [<PORT> ...]] [-c <COLUMN> [<COLUMN> ...]] [-f <FILE> [<FILE> ...]] [-q] [--debug] Get USB violation events based on the list of trusted devices. # ---------- STORAGE ---------- ~$ sudo usbrip storage list <STORAGE_TYPE> [-q] [--debug] List contents of the selected storage. STORAGE_TYPE is either \"history\" or \"violations\". ~$ sudo usbrip storage open <STORAGE_TYPE> [-t | -l] [-e] [-n <NUMBER_OF_EVENTS>] [-d <DATE> [<DATE> ...]] [--host <HOST> [<HOST> ...]] [--vid <VID> [<VID> ...]] [--pid <PID> [<PID> ...]] [--prod <PROD> [<PROD> ...]] [--manufact <MANUFACT> [<MANUFACT> ...]] [--serial <SERIAL> [<SERIAL> ...]] [--port <PORT> [<PORT> ...]] [-c <COLUMN> [<COLUMN> ...]] [-q] [--debug] Open selected storage. Behaves similarly to the EVENTS OPEN submodule. ~$ sudo usbrip storage update <STORAGE_TYPE> [IN_AUTH.JSON] [-a <ATTRIBUTE> [<ATTRIBUTE> ...]] [-e] [-n <NUMBER_OF_EVENTS>] [-d <DATE> [<DATE> ...]] [--host <HOST> [<HOST> ...]] [--vid <VID> [<VID> ...]] [--pid <PID> [<PID> ...]] [--prod <PROD> [<PROD> ...]] [--manufact <MANUFACT> [<MANUFACT> ...]] [--serial <SERIAL> [<SERIAL> ...]] [--port <PORT> [<PORT> ...]] [--lvl <COMPRESSION_LEVEL>] [-q] [--debug] Update storage -- add USB events to the existing storage. COMPRESSION_LEVEL is a number in [0..9]. ~$ sudo usbrip storage create <STORAGE_TYPE> [IN_AUTH.JSON] [-a <ATTRIBUTE> [<ATTRIBUTE> ...]] [-e] [-n <NUMBER_OF_EVENTS>] [-d <DATE> [<DATE> ...]] [--host <HOST> [<HOST> ...]] [--vid <VID> [<VID> ...]] [--pid <PID> [<PID> ...]] [--prod <PROD> [<PROD> ...]] [--manufact <MANUFACT> [<MANUFACT> ...]] [--serial <SERIAL> [<SERIAL> ...]] [--port <PORT> [<PORT> ...]] [--lvl <COMPRESSION_LEVEL>] [-q] [--debug] Create storage -- create 7-Zip archive and add USB events to it according to the selected options. ~$ sudo usbrip storage passwd <STORAGE_TYPE> [--lvl <COMPRESSION_LEVEL>] [-q] [--debug] Change password of the existing storage. # ---------- IDs ---------- ~$ usbrip ids search [--vid <VID>] [--pid <PID>] [--offline] [-q] [--debug] Get extra details about a specific USB device by its <VID> and/or <PID> from the USB ID database. ~$ usbrip ids download [-q] [--debug] Update (download) the USB ID database. Help To get a list of module names use: To get a list of submodule names for a specific module use: To get a list of all switches for a specific submodule use: ~$ usbrip <MODULE> <SUBMODULE> -h Examples Show the event history of all USB devices, suppressing banner output, info messages and user interaction (-q, --quiet), represented as a list (-l, --list) with latest 100 entries (-n NUMBER, --number NUMBER): ~$ usbrip events history -ql -n 100 Show the event history of the external USB devices (-e, --external, which were actually disconnected) represented as a table (-t, --table) containing Connected, VID, PID, Disconnected and Serial Number columns (-c COLUMN [COLUMN ...], --column COLUMN [COLUMN ...]) filtered by date (-d DATE [DATE ...], --date DATE [DATE ...]) and PID (--pid <PID> [<PID> ...]) with logs taken from outer files (-f FILE [FILE ...], --file FILE [FILE ...]): ~$ usbrip events history -et -c conn vid pid disconn serial -d '1995-09-15' '2018-07-01' --pid 1337 -f /var/log/syslog.1 /var/log/syslog.2.gz Note: there is a thing to remember when working with filters. There are 4 types of filtering available: only external USB events (devices that can be pulled out easily, -e), by date (-d), by fields (--host, --vid, --pid, --product, --manufact, --serial, --port) and by number of entries you get as the output (-n). When applying different filters simultaneously, you will get the following behavior: firstly, external and by date filters are applied, then usbrip will search for specified field values in the intersection of the last two filters, and in the end it will cut the output to the number you defined with the -n option. So think of it as an intersection for external and by date filtering and union for by fields filtering. Hope it makes sense. Build the event history of all USB devices and redirect the output to a file for further analysis. When the output stream is NOT terminal stdout (| or > for example) there would be no ANSI escape characters (color) in the output so feel free to use it that way. Also notice that usbrip uses some UNICODE symbols so it would be nice to convert the resulting file to UTF-8 encoding (with encov for example) as well as change newline characters to Windows style for portability (with awk for example): ~$ usbrip events history -t | awk '{ sub(\"$\", \"\\r\"); print }' > usbrip.out && enconv -x UTF8 usbrip.out Note: you can always get rid of the escape characters by yourself even if you have already got the output to stdout. To do that just copy the output data to usbrip.out and apply one more awk instruction: ~$ awk '{ sub(\"$\", \"\\r\"); gsub(\"\\\\x1B\\\\[[0-?]*[ -/]*[@-~]\", \"\"); print }' usbrip.out && enconv -x UTF8 usbrip.out Generate a list of trusted USB devices as a JSON file (trusted/auth.json) with VID and PID attributes containing the first three devices connected on November 30, 1984: ~$ sudo usbrip events genauth trusted/auth.json -a vid pid -n 3 -d '1984-11-30' Warning: there are cases when different USB flash drives might have identical serial numbers. This could happen as a result of a manufacturing error or just some black hats were able to rewrite the drive's memory chip which turned out to be non-one-time programmable and so on... Anyways, \"no system is safe\". usbrip does not handle such cases in a smart way so far, namely it will treat a pair of devices with identical SNs (if there exists one) as the same device regarding to the trusted device list and genauth module. Search the event history of the external USB devices for violations based on the list of trusted USB devices (trusted/auth.json) by PID attribute, restrict resulting events to those which have Bob-PC as a hostname, EvilUSBManufacturer as a manufacturer, 0123456789 as a serial number and represent the output as a table with Connected, VID and PID columns: ~$ sudo usbrip events violations trusted/auth.json -a pid -et --host Bob-PC --manufact EvilUSBManufacturer --serial 0123456789 -c conn vid pid Search for details about a specific USB device by its VID (--vid VID) and PID (--pid PID): ~$ usbrip ids search --vid 0781 --pid 5580 Download the latest version of usb.ids database: ToDo Migrate from 7-Zip archives as the protected storages to SQLite DB Credits & References usbrip / Kali Linux , USB Linux / HackWare.ru usbrip: USB- , / Codeby.net Stargazers Chart Post Scriptum If this tool has been useful for you, feel free to buy me a coffee. ",
        "_version_":1718938202438369280},
      {
        "story_id":21270861,
        "story_author":"slowhand09",
        "story_descendants":3,
        "story_score":17,
        "story_time":"2019-10-16T14:41:57Z",
        "story_title":"Pack Your Bags – Systemd Is Taking You to a New Home",
        "search":["Pack Your Bags – Systemd Is Taking You to a New Home",
          "Normal",
          "https://hackaday.com/2019/10/16/pack-your-bags-systemd-is-taking-you-to-a-new-home/",
          "Home directories have been a fundamental part on any Unixy system since day one. Theyre such a basic element, we usually dont give them much thought. And why would we? From a low level point of view, whatever location $HOME is pointing to, is a directory just like any other of the countless ones you will find on the system apart from maybe being located on its own disk partition. Home directories are so unspectacular in their nature, it wouldnt usually cross anyones mind to even consider to change anything about them. And then theres Lennart Poettering. In case youre not familiar with the name, he is the main developer behind the systemd init system, which has nowadays been adopted by the majority of Linux distributions as replacement for its oldschool, Unix-style init-system predecessors, essentially changing everything we knew about the system boot process. Not only did this change personally insult every single Perl-loving, Ken-Thompson-action-figure-owning grey beard, it engendered contempt towards systemd and Lennart himself that approaches Nickelback level. At this point, it probably doesnt matter anymore what he does next, haters gonna hate. So who better than him to disrupt everything we know about home directories? Where you _live_? Although, home directories are just one part of the equation that his latest creation the systemd-homed project is going to make people hate him even more tackle. The big picture is really more about the whole concept of user management as we know it, which sounds bold and scary, but which in its current state is also a lot more flawed than we might realize. So lets have a look at what its all about, the motivation behind homed, the problems its going to both solve and raise, and how its maybe time to leave some outdated philosophies behind us. A Quick Introduction To Systemd Just one more module Image: Shmuel Csaba Otto Traian [CC BY-SA 3.0], via Wikimedia CommonsBefore we get into the homed part of the endeavor, lets bring everyone on the same page about systemd itself. In simple words, systemd handles everything that lies beyond the kernels tasks to start up the system: setting up the user space and providing everything required to get all system processes and daemons running, and managing those processes along the way. Its both replacing the old init process running with PID 1, and providing an administrative layer between the kernel and user space, and as such, is coming with a whole suite of additional tools and components to achieve all that: system logger, login handling, network management, IPC for communication between each part, and so on. So, what was once handled by all sorts of single-purpose tools has since become part of systemd itself, which is one source of resentment from people latching on to the one tool, one job philosophy. Technically, each tool and component in systemd is still shipped as its own executable, but due to interdependency of these components theyre not really that standalone, and its really more just a logical separation. systemd-homed is therefore one additional such component, aimed to handle the user management and the home directories of those users, tightly integrated into the systemd infrastructure and all the additional services it provides. But why change it at all? Well, lets have a look at the current way users and home directories are handled in Linux, and how systemd-homed is planning to change that. The Status Quo Of $HOME And User Management Since the beginning of time, users have been stored in the /etc/passwd file, which includes among other things the username, a system-unique user id, and the home directory location. Traditionally, the users password was also stored in hashed form in that file and it might still be the case on some, for example embedded systems but was eventually moved to a separate /etc/shadow file, with more restricted file permissions. So, after successfully logging in to the system with the password found in the shadow file, the user starts off in whichever location the home directory entry in /etc/passwd is pointing to. Sure you want to live here? Rosedale homeby Sheba_Also,CC BY-SA 2.0 In other words, for the most fundamental process of logging in to a system, three individual parts that are completely independent and separated from each other are required. Thanks to well-defined rules that stem from simpler times and have since remained mostly untouched, and which every system and tool involved in the process commendably abides by, this has worked out just fine. But if we think about it: is this really the best possible approach? Note that this is just the most basic local user login. Throw in some network authentication, additional user-based resource management, or disk encryption, and you will notice that /etc/passwd isnt the place for any of that. And well, why would it be one tool, one job, right? As a result, instead of configuring everything possibly related to a user in one centralized user management system that is flexible enough for any present and future considerations, we just stack dozens of random configurations on top of and next to each other, with each one of them doing their own little thing. And we are somehow perfectly fine and content with that. Yet, if you had to design a similar system today from scratch, would you really opt for the same concept? Would your system architect, your teacher, or even you yourself really be fine with duplicate database entries (usernames both in passwd and shadow file), unenforced relationships (home directory entry and home directory itself), and just random additional data without rhyme or reason: resource management, PAM, network authentication, and so on? Well, as you may have guessed by now, Lennart Poettering isnt much a fan of that, and with systemd-homed he is aiming to unite all the separate configuration entities around user management into one centralized system, flexible enough to handle everything the future might require. Knock Knock Its Systemd So instead of each component having its own configuration for all users, systemd-homed is going to collect all the configuration data of each component based on the user itself, and store it in a user-specific record in form of a JSON file. The file will include all the obvious information such as username, group membership, and password hashes, but also any user-dependent system configurations and resource management information, and essentially really just anything relevant. Being JSON, it can virtually contain whatever you want to put there, meaning it is easily extendable whenever new features and capabilities are required. No need to wonder anymore which of those three dozen files you need to touch if you want to change something. In addition to user and user-based system management, the home directory itself will be linked to it as a LUKS encrypted container and this is where the interesting part comes, even if you dont see a need for a unified configuration place: the encryption is directly coupled to the user login itself, meaning not only is the disk automatically decrypted once the user logs in, it is equally automatic encrypted again as soon as the user logs out, locks the screen, or suspends the device. In other words, your data is inaccessible and secure whenever youre not logged in, while the operating system can continue to operate independently from that. There is, of course, one downside to that. Chicken / Egg Problem With SSH If the home directory requires an actively logged-in user to decrypt the home directory, theres going to be a problem with remote logins via SSH that depend on the SSH decryption key found inside the (at that point encrypted) home directory. For that reason, SSH simply wont work without a logged in local user, which of course defies the whole point of remote access. The OpenSSH logo. That being said, its worth noting that at this stage, systemd-homed is focusing mostly on regular, real human users on a desktop or laptop, and not so much system or service-specific users, or anything running remotely on a server. That sounds of course like a bad excuse, but keep also in mind that systemd-homed will change the very core of user handling, so its probably safe to assume that future iterations will diminish that separation and things will work properly for either type of user. I mean, no point to reinvent the wheel if you need to keep the old one around as well but feel free to call me naive here. Ideally, the SSH keys would be migrated to the common-place user management record, but that would require some work on SSHs side. This isnt happening right now, but then again, systemd-homed itself is also just a semi-implemented idea in a git branch at this point. Its unlikely going to be widely adapted and forced onto you by the evil Linux distributions until this issue in particular is solved but again, maybe Im just overly optimistically naive. Self-Contained Users With Portable Homes But with user management and home directory handling in a single place and coupled together, you can start to dream of additional possible features. For instance, portable home directories that double as self-contained users. What that means is that you could keep the home directory for example on a USB stick or external disk, and seamlessly move it between, say, your workstation at home and your laptop whenever youre on the move. No need to duplicate or otherwise sync your data, its all in one place with you. This brings security and portability benefits. Or would you rather live here? By ltenney1225 CC BY-NC 2.0 But thats not all. It wouldnt have to be your own device you can attach your home directory to, it could be a friends device or the presenter laptop at a conference or really any compatible device. As soon as you plug in your home directory into it, your whole user will automatically exist on that device as well. Well, sort of automatically. Obviously, no one would want some random people roaming around on their system, so the system owner / superuser will still have to grant you access to their system first, with a user and resource configuration based on their own terms, and signed to avoid tempering. Admittedly, for some of you, this might all sound less like amazing new features and more like a security nightmare and mayhem just waiting to happen. And it most certainly will bring a massive change to yet another core element of Linux where a lot could possibly go wrong. How it will all play out in reality remains to be seen. Clearly it wont happen over night, and it wont happen out of nowhere either the changes are just too deep and fundamental. But being part of systemd itself, and seeing its influence on Linux, one has to entertain the idea that this might happen one day in the not too distant future. And maybe it should. Times Are Changing Yes, this will disrupt everything we know about user management. And yes, it goes against everything Unix stands for. But it also gets rid of an ancient concept that goes against everything the software engineering world has painfully learned as best practices over all these decades since. After all, its neither the 70s nor the 80s anymore. We dont have wildly heterogeneous environments anymore where we need to find the least capable common denominator for each and every system anymore. Linux is the dominant, and arguably the only really relevant open source Unix system nowadays even Netflix and WhatsApp running FreeBSD is really more anecdotal in comparison. So why should Linux really care about compatibility with niche operating systems that are in the end anyway going to do their own thing? And for that matter, why should we still care about Unix altogether? It generally begs the question, why do we keep on insisting that computer science as a discipline has reached its philosophical peak in a time the majority of its current practitioners werent even born yet? Why are we so frantically holding on to a philosophy that was established decades before the world of today with its ubiquitous internet, mobile connectivity, cloud computing, Infrastructure/Platform/Software/Whatnot-as-a-Service, and everything on top and below that has become reality? Are we really that nostalgic about the dark ages? Im not saying that the complexity we have reached with technology is necessarily an outcome we should have all hoped for, but it is where we are today. And its only going to get worse. The sooner we accept that and move on from here, and maybe adjust our ancient philosophies along the way, the higher the chance that we will actually manage to tame this development and keep control over it. Yes, change is a horrible, scary, and awful thing if its brought upon us by anyone else but ourselves. But its also inevitable, and systemd-homed might just be the proverbial broken egg that will give us a delicious omelette here. Time will tell. ",
          "The idea (portable homes) that I could ssh into a system and have my home directory come with me (with a personal vim config, shell, gitconfig, etc) on any newer Linux machine is very exciting",
          "> user-specific record in form of a JSON file<p>Can we please not do any more json at the command line? The age of Xml was terrible enough. Key/value pairs please"],
        "story_type":"Normal",
        "url_raw":"https://hackaday.com/2019/10/16/pack-your-bags-systemd-is-taking-you-to-a-new-home/",
        "comments.comment_id":[21272243,
          21272779],
        "comments.comment_author":["dickeytk",
          "exabrial"],
        "comments.comment_descendants":[0,
          0],
        "comments.comment_time":["2019-10-16T16:30:29Z",
          "2019-10-16T17:12:09Z"],
        "comments.comment_text":["The idea (portable homes) that I could ssh into a system and have my home directory come with me (with a personal vim config, shell, gitconfig, etc) on any newer Linux machine is very exciting",
          "> user-specific record in form of a JSON file<p>Can we please not do any more json at the command line? The age of Xml was terrible enough. Key/value pairs please"],
        "id":"dfe33912-c71b-42e5-b486-012ebb8919c9",
        "url_text":"Home directories have been a fundamental part on any Unixy system since day one. Theyre such a basic element, we usually dont give them much thought. And why would we? From a low level point of view, whatever location $HOME is pointing to, is a directory just like any other of the countless ones you will find on the system apart from maybe being located on its own disk partition. Home directories are so unspectacular in their nature, it wouldnt usually cross anyones mind to even consider to change anything about them. And then theres Lennart Poettering. In case youre not familiar with the name, he is the main developer behind the systemd init system, which has nowadays been adopted by the majority of Linux distributions as replacement for its oldschool, Unix-style init-system predecessors, essentially changing everything we knew about the system boot process. Not only did this change personally insult every single Perl-loving, Ken-Thompson-action-figure-owning grey beard, it engendered contempt towards systemd and Lennart himself that approaches Nickelback level. At this point, it probably doesnt matter anymore what he does next, haters gonna hate. So who better than him to disrupt everything we know about home directories? Where you _live_? Although, home directories are just one part of the equation that his latest creation the systemd-homed project is going to make people hate him even more tackle. The big picture is really more about the whole concept of user management as we know it, which sounds bold and scary, but which in its current state is also a lot more flawed than we might realize. So lets have a look at what its all about, the motivation behind homed, the problems its going to both solve and raise, and how its maybe time to leave some outdated philosophies behind us. A Quick Introduction To Systemd Just one more module Image: Shmuel Csaba Otto Traian [CC BY-SA 3.0], via Wikimedia CommonsBefore we get into the homed part of the endeavor, lets bring everyone on the same page about systemd itself. In simple words, systemd handles everything that lies beyond the kernels tasks to start up the system: setting up the user space and providing everything required to get all system processes and daemons running, and managing those processes along the way. Its both replacing the old init process running with PID 1, and providing an administrative layer between the kernel and user space, and as such, is coming with a whole suite of additional tools and components to achieve all that: system logger, login handling, network management, IPC for communication between each part, and so on. So, what was once handled by all sorts of single-purpose tools has since become part of systemd itself, which is one source of resentment from people latching on to the one tool, one job philosophy. Technically, each tool and component in systemd is still shipped as its own executable, but due to interdependency of these components theyre not really that standalone, and its really more just a logical separation. systemd-homed is therefore one additional such component, aimed to handle the user management and the home directories of those users, tightly integrated into the systemd infrastructure and all the additional services it provides. But why change it at all? Well, lets have a look at the current way users and home directories are handled in Linux, and how systemd-homed is planning to change that. The Status Quo Of $HOME And User Management Since the beginning of time, users have been stored in the /etc/passwd file, which includes among other things the username, a system-unique user id, and the home directory location. Traditionally, the users password was also stored in hashed form in that file and it might still be the case on some, for example embedded systems but was eventually moved to a separate /etc/shadow file, with more restricted file permissions. So, after successfully logging in to the system with the password found in the shadow file, the user starts off in whichever location the home directory entry in /etc/passwd is pointing to. Sure you want to live here? Rosedale homeby Sheba_Also,CC BY-SA 2.0 In other words, for the most fundamental process of logging in to a system, three individual parts that are completely independent and separated from each other are required. Thanks to well-defined rules that stem from simpler times and have since remained mostly untouched, and which every system and tool involved in the process commendably abides by, this has worked out just fine. But if we think about it: is this really the best possible approach? Note that this is just the most basic local user login. Throw in some network authentication, additional user-based resource management, or disk encryption, and you will notice that /etc/passwd isnt the place for any of that. And well, why would it be one tool, one job, right? As a result, instead of configuring everything possibly related to a user in one centralized user management system that is flexible enough for any present and future considerations, we just stack dozens of random configurations on top of and next to each other, with each one of them doing their own little thing. And we are somehow perfectly fine and content with that. Yet, if you had to design a similar system today from scratch, would you really opt for the same concept? Would your system architect, your teacher, or even you yourself really be fine with duplicate database entries (usernames both in passwd and shadow file), unenforced relationships (home directory entry and home directory itself), and just random additional data without rhyme or reason: resource management, PAM, network authentication, and so on? Well, as you may have guessed by now, Lennart Poettering isnt much a fan of that, and with systemd-homed he is aiming to unite all the separate configuration entities around user management into one centralized system, flexible enough to handle everything the future might require. Knock Knock Its Systemd So instead of each component having its own configuration for all users, systemd-homed is going to collect all the configuration data of each component based on the user itself, and store it in a user-specific record in form of a JSON file. The file will include all the obvious information such as username, group membership, and password hashes, but also any user-dependent system configurations and resource management information, and essentially really just anything relevant. Being JSON, it can virtually contain whatever you want to put there, meaning it is easily extendable whenever new features and capabilities are required. No need to wonder anymore which of those three dozen files you need to touch if you want to change something. In addition to user and user-based system management, the home directory itself will be linked to it as a LUKS encrypted container and this is where the interesting part comes, even if you dont see a need for a unified configuration place: the encryption is directly coupled to the user login itself, meaning not only is the disk automatically decrypted once the user logs in, it is equally automatic encrypted again as soon as the user logs out, locks the screen, or suspends the device. In other words, your data is inaccessible and secure whenever youre not logged in, while the operating system can continue to operate independently from that. There is, of course, one downside to that. Chicken / Egg Problem With SSH If the home directory requires an actively logged-in user to decrypt the home directory, theres going to be a problem with remote logins via SSH that depend on the SSH decryption key found inside the (at that point encrypted) home directory. For that reason, SSH simply wont work without a logged in local user, which of course defies the whole point of remote access. The OpenSSH logo. That being said, its worth noting that at this stage, systemd-homed is focusing mostly on regular, real human users on a desktop or laptop, and not so much system or service-specific users, or anything running remotely on a server. That sounds of course like a bad excuse, but keep also in mind that systemd-homed will change the very core of user handling, so its probably safe to assume that future iterations will diminish that separation and things will work properly for either type of user. I mean, no point to reinvent the wheel if you need to keep the old one around as well but feel free to call me naive here. Ideally, the SSH keys would be migrated to the common-place user management record, but that would require some work on SSHs side. This isnt happening right now, but then again, systemd-homed itself is also just a semi-implemented idea in a git branch at this point. Its unlikely going to be widely adapted and forced onto you by the evil Linux distributions until this issue in particular is solved but again, maybe Im just overly optimistically naive. Self-Contained Users With Portable Homes But with user management and home directory handling in a single place and coupled together, you can start to dream of additional possible features. For instance, portable home directories that double as self-contained users. What that means is that you could keep the home directory for example on a USB stick or external disk, and seamlessly move it between, say, your workstation at home and your laptop whenever youre on the move. No need to duplicate or otherwise sync your data, its all in one place with you. This brings security and portability benefits. Or would you rather live here? By ltenney1225 CC BY-NC 2.0 But thats not all. It wouldnt have to be your own device you can attach your home directory to, it could be a friends device or the presenter laptop at a conference or really any compatible device. As soon as you plug in your home directory into it, your whole user will automatically exist on that device as well. Well, sort of automatically. Obviously, no one would want some random people roaming around on their system, so the system owner / superuser will still have to grant you access to their system first, with a user and resource configuration based on their own terms, and signed to avoid tempering. Admittedly, for some of you, this might all sound less like amazing new features and more like a security nightmare and mayhem just waiting to happen. And it most certainly will bring a massive change to yet another core element of Linux where a lot could possibly go wrong. How it will all play out in reality remains to be seen. Clearly it wont happen over night, and it wont happen out of nowhere either the changes are just too deep and fundamental. But being part of systemd itself, and seeing its influence on Linux, one has to entertain the idea that this might happen one day in the not too distant future. And maybe it should. Times Are Changing Yes, this will disrupt everything we know about user management. And yes, it goes against everything Unix stands for. But it also gets rid of an ancient concept that goes against everything the software engineering world has painfully learned as best practices over all these decades since. After all, its neither the 70s nor the 80s anymore. We dont have wildly heterogeneous environments anymore where we need to find the least capable common denominator for each and every system anymore. Linux is the dominant, and arguably the only really relevant open source Unix system nowadays even Netflix and WhatsApp running FreeBSD is really more anecdotal in comparison. So why should Linux really care about compatibility with niche operating systems that are in the end anyway going to do their own thing? And for that matter, why should we still care about Unix altogether? It generally begs the question, why do we keep on insisting that computer science as a discipline has reached its philosophical peak in a time the majority of its current practitioners werent even born yet? Why are we so frantically holding on to a philosophy that was established decades before the world of today with its ubiquitous internet, mobile connectivity, cloud computing, Infrastructure/Platform/Software/Whatnot-as-a-Service, and everything on top and below that has become reality? Are we really that nostalgic about the dark ages? Im not saying that the complexity we have reached with technology is necessarily an outcome we should have all hoped for, but it is where we are today. And its only going to get worse. The sooner we accept that and move on from here, and maybe adjust our ancient philosophies along the way, the higher the chance that we will actually manage to tame this development and keep control over it. Yes, change is a horrible, scary, and awful thing if its brought upon us by anyone else but ourselves. But its also inevitable, and systemd-homed might just be the proverbial broken egg that will give us a delicious omelette here. Time will tell. ",
        "_version_":1718938230999482368}]
  }}
