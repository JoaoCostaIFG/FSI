{
  "docs": [
    {
      "id": 20559240,
      "title": "Learn a little jq, Awk and sed",
      "search": [
        "Learn a little jq, Awk and sed",
        "https://letterstoanewdeveloper.com/2019/07/29/learn-a-little-jq-awk-and-sed/",
        "Dear new developer, You are probably going to be dealing with text files sometime during your development career. These could be plain text, csv, or json. They may have data you want to get out, or log files you want to examine. You may be transforming from one format to another. Now, if this is a regular occurrence, you may want to build a script or a program around this problem (or use a third party service which aggregates everything together). But sometimes these files are one offs. Or you use them once in a blue moon. And it can take a little while to write a script, look at the libraries, and put it all together. Another alternative is to learn some of the unix tools available on the command line. Here are three that I consider table stakes. awk This is a multi purpose line processing utility. I often want to grab lines of a log file and figure out what is going on. Heres a few lines of a log file: 54.147.20.92 - - [26/Jul/2019:20:21:04 -0600] \"GET /wordpress HTTP/1.1\" 301 241 \"-\" \"Slackbot 1.0 (+https://api.slack.com/robots)\" 185.24.234.106 - - [26/Jul/2019:20:20:50 -0600] \"GET /wordpress/archives/date/2004/02 HTTP/1.1\" 200 87872 \"http://www.mooreds.com\" \"DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)\" 185.24.234.106 - - [26/Jul/2019:20:20:50 -0600] \"GET /wordpress/archives/date/2004/08 HTTP/1.1\" 200 81183 \"http://www.mooreds.com\" \"DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)\" If I want to see only the ip addresses (assuming these are all in a file called logs.txt), Id run something like: $ awk '{print $1}' logs.txt 54.147.20.92 185.24.234.106 185.24.234.106 Theres lots more, but you can see that youd be able to slice and dice delimited data pretty easily. Heres a great article which dives in further. sed This is another line utility. You can use it for all kinds of things, but I primarily use it to do search and replace on a file. Suppose you had the same log file, but you wanted to anonymize the the ip address and the user agent. Perhaps youre going to ship them off for long term storage or something. You can easily remove this with a couple of sed commands. $ sed 's/^[^ ]*//' logs.txt |sed 's/\"[^\"]*\"$//' - - [26/Jul/2019:20:21:04 -0600] \"GET /wordpress HTTP/1.1\" 301 241 \"-\" - - [26/Jul/2019:20:20:50 -0600] \"GET /wordpress/archives/date/2004/02 HTTP/1.1\" 200 87872 \"http://www.mooreds.com\" - - [26/Jul/2019:20:20:50 -0600] \"GET /wordpress/archives/date/2004/08 HTTP/1.1\" 200 81183 \"http://www.mooreds.com\" Yes, it looks like line noise, but this is the power of regular expressions. Theyre in every language (though with slight variations) and worth learning. sed gives you the power of regular expressions at the command line for processing files. I dont have a great sed tutorial Ive found, but googling shows a number. jq If you work on the command line with modern software at all, you have encountered json. Its used for configuration files and data transmission. Sometimes you get an array of json and you just want to pick out certain attributes of it. Tools like sed and awk fail at this, because they are used to newlines separating records, not curly braces and commas. Sure, you could use regular expressions to parse simple json, and there are times when Ive done this. But a far better tool is jq. Im not as savvy with this as with the others, but have used it whenever Im dealing with an API that delivers json (which is most modern ones). I can pull the API down with curl (another great tool) and parse it out with jq. I can put these all in a script and have the exploration be repeatable. I did this a few months ago when I was doing some exploration of an elastic search system. I crafted the queries with curl and then used jq to parse out the results so that I could make some sense of this. Yes, I could have done this with a real programming language, but it would have taken longer. I could also have used a gui tool like postman, but then it would not have been replicable. sed and awk should be on every system you run across; jq is non standard, but easy to install. Its worth spending some time getting to know these tools. So next time you are processing a text file and need to extract just a bit of it, reach for sed and awk. Next time you get a hairy json file and you are peering at it, look at jq. I think youll be happy with the result. Sincerely, Dan Published July 29, 2019October 17, 2020 ",
        "When I first saw someone using zsh (omz), I was awe-struck.<p>Same thing happens to the person sitting next to me when I pipe an output to jq.",
        "however, I find jq not so friendly piping its output to other programs."
      ],
      "relevant": "true"
    },
    {
      "id": 20811829,
      "title": "Curl exercises",
      "search": [
        "Curl exercises",
        "https://jvns.ca/blog/2019/08/27/curl-exercises/",
        "Recently Ive been interested in how people learn things. I was reading Kathy Sierras great book Badass: Making Users Awesome. It talks about the idea of deliberate practice. The idea is that you find a small micro-skill that can be learned in maybe 3 sessions of 45 minutes, and focus on learning that micro-skill. So, as an exercise, I was trying to think of a computer skill that I thought could be learned in 3 45-minute sessions. I thought that making HTTP requests with curl might be a skill like that, so here are some curl exercises as an experiment! whats curl? curl is a command line tool for making HTTP requests. I like it because its an easy way to test that servers or APIs are doing what I think, but its a little confusing at first! Heres a drawing explaining curls most important command line arguments (which is page 6 of my Bite Size Networking zine). You can click to make it bigger. fluency is valuable With any command line tool, I think having fluency is really helpful. Its really nice to be able to just type in the thing you need. For example recently I was testing out the Gumroad API and I was able to just type in: curl https://api.gumroad.com/v2/sales \\ -d \"access_token=<SECRET>\" \\ -X GET -d \"before=2016-09-03\" and get things working from the command line. 21 curl exercises These exercises are about understanding how to make different kinds of HTTP requests with curl. Theyre a little repetitive on purpose. They exercise basically everything I do with curl. To keep it simple, were going to make a lot of our requests to the same website: https://httpbin.org. httpbin is a service that accepts HTTP requests and then tells you what request you made. Request https://httpbin.org Request https://httpbin.org/anything. httpbin.org/anything will look at the request you made, parse it, and echo back to you what you requested. curls default is to make a GET request. Make a POST request to https://httpbin.org/anything Make a GET request to https://httpbin.org/anything, but this time add some query parameters (set value=panda). Request googles robots.txt file (www.google.com/robots.txt) Make a GET request to https://httpbin.org/anything and set the header User-Agent: elephant. Make a DELETE request to https://httpbin.org/anything Request https://httpbin.org/anything and also get the response headers Make a POST request to https://httpbin.org/anything with the JSON body {\"value\": \"panda\"} Make the same POST request as the previous exercise, but set the Content-Type header to application/json (because POST requests need to have a content type that matches their body). Look at the json field in the response to see the difference from the previous one. Make a GET request to https://httpbin.org/anything and set the header Accept-Encoding: gzip (what happens? why?) Put a bunch of a JSON in a file and then make a POST request to https://httpbin.org/anything with the JSON in that file as the body Make a request to https://httpbin.org/image and set the header Accept: image/png. Save the output to a PNG file and open the file in an image viewer. Try the same thing with different Accept: headers. Make a PUT request to https://httpbin.org/anything Request https://httpbin.org/image/jpeg, save it to a file, and open that file in your image editor. Request https://www.twitter.com. Youll get an empty response. Get curl to show you the response headers too, and try to figure out why the response was empty. Make any request to https://httpbin.org/anything and just set some nonsense headers (like panda: elephant) Request https://httpbin.org/status/404 and https://httpbin.org/status/200. Request them again and get curl to show the response headers. Request https://httpbin.org/anything and set a username and password (with -u username:password) Download the Twitter homepage (https://twitter.com) in Spanish by setting the Accept-Language: es-ES header. Make a request to the Stripe API with curl. (see https://stripe.com/docs/development for how, they give you a test API key). Try making exactly the same request to https://httpbin.org/anything. ",
        "I really like using HTTPie (<a href=\"https://httpie.org\" rel=\"nofollow\">https://httpie.org</a>). Much friendlier syntax then curl, formats the output...<p>Also works great with fx (<a href=\"https://github.com/antonmedv/fx\" rel=\"nofollow\">https://github.com/antonmedv/fx</a>). Just pipe the output from HTTPie and you get easy json processing.",
        "This is more SysAdmin related, but one power-curl function I use atleast 30 times a day is this alias I have in my .bash_aliases<p>This will output the HTTP status code for a given URL.<p><pre><code>     alias hstat=\"curl -o /dev/null --silent --head --write-out '%{http_code}\\n'\" $1  \n</code></pre>\nExample:<p><pre><code>  $ hstat google.com\n  301\n</code></pre>\nI also use curl as an 'uptime monitor' by adding onto that code section.  (a file with a list of URLs, and \"if http status code !=200\" then email me.)<p>There are variations on this all over the place but I really depend on it and I like it."
      ],
      "relevant": "true"
    },
    {
      "id": 21363121,
      "title": "An Illustrated Guide to Useful Command Line Tools",
      "search": [
        "An Illustrated Guide to Useful Command Line Tools",
        "https://www.wezm.net/technical/2019/10/useful-command-line-tools/",
        "Published on Sat, 26 October 2019 Inspired by a similar post by Ben Boyter this a list of useful command line tools that I use. Its not a list of every tool I use. These are tools that are new or typically not part of a standard POSIX command line environment. This post is a living document and will be updated over time. It should be obvious that I have a strong preference for fast tools without a large runtime dependency like Python or node.js. Most of these tools are portable to *BSD, Linux, macOS. Many also work on Windows. For OSes that ship up to date software many are available via the system package repository. Last updated: 31 Oct 2019 About my CLI environment: I use the zsh shell, Pragmata Pro font, and base16 default dark color scheme. My prompt is generated by promptline. Table of Contents Alacritty Terminal emulator alt Find alternate files bat cat with syntax highlighting bb System monitor chars Unicode character search dot Dot files manager dust Disk usage analyser eva Calculator exa Replacement for ls fd Replacement for find hexyl Hex viewer hyperfine Benchmarking tool jq awk/XPath for JSON mdcat Render Markdown in the terminal pass Password manager Podman Docker alternative Restic Encrypted backup tool ripgrep Fast, intelligent grep shotgun Take screenshots skim Fuzzy finder slop Graphical region selection Syncthing Decentralised file synchronisation tig TUI for git titlecase Convert text to title case Universal Ctags Maintained ctags fork watchexec Run commands in response to file system changes z Jump to directories zola Static site compiler Changelog The changelog for this page Alacritty Alacritty is fast terminal emulator. Whilst not strictly a command line tool, it does host everything I do in the command line. It is the terminal emulator in use in all the screenshots on this page. Homepage alt alt is a tool for finding the alternate to a file. E.g. the header for an implementation or the test for an implementation. I use it paired with Neovim to easily toggle between tests and implementation. $ alt app/models/page.rb spec/models/page_spec.rb Homepage bat bat is an alternative to the common (mis)use of cat to print a file to the terminal. It supports syntax highlighting and git integration. Homepage bb bb is system monitor like top. It shows overall CPU and memory usage as well as detailed information per process. Homepage chars chars shows information about Unicode characters matching a search term. Homepage dot dot is a dotfiles manager. It maintains a set of symlinks according to a mappings file. I use it to manage my dotfiles. Homepage dust dust is an alternative du -sh. It calculates the size of a directory tree, printing a summary of the largest items. Homepage exa exa is a replacement for ls with sensible defaults and added features like a tree view, git integration, and optional icons. I have ls aliased to exa in my shell. Homepage eva eva is a command line calculator similar to bc, with syntax highlighting and persistent history. Homepage fd fd is an alternative to find and has a more user friendly command line interface and respects ignore files, like .gitignore. The combination of its speed and ignore file support make it excellent for searching for files in git repositories. Homepage hexyl hexyl is a hex viewer that uses Unicode characters and colour to make the output more readable. Homepage hyperfine hyperfine command line benchmarking tool. It allows you to benchmark commands with warmup and statistical analysis. Homepage jq jq is kind of like awk for JSON. It lets you transform and extract information from JSON documents. Homepage mdcat mdcat renders Markdown files in the terminal. In supported terminals (not Alacritty) links are clickable (without the url being visible like in a web browser) and images are rendered. Homepage pass pass is a password manager that uses GPG to store the passwords. I use it with the passff Firefox extension and Pass for iOS on my phone. Homepage Podman podman is an alternative to Docker that does not require a daemon. Containers are run as the user running Podman so files written into the host dont end up owned by root. The CLI is largely compatible with the docker CLI. Homepage Restic restic is a backup tool that performs client side encryption, de-duplication and supports a variety of local and remote storage backends. Homepage ripgrep ripgrep (rg) recursively searches file trees for content in files matching a regular expression. Its extremely fast, and respects ignore files and binary files by default. Homepage shotgun shotgun is a tool for taking screenshots on X.org based environments. All the screenshots in this post were taken with it. It pairs well with slop. $ shotgun $(slop -c 0,0,0,0.75 -l -f \"-i %i -g %g\") eva.png Homepage skim skim is a fuzzy finder. It can be used to fuzzy match input fed to it. I use it with Neovim and zsh for fuzzy matching file names. Homepage slop slop (Select Operation) presents a UI to select a region of the screen or a window and prints the region to stdout. Works well with shotgun. $ slop -c 0,0,0,0.75 -l -f \"-i %i -g %g\" -i 8389044 -g 1464x1008+291+818 Homepage Syncthing Syncthing is a decentralised file synchronisation tool. Like Dropbox but self hosted and without the need for a central third-party file store. Homepage tig tig is a ncurses TUI for git. Its great for reviewing and staging changes, viewing history and diffs. Homepage titlecase titlecase is a little tool I wrote to format text using a title case format described by John Gruber. It correctly handles punctuation, and words like iPhone. I use it to obtain consistent titles on all my blog posts. $ echo 'an illustrated guide to useful command line tools' | titlecase An Illustrated Guide to Useful Command Line Tools I typically use it from within Neovim where selected text is piped through it in-place. This is done by creating a visual selection and then typing: :!titlecase. Homepage Universal Ctags Universal Ctags is a fork of exuberant ctags that is actively maintained. ctags is used to generate a tags file that vim and other tools can use to navigate to the definition of symbols in files. $ ctags --recurse src Homepage watchexec watchexec is a file and directory watcher that can run commands in response to file-system changes. Handy for auto running tests or restarting a development web server when source files change. # run command on file change $ watchexec -w content cobalt build # kill and restart server on file change $ watchexec -w src -s SIGINT -r 'cargo run' Homepage z z tracks your most used directories and allows you to jump to them with a partial name. Homepage zola zola is a full-featured very fast static site compiler. Homepage Changelog 31 Oct 2019 Add bb, and brief descriptions to the table of contents 28 Oct 2019 Add hyperfine Comments Comments on Lobsters Comments on Hacker News Previous Post: What I Learnt Building a Lobsters TUI in Rust ",
        "I quite like this list, there are a number of utilities here that I already use on a daily basis. There are also a few utilities that I like that weren't on this list, or some alternatives to what was shown. Some off the top of my head:<p>- nnn[0] (C) - A terminal file manager, similar to ranger. It allows you to navigate directories, manipulate files, analyze disk usage, and fuzzy open files.<p>- ncdu[1] (C) - ncurses disk analyzer. Similar to du -sh, but allows for directory navigation as well.<p>- z.lua[2] (Lua) - It's an alternative to the z utility mentioned in the article. I haven't done the benchmarks, but they claim to be faster than z. I'm mostly just including it because it's what I use.<p>[0] <a href=\"https://github.com/jarun/nnn\" rel=\"nofollow\">https://github.com/jarun/nnn</a>\n[1] <a href=\"https://dev.yorhel.nl/ncdu\" rel=\"nofollow\">https://dev.yorhel.nl/ncdu</a>\n[2] <a href=\"https://github.com/skywind3000/z.lua\" rel=\"nofollow\">https://github.com/skywind3000/z.lua</a>",
        "An observation: I have always taken the meaning of the word \"illustrated\" to specifically refer to non-lexical graphics. Searching the dictionary definition of the word, I find a looser definition that applies to \"examples\" intended to aid an explanation.<p>This is a similar cognitive dissonance to when I first learned that \"Visual Basic\" and \"Visual Studio\" meant that the syntax of the displayed code was highlighted, not graphically represented in a non-lexical way."
      ],
      "relevant": "true"
    },
    {
      "id": 21572308,
      "title": "Run an Internet Speed Test from the Command Line",
      "search": [
        "Run an Internet Speed Test from the Command Line",
        "https://www.putorius.net/speed-test-command-line.html",
        "We have all used tools like speedtest.net to test upload and download speeds. Whether it was to test the WiFi in that coffee shop (I use my own tether, never unknown hot spots), preparing for a LAN party (do people still do that?), or just a step in troubleshooting, we have all been there. For one reason or another you simply think you are being cheated of bandwidth, so you want independent verification of your speeds. This typically means opening a browser and going to a website to test your connection. But what if you want to run a speed test on a remote server? In this article we will discuss running an internet speed test from the Linux command line, and skipping the browser. There is something about the raw efficiency of the command line that I am really attracted to. As I discussed in the article 5 Command Line Tool to Break Your Dependence on the GUI, I try my best to stay away from the browser. It usually creates an unnecessary distraction. The internet is designed to grab your attention like a laser pointer does to a cat. So lets get started, and figure out one more way to stay away from the GUI. Different Speed Test Packages There are a few different tools you can use to run a speed test from the command line. To make things even more confusing the two most popular share the same exact name, but both use the speedtest.net service. Unofficial Speedtest-CLI Python Script The first one is an independently written Python script that is simple to install and use. It is available in the default repositories for some popular Linux distributions. Pros: Easy to installWide AvailabilityFull list of serversCan specify upload test, download test, or both Cons: Minimal output format optionsNo verbose output option Jump to Installing speedtest-cli Python Script or How to Use speedtest-cli Python Script. Official Ookla Speedtest CLI The second tool is built by Ookla, the people who bring you the speedtest.net website and service. Installing it requires you to add a repo for your package manager. But the maintainers offer simple instructions for installation. Pros: Official release from OoklaMore robust formatting optionsOutput easier to read, better layoutVerbose output availableHas repo making it easy to get updates Cons: Use limited to nearby serversCannot specify download or upload only Jump to Official Ookla Speedtest CLI The Speedtest-cli Python Script This is an easy way to get started running a speed test on the Linux command line. Installing the Speedtest-cli Python Script Simply use your package manager to install the package. Install on Fedora using DNF sudo dnf install speedtest-cli Ubuntu or Debian using APT sudo apt-get install speedtest-cli CentOS/Red Hat 7 / 8 Unfortunately, CentOS does not offer the rpm in their repos. It can still be easily installed. Change to /usr/bin directory to make command available to all users: cd /usr/bin Install dependencies: sudo yum install -y python wget Fetch script from github: sudo wget https://raw.githubusercontent.com/sivel/speedtest-cli/master/speedtest.py -O speedtest-cli Make script executable: sudo chmod +x speedtest-cli Or just copy and paste the whole thing below as a single line: cd /usr/bin; sudo yum install -y python wget && wget https://raw.githubusercontent.com/sivel/speedtest-cli/master/speedtest.py -O speedtest-cli && sudo chmod +x speedtest-cli How to Use the Python Script to Run a Speed Test The most basic usage is to simply run the command. It will automatically select the best server based on ping responses. Speedtest-cli Python Script Options There are several options available to change the default behavior. Here we will outline the most popular options. List Available Speed Test Servers You can use the list option to find a list of available servers to run your test against. At the time of writing this list is pretty extensive with 8829 possible servers. NOTE: The servers are sorted by distance, closest first. [[emailprotected] ~]$ speedtest-cli --list Retrieving speedtest.net configuration 4847) Hotwire Fision (Philadelphia, PA, United States) [10.92 km] 10979) School District of Philadelphia (Philadelphia, PA, United States) [10.92 km] ...OUTPUT TRUNCATED... Specify Specific Server to Test Against Once you have found the server you want to test against, you can use the server <SERVER ID> to select it. The server ID is the first column in the output of the list option above. [[emailprotected] ~]$ speedtest-cli --server 4847 Retrieving speedtest.net configuration Testing from Verizon Fios (x.x.x.x) Retrieving speedtest.net server list Selecting best server based on ping Hosted by School District of Philadelphia (Philadelphia, PA) [10.92 km]: 25.033 ms Testing download speed.. Download: 384.07 Mbit/s Testing upload speed Upload: 417.93 Mbit/s Only Test Upload or Download Speeds The option is actually designed to exclude a test. But since there are only two options it is effectively the same as selecting only one. To run only the download test, you exclude the upload, and vice versa. [[emailprotected] ~]$ speedtest-cli --no-upload Retrieving speedtest.net configuration Testing from Verizon Fios (x.x.x.x) Retrieving speedtest.net server list Selecting best server based on ping Hosted by KamaTera INC (New Jersey, NJ) [62.36 km]: 19.785 ms Testing download speed.. Download: 600.89 Mbit/s Skipping upload test Format Output in JSON or CSV You can specify the output format in JSON or CSV. You also have the opton to use CSV with a custom delimiter. This is handy if you are going to use the output in some other script or application. [[emailprotected] ~]$ speedtest-cli --json {\"download\": 597726146.0529929, \"upload\": 562476134.8046777, \"ping\": 17.004, \"server\": {\"url\": \"http://speedtest.us-ny2.kamatera.com:8080/speedtest/upload.php\", \"lat\": \"40.0583\", \"lon\": \"-74.4057\", \"name\": \"New Jersey, NJ\", \"country\": \"United States\", \"cc\": \"US\", \"sponsor\": \"KamaTera INC\", \"id\": \"11612\" ...OUTPUT TRUNCATED... Using CSV with a custom delimiter. The default delimiter is a comma, which is implied by the name CSV. Here we use the csv-delimiter option to change the delimiter to a pipe character. [[emailprotected] ~]$ speedtest-cli --csv --csv-delimiter \"|\" 11612|KamaTera INC|New Jersey, NJ|2019-11-17T14:51:53.636981Z|62.35865439150934|8.546|588013638.8767571|512001168.48230773||x.x.x.x The Official Ookla Speedtest CLI The official Speedtest CLI (Command Line Interface) from Ookla is a little more robust. It has all of the options of the python script and more. There are also several output formats not available with the unofficial python script. Ooklas speedtest is also a little easier on the eyes. It spreads the information out which makes it easier to read and displays a neat little progress bar. A URL you can use to share the results is also displayed by default. Installing the Official Speedtest CLI Install Speedtest CLI on Ubuntu / Debian: The Speedtest CLI from Ookla is supported on Ubuntu (xenial & bionic) and Debian (jessie, stretch, buster). $ sudo apt-get install gnupg1 apt-transport-https dirmngr $ export INSTALL_KEY=379CE192D401AB61 $ export DEB_DISTRO=$(lsb_release -sc) $ sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys $INSTALL_KEY $ echo \"deb https://ookla.bintray.com/debian ${DEB_DISTRO} main\" | sudo tee /etc/apt/sources.list.d/speedtest.list $ sudo apt-get update $ sudo apt-get install speedtest Install Speedtest CLI on Fedora / Redhat / CentOS: Fedora has moved on to DNF for package management, but is still compatible with YUM. These instructions were tested on Fedora 31, CentOS 7 and Red Hat 8. $ sudo yum install wget $ wget https://bintray.com/ookla/rhel/rpm -O bintray-ookla-rhel.repo $ sudo mv bintray-ookla-rhel.repo /etc/yum.repos.d/ $ sudo yum install speedtest How to Use the Official Speedtest CLI Once installed you can simply call the utility by typing speedtest at the command line. This will give you all the default information that you would see on the web version of speedtest.net. Official Speedtest CLI Options The options available in the official release are more robust. Here we will outline the popular options and how to use them. List Available Speed Test Servers Using the -L (servers) option will give you a list of servers available to run a test against. This option will only show you servers that are nearby. What exactly determines nearby is undefined. But for me it looks like they are staying in the tri-state area (PA, NJ, DE). [[emailprotected] ~]$ speedtest -L Closest servers: ID Name Location Country 4847 Hotwire Fision Philadelphia, PA United States 10979 School District of Philadelphia Philadelphia, PA United States 9840 Comcast New Castle, DE United States 11612 KamaTera INC New Jersey, NJ United States ...OUTPUT TRUNCATED... Optionally, you can use the -o (host) option and specify the FQDN of the server instead of the ID. But oddly, I dont see a way to get the FQDN of the servers on the list. I am guessing this option is available for using a custom server. I havent found a way to list all servers. If you are looking to test against a server on the other side of the country, you will have to find it another way. Select Specific Server to Run Speed Test Against You can use the -s (server-id) option to select a server to use from the list. You must supply the server ID with this option. The server ID is the number in the first column of the list output above. [[emailprotected] ~]$ speedtest -s 4847 Speedtest by Ookla Server: Hotwire Fision - Philadelphia, PA (id = 4847) Change Unit Used for Speed Output The -u (unit) option can display the speed output in many different formats. Decimal prefix, bits per second: bps, kbps, Mbps, Gbps Decimal prefix, bytes per second: B/s, kB/s, MB/s, GB/s Binary prefix, bits per second: kibps, Mibps, Gibps Binary prefix, bytes per second: kiB/s, MiB/s, GiB/s [[emailprotected] ~]$ speedtest -u MiB/s Speedtest by Ookla Server: Hotwire Fision - Philadelphia, PA (id = 4847) ISP: Verizon Fios Latency: 10.67 ms ( 0.95 ms jitter) Download: 63.45 MiB/s (data used: 700.2 MiB) ...OUTPUT TRUNCATED... Output Formatting Options The Ookla Speedtest CLI offers decent options for output formats. Human Readable DefaultCSV Comma Separated ValueTSV Tab Separated ValueJSON JavaScript Object NotationJSONL JSON LinesJSON-PRETTY JSON Pretty Printed Here is an example using json-pretty. [[emailprotected] ~]$ speedtest -f json-pretty { \"type\": \"result\", \"timestamp\": \"2019-11-17T16:42:06Z\", \"ping\": { \"jitter\": 0.29899999999999999, \"latency\": 17.474 }, \"download\": { \"bandwidth\": 92184614, \"bytes\": 491967724, \"elapsed\": 5303 }, \"upload\": { \"bandwidth\": 45010100, \"bytes\": 313859035, \"elapsed\": 6714 }, ...OUTPUT TRUNCATED... Conclusion Running a speed test from the command line may not be something that is needed on a daily basis for most people. However, it may prove useful in some troubleshooting situations. In this article we cover how to run a speed test from the command line using two similar tools. The unofficial python script and the official Ookla Speedtest CLI. We discussed installing, using and setting options for each one. This should be enough to get you started. For more information on these tools, visit their respective home pages found in the resources section below. Resources and Links: Ookla Speedtest CLISpeedtest-cli (Python Version) on GitHub ",
        "Note that some ISPs prioritise traffic to known speedtest targets, turning off traffic shaping rules that might otherwise slow bulk transfers. When this happens it means you are testing the likely maximum throughput of your connection not necessarily the throughput you will see more generally.<p>This is why Netflix started fast.com - because it draws data from the same distribution points as their video streaming apps it means you can't prioritise the speedtest without also doing so for the video traffic or (more likely) you can't de-prioritise the video traffic without also getting bad scores in that particular speedtest. From Netflix's point of view it is an answer to people contacting support with \"my speedtest results are fine, the problem must be your servers\" when they are experiencing video lag/drops and other such problems and the issue is due to ISP traffic shaping or the ISP simply not having enough backhaul bandwidth.<p>A more reliable test might be taking part in a busy public torrent: that way you are testing against arbitrary locations so your ISP can't be setting different shaping rules for them. Just remember to throttle upstream when testing downstream and vice-versa or saturation in the other direction will slow control packets that will in turn give you lower results for the one you are testing. This may fall into another trap though: unless you limit the number of active streams it may be an unrealistic test as more generally most processes use a small number of streams (or just a single one), and if you limit the number of streams too much you might get a lower result because each swarm member you connect to may be fairly saturated and sharing its bandwidth amongst many connections.",
        "speedtest-cli is <i>garbage</i> if you have >100Mbps Speeds. The dev refuses to acknowledge this: <a href=\"https://github.com/sivel/speedtest-cli/issues/226\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/226</a><p>Also not just me:\n<a href=\"https://github.com/sivel/speedtest-cli/issues/649\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/649</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/648\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/648</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/641\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/641</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/616\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/616</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/601\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/601</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/588\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/588</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/546\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/546</a>"
      ],
      "relevant": "true"
    },
    {
      "id": 21858952,
      "title": "JSON on the Command Line with Jq",
      "search": [
        "JSON on the Command Line with Jq",
        "https://shapeshed.com/jq-json/",
        "HomePostsContactLast updated Saturday, Nov 16, 2019A series of how to examples on using jq, a command-line JSON processorEstimated reading time: 4 minutesTable of contentsHow to pretty print JSONHow to use pipes with jqHow to find a key and valueHow to find items in an arrayHow to combine filtersHow to transform JSONHow to transform data values within JSONHow to remove keys from JSONHow to map valuesHow to wrangle JSON how you wantFurther readingjq is a fantastic command-line JSON processor. It plays nice with UNIX pipes and offers extensive functionality for interrogating, manipulating and working with JSON file.How to pretty print JSONjq can do a lot but probably the highest frequency use for most users is to pretty print JSON either from a file or after a network call. Suppose that we have a file names.json containing the following json.[{\"id\": 1, \"name\": \"Arthur\", \"age\": \"21\"},{\"id\": 2, \"name\": \"Richard\", \"age\": \"32\"}] jq can pretty print this file using the . filter. This takes the entire input and sends it to standard output. Unless told not to jq will pretty print making JSON readable.jq '.' names.json [ { \"id\": 1, \"name\": \"Arthur\", \"age\": \"21\" }, { \"id\": 2, \"name\": \"Richard\", \"age\": \"32\" } ] How to use pipes with jqBecause jq is UNIX friendly it is possible to pipe data in and out of it. This can be useful for using jq as a filter or interacting with other tools. In the following pipeline cat pipes the file into jq and this is piped onto less. This can be very useful for viewing large JSON files.cat names.json | jq '.' | less How to find a key and valueTo find a key and value jq can filter based on keys and return the value. Suppose we have the following simple JSON document saved as dog.json.{ \"name\": \"Buster\", \"breed\": \"Golden Retriever\", \"age\": \"4\", \"owner\": { \"name\": \"Sally\" }, \"likes\": [ \"bones\", \"balls\", \"dog biscuits\" ] } jq can retrieve values from this document by passing key names.jq '.name' \"Buster\" Multiple keys can by passed separated by commas.jq '.breed,.age' \"Golden Retriever\" \"4\" To search for nested objects chain values using the dot operator just as you would in JavaScript.jq '.owner.name' \"Sally\" How to find items in an arrayTo search for items in arrays use bracket syntax with the index starting at 0.jq '.likes[0]' \"bones\" Multiple elements of an array may also be returned.echo '[\"a\",\"b\",\"c\",\"d\",\"e\"]' | jq '.[2:4]' [ \"c\", \"d\" ] How to combine filtersjq can combine filters to search within a selection. For the following JSON document suppose that the names need to be filtered.[ { \"id\": 1, \"name\": \"Arthur\", \"age\": \"21\" }, { \"id\": 2, \"name\": \"Richard\", \"age\": \"32\" } ] This can be achieved with a pipe with the jq filter.jq '.[] | .name' names.json \"Arthur\" \"Richard\" How to transform JSONjq can be used for more than just reading values from a JSON object. It can also transform JSON into new data structures. Returning to the dog.json example earlier a new array can be created containing the name and likes as follows.jq '[.name, .likes[]]' dog.json [ \"Buster\", \"Bones\", \"balls\", \"dog biscuits\" ] This can be very useful in data transform pipelines to shift JSON data from one structure to another.How to transform data values within JSONjq can also operate on data within JSON objects. Suppose the following JSON file exists and is saved as inventory.json.{ \"eggs\": 2, \"cheese\": 1, \"milk\": 1 } jq can be used to perform basic arithmetic on number values.jq '.eggs + 1' inventory.json 3 How to remove keys from JSONjq can remove keys from JSON objects. This outputs the JSON object without the deleted key. Suppose the following JSON is saved as cheeses.json.{ \"maroilles\": \"stinky\", \"goat\": \"mild\" } jq can remove keys as follows leaving just wonderful stinky cheese.jq 'del(.goat)' cheeses.json { \"maroilles\": \"stinky\" } How to map valuesjq can map values and perform an operation on each one. In the following example each item in an array is mapped and has two subtracted.echo '[12,14,15]' | jq 'map(.-2)' [ 10, 12, 13 ] How to wrangle JSON how you wantjq has many more advanced features to help manipulating and wrangling JSON however you want to. For more run man jq.Further readingjq project pagejq manualjq is sed for JSONbash that JSON (jq)Parsing JSON with jqHave an update or suggestion for this article? You can edit it here and send me a pull request.TagsRecent Posts ",
        "Thread from 2016: <a href=\"https://news.ycombinator.com/item?id=13090604\" rel=\"nofollow\">https://news.ycombinator.com/item?id=13090604</a><p>2015: <a href=\"https://news.ycombinator.com/item?id=9446980\" rel=\"nofollow\">https://news.ycombinator.com/item?id=9446980</a><p>2014: <a href=\"https://news.ycombinator.com/item?id=7895076\" rel=\"nofollow\">https://news.ycombinator.com/item?id=7895076</a><p>2013: <a href=\"https://news.ycombinator.com/item?id=5734683\" rel=\"nofollow\">https://news.ycombinator.com/item?id=5734683</a><p>2012: <a href=\"https://news.ycombinator.com/item?id=4985250\" rel=\"nofollow\">https://news.ycombinator.com/item?id=4985250</a><p><a href=\"https://news.ycombinator.com/item?id=4679933\" rel=\"nofollow\">https://news.ycombinator.com/item?id=4679933</a><p>(for the curious)",
        "Every time I try to do something involved with jq (i.e. more than a couple commands deep) and inevitably end up wandering through the manual, I get a persistent feeling that jq could be cleaned up quite a bit. That it either has structure throughout that I fail to grasp, or that it's really rather disorganized and convoluted in places. This is exacerbated by me never being able to find anything in the manual without skimming half of it each single time.<p>E.g.:<p>Filtering and transforming objects by reaching a few levels deeper in them seems to be way easier to slap together in Python's list comprehensions or some functional notation. Stuff like [{ * * x, qwe: x.qwe*2} for x in a.b if x.y.z == 1].<p>Add nested lists to the above, and I can spend another fifteen minutes writing a one-liner (i.e. `a` is a list of objects with lists in some field).<p>I once had to deal with a conditional structure where a field might be one object or a list of them. Hoo boy.<p>Every time I want to check the number of entries my filters print out, I need to wrap them in an array. Repeat this a couple dozen times during writing a complex query. Weirdly, this is where strict structure gets in the way―and while I understand the reasoning, I feel that something could be done for such basic need. (Like, aggregate filters should be able to aggregate instead of acting on individual items? Maybe it's already in the language, but I'm not in the mood to go over the manual again.)<p>Variables seem to be bolted on as an afterthought, or at least the syntax doesn't exactly accommodate them. Meanwhile, they're necessary to implement some filters omitted in the language. Compare that to Lisp's simple `(let)`. IIRC the manual also says that jq has some semblance of functions, but I'm afraid to think about them with this syntax.<p>I like the idea a lot, but execution not so much. Frankly I'll probably end up throwing together a script in Lumo or something, that will accept Lisp expressions and feed JSON structure to them. (I'd use Fennel, but JSON has actual null while Lua... doesn't.)<p>Btw, I have pretty much the same sentiment about Git. Git structures, great. Git tools, oy vey. Maybe I need Lisp for Git, or at least Python for Git."
      ],
      "relevant": "true"
    },
    {
      "id": 19954195,
      "title": "Show HN: UXY – adding structure to Unix tools",
      "search": [
        "Show HN: UXY – adding structure to Unix tools",
        "https://github.com/sustrik/uxy",
        "Treating everything as a string is the way through which the great power and versatility of UNIX tools is achieved. However, sometimes the constant parsing of strings gets a bit cumbersome. UXY is a tool to manipulate UXY format, which is basically a two-dimensional table that's both human- and machine-readable. The format is deliberately designed to be as similar to the output of standard tools, such as ls or ps, as possible. UXY tool also wraps some common UNIX tools and exports their output in UXY format. Along with converters from/to other common data formats (e.g. JSON) it is meant to allow for quick and painless access to the data. Examples $ uxy ls TYPE PERMISSIONS LINKS OWNER GROUP SIZE TIME NAME - rw-r--r-- 1 martin martin 3204 2019-05-25T15:44:46.371308721+02:00 README.md - rwxr-xr-x 1 martin martin 25535 2019-05-25T16:29:28.518397541+02:00 uxy $ uxy ls | uxy fmt \"NAME SIZE\" NAME SIZE README.md 7451 uxy 11518 $ uxy ls | uxy fmt \"NAME SIZE\" | uxy align NAME SIZE README.md 7451 uxy 11518 $ uxy top | uxy fmt \"PID CPU COMMAND\" | uxy to-json [ { \"PID\": \"4704\", \"CPU\": \"12.5\", \"COMMAND\": \"top\" }, { \"PID\": \"2903\", \"CPU\": \"6.2\", \"COMMAND\": \"Web Content\" }, { \"PID\": \"1\", \"CPU\": \"0.0\", \"COMMAND\": \"systemd\" } ] $ uxy ls | uxy grep test NAME TYPE PERMISSIONS LINKS OWNER GROUP SIZE TIME NAME - rw-r--r-- 1 martin martin 45 2019-05-25T16:09:58.755551983+02:00 test.csv - rw-r--r-- 1 martin martin 84 2019-05-25T16:09:58.755552856+02:00 test.txt - rw-r--r-- 1 martin martin 75 2019-05-25T16:09:58.755559998+02:00 test.uxy $ uxy ps | uxy to-json | jq '.[].CMD' \"bash\" \"uxy\" \"uxy\" \"jq\" \"ps\" $ cat test.csv NAME,TIME Quasimodo,14:30 Moby Dick,14:22 $ cat test.csv | uxy from-csv | uxy align NAME TIME Quasimodo 14:30 \"Moby Dick\" 14:22 TOOLS UXY tools All UXY tools take input from stdin and write the result to stdout. The tools follow the Postel's principle: \"Be liberal in what you accept, conservative in what you output.\" They accept any UXY input, but they try to align the fields in the output to make it more convenient to read. uxy align uxy from-csv uxy from-json uxy from-yaml uxy grep uxy import uxy fmt uxy to-csv uxy to-json uxy to-yaml uxy trim Wrapped UNIX tools Any argument that could be passed to the original tool can also be passed to the UXY-wrapped version of the tool. The exception are the arguments that modify how the output looks like. UXY manages those arguments itself. The only control you have over the output is to either print the default (short) set of result fields (mostly defined as \"the most useful info that fits on page\") or long set of result fields (\"all the information UXY was able to extract\"): $ uxy -l ps When running with -l option it often happens that the output exceeds the terminal width, gets wrapped and unreadable. In such cases you can either filter out just the fields you are intersed in using fmt subcommand or convert the result to YAML (uxy to-yaml) which happens to render each field on a separate line: $ uxy -l ifconfig | uxy fmt \"NAME INET-ADDR\" NAME INET-ADDR enp0s31f6 \"\" lo 127.0.0.1 wlp3s0 192.168.1.7 $ uxy -l ifconfig | uxy to-yaml - ETHER-ADDR: e4:42:a6:f4:1d:02 FLAGS: UP,BROADCAST,RUNNING,MULTICAST INET-ADDR: 192.168.1.7 INET-NETMASK: 255.255.255.0 INET6-ADDR: fe80::fd53:a17f:12ce:38a8 INET6-PREFIXLEN: '64' INET6-SCOPEID: 0x20 ... uxy du uxy ifconfig uxy ls uxy lsof uxy netstat uxy ps uxy top uxy w TESTING To test, run ./test script. ",
        "This is becoming a really crowded space. Some other similar tools that make slightly different design choices and that have variable envisioned use cases:<p>- <a href=\"https://github.com/dkogan/vnlog\" rel=\"nofollow\">https://github.com/dkogan/vnlog</a><p>- <a href=\"https://csvkit.readthedocs.io/\" rel=\"nofollow\">https://csvkit.readthedocs.io/</a><p>- <a href=\"https://github.com/johnkerl/miller\" rel=\"nofollow\">https://github.com/johnkerl/miller</a><p>- <a href=\"https://github.com/BurntSushi/xsv\" rel=\"nofollow\">https://github.com/BurntSushi/xsv</a><p>- <a href=\"https://github.com/eBay/tsv-utils-dlang\" rel=\"nofollow\">https://github.com/eBay/tsv-utils-dlang</a><p>- <a href=\"https://stedolan.github.io/jq/\" rel=\"nofollow\">https://stedolan.github.io/jq/</a><p>- <a href=\"http://harelba.github.io/q/\" rel=\"nofollow\">http://harelba.github.io/q/</a><p>- <a href=\"https://github.com/BatchLabs/charlatan\" rel=\"nofollow\">https://github.com/BatchLabs/charlatan</a><p>- <a href=\"https://github.com/dinedal/textql\" rel=\"nofollow\">https://github.com/dinedal/textql</a><p>- <a href=\"https://github.com/dbohdan/sqawk\" rel=\"nofollow\">https://github.com/dbohdan/sqawk</a><p>(disclaimer: vnlog is my tool)",
        "A related problem is the constant churn of logging.. taking structured data, destructuring it with a string serialization and then parsing it again.<p>This resource-wasting antipattern pops up over and over again.<p>Also, logs are message-oriented entries and serializing them as discrete, lengthy files is insane.<p>Structured data should stay structured, say a time-series / log-structured database. Destructuring should be a rare event."
      ],
      "relevant": "true"
    },
    {
      "id": 21196177,
      "title": "I wrote a command-line Ruby program to manage EC2 instances for me",
      "search": [
        "I wrote a command-line Ruby program to manage EC2 instances for me",
        "https://www.codewithjason.com/wrote-command-line-ruby-program-manage-ec2-instances/",
        "Why I did this Heroku is great, but not in 100% of cases When I want to quickly deploy a Rails application, my go-to choice is Heroku. Im a big fan of the idea that I can just run heroku create and have a production application online in just a matter of seconds. Unfortunately, Heroku isnt always a desirable option. If Im just messing around, I dont usually want to pay for Heroku features, but I also dont always want my dynos to fall asleep after 30 minutes like on the free tier. (Im aware that there are ways around this but I dont necessarily want to deal with the hassle of all that.) Also, sometimes I want finer control than what Heroku provides. I want to be closer to the metal with the ability to directly manage my EC2 instances, RDS instances, and other AWS services. Sometimes I desire this for cost reasons. Sometimes I just want to learn what I think is the valuable developer skill of knowing how to manage AWS infrastructure. Unfortunately, using AWS by itself isnt very easy. Setting up Rails on bare EC2 is a time-consuming and brain-consuming hassle Getting a Rails app standing up on AWS is pretty hard and time-consuming. Im actually not even going to get into Rails-related stuff in this post because even the small task of getting an EC2 instance up and runningwithout no additional software installed on that instanceis a lot harder than I think it should be, and theres a lot to discuss and improve just inside that step. Just to briefly illustrate what a pain in the ass it is to get an EC2 instance launched and to SSH into it, here are the steps. The steps that follow are the command-line steps. I find the AWS GUI console steps roughly equally painful. 1. Use the AWS CLI create-key-pair command to create a key pair. This step is necessary for later when I want to SSH into my instance. 2. Think of a name for the key pair and save it somewhere. Thinking of a name might seem like a trivially small hurdle, but every tiny bit of mental friction adds up. I dont want to have to think of a name, and I dont want to have to think about where to put the file (even if that means just remembering that I want to put the key in ~/.ssh, which is the most likely case. 3. Use the run-instances command, using an AMI ID (AMI == Amazon Machine Image) and passing in my key name. Now I have to go look up the run-instances (because I sure as hell dont remember it) and, look up my AMI ID, and remember what my key name is. (If you dont know what an AMI ID is, thats what determines whether the instance will be Ubuntu, Amazon Linux, Windows, etc.) 4. Use the describe-instances command to find out the public DNS name of the instance I just launched. This means I either have to search the JSON response of describe-instances for the PublicDnsName entry or apply a filter. Just like with every AWS CLI command, Id have to go look up the exact syntax for this. 5. Run the ssh command, passing in my instances DNS and the path to my key. This step is probably the easiest, although it took me a long time to commit the exact ssh -i syntax to memory. For the record, the command is ssh -i ~/.ssh/my_key.pem ubuntu@mypublicdns.com. Its a small pain in the ass to have to look up the public DNS for my instance again and remember whether my EC2 user is going to be ubuntu or ec2-user (it depends on what AMI I used). My goals for my AWS command-line tool All this fuckery was a big hassle so I decided to write my own command-line tool to manage EC2 instances. I call the tool Exosuit. You can actually try it out yourself by following these instructions. There were four specific capabilities I wanted Exosuit to have. Launch an instance By running bin/exo launch, it should launch an EC2 instance for me. It should assume I want Ubuntu. It should let me know when the instance is ready, and what its instance ID and public DNS are. SSH into an instance I should be able to run bin/exo ssh, get prompted for which instance I want to SSH into, and then get SSHd into that instance. List all running instances I should be able to run bin/exo instances to see all my running instances. It should show the instance ID and public DNS for each. Terminate instances I should be able to run bin/exo terminate which will show me all my instance IDs and allow me to select one or more of them for termination. How I did it Side note: when I first wrote this, I forgot that the AWS SDK for Ruby existed, so I reinvented some wheels. Whoops. After I wrote this I refactored the project to use AWS SDK instead of shell out to AWS CLI. For brevity Ill focus on the bin/exo launch command. Using the AW CLI run-instances command The AWS CLI command for launching an instance looks like this: aws ec2 run-instances \\ --count 1 \\ --image-id ami-05c1fa8df71875112 \\ --instance-type t2.micro \\ --key-name normal-quiet-carrot \\ --profile personal Hopefully most of these flags are self-explanatory. You might wonder where the key name of normal-quiet-carrot came from. When the bin/exo launch command is run, Exosuit asks Is there a file defined at .exosuit/config.yml that contains a key pair name and path? If not, create that file, create a new key pair with a random phrase for a name, and save the name and path to that file. Heres what my .exosuit/config.yml looks like: --- aws_profile_name: personal key_pair: name: normal-quiet-carrot path: \"~/.ssh/normal-quiet-carrot.pem\" The aws_profile_name is something that I imagine most users arent likely to need. I personally happen to have multiple AWS accounts, so its necessary for me to send a --profile flag when using AWS CLI commands so AWS knows which account of mine to use. If a profile isnt specified in .exosuit/config.yml, Exosuit will just leave the --profile flag off and everything will still work fine. Abstracting the run-instances command Once I had coded Exosuit to construct a few different AWS CLI commands (e.g. run-instances, terminate-instances), I noticed that things were getting a little repetitive. Most troubling, I had to always remember to include the --profile flag (just as I would if I were typing all this on the command line manually), and I didnt always remember to do so. In those cases my command would get sent to the wrong account. Thats bad. So I created an abstraction called AWSCommand. Heres what a usage of it looks like: command = AWSCommand.new( :run_instances, count: 1, image_id: IMAGE_ID, instance_type: INSTANCE_TYPE, key_name: key_pair.name ) JSON.parse(command.run) You can probably see the resemblance it bears to the bare run-instances usage. Note the conspicuous absence of the profile flag, which is now automatically included every single time. Listening for launch success One of my least favorite things about manually launching EC2 instances is having to check periodically to see when theyve started running. So I wanted Exosuit to tell me when my EC2 instance was running. I achieved this by writing a loop that hits AWS once per second, checking the state of my new instance each time. module Exosuit def self.launch_instance response = Instance.launch(self.key_pair) instance_id = response['Instances'][0]['InstanceId'] print \"Launching instance #{instance_id}...\" while true sleep(1) print '.' instance = Instance.find(instance_id) if instance && instance.running? puts break end end puts 'Instance is now running' puts \"Public DNS: #{instance.public_dns_name}\" end end You might wonder what Instance.find and instance.running? do. The Instance.find method will run the aws ec2 describe-instances command, parse the JSON response, then grab the relevant JSON data for whatever instance_id I passed to it. The return value is an instance of the Instance class. When an instance of Instance is instantiated, an instance variable gets set (pardon all the instances) with all the JSON data for that instance that was returned by the AWS CLI. The instance.running? method simply looks at that JSON data (which has since been converted to a Ruby hash) and checks to see what the value of ['State']['Name'] is. Heres an abbreviated version of the Instance class for reference. module Exosuit class Instance def initialize(info) @info = info end def state @info['State']['Name'] end def running? state == 'running' end end end (By the way, all the Exosuit code is available on GitHub if youd like to take a look.) Success notification As you can see from the code a couple snippets above, Exosuit lets me know once my instances has entered a running state. At this point I can run bin/exo ssh, bin/exo instances or bin/exo terminate to mess with my instance(s) as I please. Demo video Heres a small sample of Exosuit in action: Try it out yourself If youd like to try out Exosuit, just visit the Getting Started with Exosuit guide. If you think this idea is cool and useful, please let me know by opening a GitHub issue for a feature youd like to see, or tweeting at me, or simply starring the project on GitHub so I can gage interest. I hope you enjoyed this explanation and I look forward to sharing the next steps I take with this project. ",
        "Dude, thank you so much. This looks like exactly what I need for a project I'm on."
      ],
      "relevant": "false"
    },
    {
      "id": 20062064,
      "title": "Semantic: Parsing, analyzing, and comparing source code across many languages",
      "search": [
        "Semantic: Parsing, analyzing, and comparing source code across many languages",
        "https://github.com/github/semantic",
        "semantic is a Haskell library and command line tool for parsing, analyzing, and comparing source code. In a hurry? Check out our documentation of example uses for the semantic command line tool. Table of Contents Usage Language support Development Technology and architecture Licensing Usage Run semantic --help for complete list of up-to-date options. Parse Usage: semantic parse [--sexpression | (--json-symbols|--symbols) | --proto-symbols | --show | --quiet] [FILES...] Generate parse trees for path(s) Available options: --sexpression Output s-expression parse trees (default) --json-symbols,--symbols Output JSON symbol list --proto-symbols Output protobufs symbol list --show Output using the Show instance (debug only, format subject to change without notice) --quiet Don't produce output, but show timing stats -h,--help Show this help text Language support Language Parse AST Symbols Stack graphs Ruby JavaScript TypeScript Python Go PHP Java JSON JSX TSX CodeQL Haskell Used for code navigation on github.com. Supported Partial support Under development - N/A Development semantic requires at least GHC 8.10.1 and Cabal 3.0. We strongly recommend using ghcup to sandbox GHC versions, as GHC packages installed through your OS's package manager may not install statically-linked versions of the GHC boot libraries. semantic currently builds only on Unix systems; users of other operating systems may wish to use the Docker images. We use cabal's Nix-style local builds for development. To get started quickly: git clone git@github.com:github/semantic.git cd semantic script/bootstrap cabal v2-build all cabal v2-run semantic:test cabal v2-run semantic:semantic -- --help You can also use the Bazel build system for development. To learn more about Bazel and why it might give you a better development experience, check the build documentation. git clone git@github.com:github/semantic.git cd semantic script/bootstrap-bazel bazel build //... stack as a build tool is not officially supported; there is unofficial stack.yaml support available, though we cannot make guarantees as to its stability. Technology and architecture Architecturally, semantic: Generates per-language Haskell syntax types based on tree-sitter grammar definitions. Reads blobs from a filesystem or provided via a protocol buffer request. Returns blobs or performs analysis. Renders output in one of many supported formats. Throughout its lifestyle, semantic has leveraged a number of interesting algorithms and techniques, including: Myers' algorithm (SES) as described in the paper An O(ND) Difference Algorithm and Its Variations RWS as described in the paper RWS-Diff: Flexible and Efficient Change Detection in Hierarchical Data. Open unions and data types la carte. An implementation of Abstracting Definitional Interpreters extended to work with an la carte representation of syntax terms. Contributions Contributions are welcome! Please see our contribution guidelines and our code of conduct for details on how to participate in our community. Licensing Semantic is licensed under the MIT license. ",
        "Looks very interesting - would benefit from showing some examples and/or use cases",
        "The late 20th century, early 00's version:<p><a href=\"http://www.program-transformation.org/Transform/CodeCrawler\" rel=\"nofollow\">http://www.program-transformation.org/Transform/CodeCrawler</a><p>(And MOOSE)"
      ],
      "relevant": "true"
    },
    {
      "id": 19427332,
      "title": "Show HN: Drogon – A C++14/17 based high performance HTTP application framework",
      "search": [
        "Show HN: Drogon – A C++14/17 based high performance HTTP application framework",
        "https://github.com/an-tao/drogon",
        "English | | Overview Drogon is a C++14/17-based HTTP application framework. Drogon can be used to easily build various types of web application server programs using C++. Drogon is the name of a dragon in the American TV series \"Game of Thrones\" that I really like. Drogon is a cross-platform framework, It supports Linux, macOS, FreeBSD, OpenBSD, HaikuOS, and Windows. Its main features are as follows: Use a non-blocking I/O network lib based on epoll (kqueue under macOS/FreeBSD) to provide high-concurrency, high-performance network IO, please visit the TFB Tests Results for more details; Provide a completely asynchronous programming mode; Support Http1.0/1.1 (server side and client side); Based on template, a simple reflection mechanism is implemented to completely decouple the main program framework, controllers and views. Support cookies and built-in sessions; Support back-end rendering, the controller generates the data to the view to generate the Html page. Views are described by CSP template files, C++ codes are embedded into Html pages through CSP tags. And the drogon command-line tool automatically generates the C++ code files for compilation; Support view page dynamic loading (dynamic compilation and loading at runtime); Provide a convenient and flexible routing solution from the path to the controller handler; Support filter chains to facilitate the execution of unified logic (such as login verification, Http Method constraint verification, etc.) before handling HTTP requests; Support https (based on OpenSSL); Support WebSocket (server side and client side); Support JSON format request and response, very friendly to the Restful API application development; Support file download and upload; Support gzip, brotli compression transmission; Support pipelining; Provide a lightweight command line tool, drogon_ctl, to simplify the creation of various classes in Drogon and the generation of view code; Support non-blocking I/O based asynchronously reading and writing database (PostgreSQL and MySQL(MariaDB) database); Support asynchronously reading and writing sqlite3 database based on thread pool; Support Redis with asynchronous reading and writing; Support ARM Architecture; Provide a convenient lightweight ORM implementation that supports for regular object-to-database bidirectional mapping; Support plugins which can be installed by the configuration file at load time; Support AOP with build-in joinpoints. Support C++ coroutines A very simple example Unlike most C++ frameworks, the main program of the drogon application can be kept clean and simple. Drogon uses a few tricks to decouple controllers from the main program. The routing settings of controllers can be done through macros or configuration file. Below is the main program of a typical drogon application: #include <drogon/drogon.h> using namespace drogon; int main() { app().setLogPath(\"./\") .setLogLevel(trantor::Logger::kWarn) .addListener(\"0.0.0.0\", 80) .setThreadNum(16) .enableRunAsDaemon() .run(); } It can be further simplified by using configuration file as follows: #include <drogon/drogon.h> using namespace drogon; int main() { app().loadConfigFile(\"./config.json\").run(); } Drogon provides some interfaces for adding controller logic directly in the main() function, for example, user can register a handler like this in Drogon: app().registerHandler(\"/test?username={name}\", [](const HttpRequestPtr& req, std::function<void (const HttpResponsePtr &)> &&callback, const std::string &name) { Json::Value json; json[\"result\"]=\"ok\"; json[\"message\"]=std::string(\"hello,\")+name; auto resp=HttpResponse::newHttpJsonResponse(json); callback(resp); }, {Get,\"LoginFilter\"}); While such interfaces look intuitive, they are not suitable for complex business logic scenarios. Assuming there are tens or even hundreds of handlers that need to be registered in the framework, isn't it a better practice to implement them separately in their respective classes? So unless your logic is very simple, we don't recommend using above interfaces. Instead, we can create an HttpSimpleController as follows: /// The TestCtrl.h file #pragma once #include <drogon/HttpSimpleController.h> using namespace drogon; class TestCtrl:public drogon::HttpSimpleController<TestCtrl> { public: virtual void asyncHandleHttpRequest(const HttpRequestPtr& req, std::function<void (const HttpResponsePtr &)> &&callback) override; PATH_LIST_BEGIN PATH_ADD(\"/test\",Get); PATH_LIST_END }; /// The TestCtrl.cc file #include \"TestCtrl.h\" void TestCtrl::asyncHandleHttpRequest(const HttpRequestPtr& req, std::function<void (const HttpResponsePtr &)> &&callback) { //write your application logic here auto resp = HttpResponse::newHttpResponse(); resp->setBody(\"<p>Hello, world!</p>\"); resp->setExpiredTime(0); callback(resp); } Most of the above programs can be automatically generated by the command line tool drogon_ctl provided by drogon (The command is drogon_ctl create controller TestCtrl). All the user needs to do is add their own business logic. In the example, the controller returns a Hello, world! string when the client accesses the http://ip/test URL. For JSON format response, we create the controller as follows: /// The header file #pragma once #include <drogon/HttpSimpleController.h> using namespace drogon; class JsonCtrl : public drogon::HttpSimpleController<JsonCtrl> { public: virtual void asyncHandleHttpRequest(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback) override; PATH_LIST_BEGIN //list path definitions here; PATH_ADD(\"/json\", Get); PATH_LIST_END }; /// The source file #include \"JsonCtrl.h\" void JsonCtrl::asyncHandleHttpRequest(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback) { Json::Value ret; ret[\"message\"] = \"Hello, World!\"; auto resp = HttpResponse::newHttpJsonResponse(ret); callback(resp); } Let's go a step further and create a demo RESTful API with the HttpController class, as shown below (Omit the source file): /// The header file #pragma once #include <drogon/HttpController.h> using namespace drogon; namespace api { namespace v1 { class User : public drogon::HttpController<User> { public: METHOD_LIST_BEGIN //use METHOD_ADD to add your custom processing function here; METHOD_ADD(User::getInfo, \"/{id}\", Get); //path is /api/v1/User/{arg1} METHOD_ADD(User::getDetailInfo, \"/{id}/detailinfo\", Get); //path is /api/v1/User/{arg1}/detailinfo METHOD_ADD(User::newUser, \"/{name}\", Post); //path is /api/v1/User/{arg1} METHOD_LIST_END //your declaration of processing function maybe like this: void getInfo(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback, int userId) const; void getDetailInfo(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback, int userId) const; void newUser(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback, std::string &&userName); public: User() { LOG_DEBUG << \"User constructor!\"; } }; } // namespace v1 } // namespace api As you can see, users can use the HttpController to map paths and parameters at the same time. This is a very convenient way to create a RESTful API application. In addition, you can also find that all handler interfaces are in asynchronous mode, where the response is returned by a callback object. This design is for performance reasons because in asynchronous mode the drogon application can handle a large number of concurrent requests with a small number of threads. After compiling all of the above source files, we get a very simple web application. This is a good start. For more information, please visit the wiki or DocsForge Contributions Every contribution is welcome. Please refer to the contribution guidelines for more information. ",
        "2 small things.<p>a) Show me the code to start with, don't send me digging for it. A SimpleController example as the very first thing would help give a feel for the project, and makes me more likely to consider the project.<p>b) If there's an easier way (like the drogon_ctl utility at the start of Quickstart [0]), show that first, and the more detailed way second.<p>Other than that, it looks great. I've used libmicrohttpd a few times, so a bit less of an overhead always looks great.<p>[0] <a href=\"https://github.com/an-tao/drogon/wiki/quick-start\" rel=\"nofollow\">https://github.com/an-tao/drogon/wiki/quick-start</a>",
        "This is cool, and I like it. Very Haskell like, which is a compliment in my book.<p>But one thing that surprises me is that folks are essentially sleeping on HTTP/2. HTTP/2 is just a hell of a lot better in most every dimension. It's better for handshake latency, it's better for bandwidth in most cases, it's better for eliminating excess SSL overhead and also, it's kinda easier to write client libraries for, because it's so much simpler (although the parallel and concurrent nature of connections will challenge a lot of programmers).<p>It's not bad to see a new contender in this space, but it's surprising that it isn't http/2 first. Is there a good reason for this? It's busted through 90% support on caniuse, so it's hard to make an argument that adoption holds it back."
      ],
      "relevant": "false"
    },
    {
      "id": 21912838,
      "title": "Zwitterion: a web dev server that lets you import anything",
      "search": [
        "Zwitterion: a web dev server that lets you import anything",
        "https://github.com/lastmjs/zwitterion",
        "Zwitterion A web dev server that lets you import anything* * If by anything you mean: JavaScript ES2015+, TypeScript, JSON, JSX, TSX, AssemblyScript, Rust, C, C++, WebAssembly, and in the future anything that compiles to JavaScript or WebAssembly. Zwitterion is designed to be an instant replacement for your current web development static file server. Production deployments are also possible through the static build. For example, you can write stuff like the following and it just works: ./index.html: <!DOCTYPE html> <html> <head> <script type=\"module\" src=\"app.ts\"></script> </head> <body> This is the simplest developer experience I've ever had! </body> </html> ./app.ts: import { getHelloWorld } from './hello-world.ts'; const helloWorld: string = getHelloWorld(); console.log(helloWorld); ./hello-world.ts: export function getHelloWorld(): string { return 'Why hello there world!'; } Really, it just works. Zwitterion lets you get back to the good old days of web development. Just write your source code in any supported language and run it in the browser. Also...Zwitterion is NOT a bundler. It eschews bundling for a simpler experience. Current Features ES2015+ TypeScript JSON JSX TSX AssemblyScript Rust (basic support) C (basic support) C++ (basic support) WebAssembly Text Format (Wat) WebAssembly (Wasm) Bare imports (import * as stuff from 'library'; instead of import * as stuff from '../node_modules/library/index.js';) Single Page Application routing (by default the server returns index.html on unhandled routes) Static build for production deployment Upcoming Features More robust Rust integration (i.e. automatic local Rust installation during npm installation) More robust C integration More robust C++ integration Import maps HTTP2 optimizations Documentation Examples Installation and Basic Use Production Use Languages JavaScript TypeScript JSON JSX TSX AssemblyScript Rust C C++ WebAssembly Text Format (Wat) WebAssembly (Wasm) Command Line Options Special Considerations Under the Hood Installation and Basic Use Local Installation and Use Install Zwitterion in the directory that you would like to serve files from: Run Zwitterion by accessing its executable directly from the terminal: node_modules/.bin/zwitterion or from an npm script: { ... \"scripts\": { \"start\": \"zwitterion\" } ... } Global Installation and Use Install Zwitterion globally to use across projects: npm install -g zwitterion Run Zwitterion from the terminal: or from an npm script: { ... \"scripts\": { \"start\": \"zwitterion\" } ... } Production Use It is recommended to use Zwitterion in production by creating a static build of your project. A static build essentially runs all relevant files through Zwitterion, and copies those and all other files in your project to a dist directory. You can take this directory and upload it to a Content Delivery Network (CDN), or another static file hosting service. You may also use a running Zwitterion server in production, but for performance and potential security reasons it is not recommended. To create a static build, run Zwitterion with the --build-static option. You will probably need to add the application/javascript MIME type to your hosting provider for your TypeScript, AssemblyScript, Rust, Wasm, and Wat files. From the terminal: zwitterion --build-static From an npm script: { ... \"scripts\": { \"build-static\": \"zwitterion --build-static\" } ... } The static build will be located in a directory called dist, in the same directory that you ran the --build-static command from. Languages JavaScript JavaScript is the language of the web. You can learn more here. Importing JavaScript ES2015+ is straightforward and works as expected. Simply use import and export statements without any modifications. It is recommended to use explicit file extensions: ./app.js: import { helloWorld } from './hello-world.js'; console.log(helloWorld()); ./hello-world.js: export function helloWorld() { return 'Hello world!'; } JavaScript transpilation is done by the TypeScript compiler. By default, the TypeScript compiler's compilerOptions are set to the following: { \"module\": \"ES2015\", \"target\": \"ES2015\" } You can override these options by creating a .json file with your own compilerOptions and telling Zwitterion where to locate it with the --tsc-options-file command line option. The available options can be found here. Options are specified as a JSON object. For example: tsc-options.json: Tell Zwitterion where to locate it: zwitterion --tsc-options-file tsc-options.json TypeScript TypeScript is a typed superset of JavaScript. You can learn more here. Importing TypeScript is straightforward and works as expected. Simply use import and export statements without any modifications. It is recommended to use explicit file extensions: ./app.ts: import { helloWorld } from './hello-world.ts'; console.log(helloWorld()); ./hello-world.ts: export function helloWorld(): string { return 'Hello world!'; } By default, the TypeScript compiler's compilerOptions are set to the following: { \"module\": \"ES2015\", \"target\": \"ES2015\" } You can override these options by creating a .json file with your own compilerOptions and telling Zwitterion where to locate it with the --tsc-options-file command line option. The available options can be found here. Options are specified as a JSON object. For example: tsc-options.json: Tell Zwitterion where to locate it: zwitterion --tsc-options-file tsc-options.json JSON JSON is provided as a default export. It is recommended to use explicit file extensions: ./app.js: import helloWorld from './hello-world.json'; console.log(helloWorld); ./hello-world.json: JSX Importing JSX is straightforward and works as expected. Simply use import and export statements without any modifications. It is recommended to use explicit file extensions: ./app.js: import { helloWorldElement } from './hello-world.jsx'; ReactDOM.render( helloWorldElement, document.getElementById('root') ); ./hello-world.jsx: export const hellowWorldElement = <h1>Hello, world!</h1>; JSX transpilation is done by the TypeScript compiler. By default, the TypeScript compiler's compilerOptions are set to the following: { \"module\": \"ES2015\", \"target\": \"ES2015\" } You can override these options by creating a .json file with your own compilerOptions and telling Zwitterion where to locate it with the --tsc-options-file command line option. The available options can be found here. Options are specified as a JSON object. For example: tsc-options.json: Tell Zwitterion where to locate it: zwitterion --tsc-options-file tsc-options.json TSX Importing TSX is straightforward and works as expected. Simply use import and export statements without any modifications. It is recommended to use explicit file extensions: ./app.js: import { helloWorldElement } from './hello-world.tsx'; ReactDOM.render( helloWorldElement, document.getElementById('root') ); ./hello-world.tsx: const helloWorld: string = 'Hello, world!'; export const hellowWorldElement = <h1>{ helloWorld }</h1>; TSX transpilation is done by the TypeScript compiler. By default, the TypeScript compiler's compilerOptions are set to the following: { \"module\": \"ES2015\", \"target\": \"ES2015\" } You can override these options by creating a .json file with your own compilerOptions and telling Zwitterion where to locate it with the --tsc-options-file command line option. The available options can be found here. Options are specified as a JSON object. For example: tsc-options.json: Tell Zwitterion where to locate it: zwitterion --tsc-options-file tsc-options.json AssemblyScript AssemblyScript is a new language that compiles a strict subset of TypeScript to WebAssembly. You can learn more about it in The AssemblyScript Book. Zwitterion assumes that AssemblyScript files have the .as file extension. This is a Zwitterion-specific extension choice, as the AssemblyScript project has not yet chosen its own official file extension. You can follow that discussion here. Zwitterion will follow the official extension choice once it is made. Importing AssemblyScript is nearly identical to importing JavaScript or TypeScript. The key difference is that the default export of your entry AssemblyScript module is a function that returns a promise. This function takes as its one parameter an object containing imports to the AssemblyScript module. Passing values to and from functions exported from AssemblyScript modules should be straightforward, but there are some limitations. Zwitterion uses as-bind under the hood to broker values to and from AssemblyScript modules. Look there if you need more information. You can import AssemblyScript from JavaScript or TypeScript files like this: ./app.js: import addModuleInit from './add.as'; runAssemblyScript(); async function runAssemblyScript() { const adddModule = await addModuleInit(); console.log(addModule.add(1, 1)); } ./add.as: export function add(x: i32, y: i32): i32 { return x + y; } If you want to pass in imports from outside of the AssemblyScript environment, you create a file with export declarations defining the types of the imports. You then pass your imports in as an object to the AssemblyScript module init function. The name of the property that defines your imports for a module must be the exact filename of the file exporting the import declarations. For example: ./app.js: import addModuleInit from './add.as'; runAssemblyScript(); async function runAssemblyScript() { const adddModule = await addModuleInit({ 'env.as': { log: console.log } }); console.log(addModule.add(1, 1)); } ./env.as: export declare function log(x: number): void; ./add.as: import { log } from './env.as'; export function add(x: i32, y: i32): i32 { log(x + y); return x + y; } You can also import AssemblyScript from within AssemblyScript files, like so: ./add.as: import { subtract } from './subtract.as'; export function add(x: i32, y: i32): i32 { return subtract(x + y, 0); } ./subtract.as: export function subtract(x: i32, y: i32): i32 { return x - y; } By default, no compiler options have been set. The available options can be found here. You can add options by creating a .json file with an array of option names and values, and telling Zwitterion where to locate it with the --asc-options-file command line option. For example: ./asc-options.json: [ \"--optimizeLevel\", \"3\", \"--runtime\", \"none\", \"--shrinkLevel\", \"2\" ] Tell Zwitterion where to locate it: zwitterion --asc-options-file asc-options.json Rust Rust is a low-level language focused on performance, reliability, and productivity. Learn more here. Rust support is currently very basic (i.e. no wasm-bindgen support). You must have Rust installed on your machine. You can find instructions for installing Rust here. It is a goal of Zwitterion to automatically install a local version of the necessary Rust tooling when Zwitterion is installed, but that is currently a work in progress. Importing Rust is nearly identical to importing JavaScript or TypeScript. The key difference is that the default export of your entry Rust module is a function that returns a promise. This function takes as its one parameter an object containing imports to the Rust module. You can import Rust from JavaScript or TypeScript files like this: ./app.js import addModuleInit from './add.rs'; runRust(); async function runRust() { const addModule = await addModuleInit(); console.log(addModule.add(5, 5)); } ./add.rs #![no_main] #[no_mangle] pub fn add(x: i32, y: i32) -> i32 { return x + y; } C C support is currently very basic. You must have Emscripten installed on your machine. You can find instructions for installing Emscripten here. It is a goal of Zwitterion to automatically install a local version of the necessary C tooling when Zwitterion is installed, but that is currently a work in progress. Importing C is nearly identical to importing JavaScript or TypeScript. The key difference is that the default export of your entry C module is a function that returns a promise. This function takes as its one parameter an object containing imports to the C module. You can import C from JavaScript or TypeScript files like this: ./app.js import addModuleInit from './add.c'; runC(); async function runC() { const addModule = await addModuleInit(); console.log(addModule.add(5, 5)); } ./add.c int add(int x, int y) { return x + y; } C++ C++ support is currently very basic. You must have Emscripten installed on your machine. You can find instructions for installing Emscripten here. It is a goal of Zwitterion to automatically install a local version of the necessary C++ tooling when Zwitterion is installed, but that is currently a work in progress. Importing C++ is nearly identical to importing JavaScript or TypeScript. The key difference is that the default export of your entry C++ module is a function that returns a promise. This function takes as its one parameter an object containing imports to the C++ module. You can import C++ from JavaScript or TypeScript files like this: ./app.js import addModuleInit from './add.cpp'; runCPP(); async function runCPP() { const addModule = await addModuleInit(); console.log(addModule.add(5, 5)); } ./add.cpp extern \"C\" { int add(int x, int y) { return x + y; } } WebAssembly Text Format (Wat) Wat is a textual representation of the Wasm binary format. It allows Wasm to be more easily written by hand. Learn more here. Importing Wat is nearly identical to importing JavaScript or TypeScript. The key difference is that the default export of your entry Wat module is a function that returns a promise. This function takes as its one parameter an object containing imports to the Wat module. You can import Wat from JavaScript or TypeScript files like this: ./app.js import addModuleInit from './add.wat'; runWat(); async function runWat() { const addModule = await addModuleInit(); console.log(addModule.add(5, 5)); } ./add.wat (module (func $add (param $x i32) (param $y i32) (result i32) (i32.add (get_local $x) (get_local $y)) ) (export \"add\" (func $add)) ) WebAssembly (Wasm) Wasm is a binary instruction format built to be efficient, safe, portable, and open. Learn more here. Importing Wasm is nearly identical to importing JavaScript or TypeScript. The key difference is that the default export of your entry Wasm module is a function that returns a promise. This function takes as its one parameter an object containing imports to the Wasm module. You can import Wasm from JavaScript or TypeScript files like this: ./app.js import addModuleInit from './add.wasm'; runWasm(); async function runWasm() { const addModule = await addModuleInit(); console.log(addModule.add(5, 5)); } ./add.wasm Imagine this is a compiled Wasm binary file with a function called `add` Command Line Options Port Specify the server's port: Build Static Create a static build of the current working directory. The output will be in a directory called dist in the current working directory: Exclude A comma-separated list of paths, relative to the current directory, to exclude from the static build: Include A comma-separated list of paths, relative to the current directory, to include in the static build SPA Root A path to a file, relative to the current directory, to serve as the SPA root. It will be returned for the root path and when a file cannot be found: Disable SPA Disable the SPA redirect to index.html: Headers File A path to a JSON file, relative to the current directory, for custom HTTP headers: --headers-file [headersFile] Custom HTTP headers are specified as a JSON object with the following shape: type CustomHTTPHeaders = { [regexp: string]: HTTPHeaders; } type HTTPHeaders = { [key: string]: string; } For example: ./headers.json { \"^service-worker.ts$\": { \"Service-Worker-Allowed\": \"/\" } } TSC Options File A path to a JSON file, relative to the current directory, for tsc compiler options: --tsc-options-file [tscOptionsFile] The available options can be found here. Options are specified as a JSON object. For example: tsc-options.json: ASC Options File A path to a JSON file, relative to the current directory, for asc compiler options: --asc-options-file [ascOptionsFile] By default, no compiler options have been set. The available options can be found here. Options are specified as an array of option names and values. For example: ./asc-options.json: [ \"--optimizeLevel\", \"3\", \"--runtime\", \"none\", \"--shrinkLevel\", \"2\" ] Special Considerations Third-party Packages Third-party packages must be authored as if they were using Zwitterion. Essentially this means they should be authored in standard JavaScript or TypeScript, and AssemblyScript, Rust, C, and C++ must be authored according to their WebAssembly documentation. Notable exceptions will be explained in this documentation. CommonJS (the require syntax), JSON, HTML, or CSS ES Module imports, and other non-standard features that bundlers commonly support are not suppored in source code. Root File It's important to note that Zwitterion assumes that the root file (the file found at /) of your web application is always an index.html file. ES Module script elements Zwitterion depends on native browser support for ES modules (import/export syntax). You must add the type=\"module\" attribute to script elements that reference modules, for example: <script type=\"module\" src=\"amazing-module.ts\"></script> Performance It's important to note that Zwitterion does not bundle files nor engage in tree shaking. This may impact the performance of your application. HTTP2 and ES modules may help with performance, but at this point in time signs tend to point toward worse performance. Zwitterion has plans to improve performance by automatically generating HTTP2 server push information from the static build, and looking into tree shaking, but it is unclear what affect this will have. Stay tuned for more information about performance as Zwitterion matures. With all of the above being said, the performance implications are unclear. Measure for yourself. Read the following for more information on bundling versus not bundling with HTTP2: https://medium.com/@asyncmax/the-right-way-to-bundle-your-assets-for-faster-sites-over-http-2-437c37efe3ff https://stackoverflow.com/questions/30861591/why-bundle-optimizations-are-no-longer-a-concern-in-http-2 http://engineering.khanacademy.org/posts/js-packaging-http2.htm https://blog.newrelic.com/2016/02/09/http2-best-practices-web-performance/ https://mattwilcox.net/web-development/http2-for-front-end-web-developers https://news.ycombinator.com/item?id=9137690 https://www.sitepoint.com/file-bundling-and-http2/ https://medium.freecodecamp.org/javascript-modules-part-2-module-bundling-5020383cf306 https://css-tricks.com/musings-on-http2-and-bundling/ Under the Hood Zwitterion is simple. It is more or less a static file server, but it rewrites requested files in memory as necessary to return to the client. For example, if a TypeScript file is requested from the client, Zwitterion will retrieve the text of the file, compile it to JavaScript, and then return the compiled text to the client. The same thing is done for JavaScript files. In fact, nearly the same process will be used for any file extension that we want to support in the future. For example, in the future, if a C file is requested it will be read into memory, the text will be compiled to WebAssembly, and the WebAssembly will be returned to the client. All of this compilation is done server-side and hidden from the user. To the user, it's just a static file server. ",
        "Can anyone point me to a workflow where the cognitive load is similar to how it was in the good old days?<p>i.e. some <i>thing</i> that does everything for me apart from the bit where I write application and presentation code? That mostly just works and doesn't require me to understand the whole stack.<p>Because when I switch to other languages/environments I can most remain blissfully unaware of the plumbing that keeps the thing running. I learned all I needed to learn about pip and virtualenv in under an hour. My C# editor just lets me edit code and hit play.<p>Is there any chance of getting to this state of nirvana for web development?",
        "It doesn't appear to support CSS files (or other flavors like SCSS), so calling it a \"webpack killer\" seems extra. Also, a lot of people need polyfills for node modules written for NodeJS that are used in browser; for example, Buffer. Webpack takes care of all of those nuances for you. I wouldn't call this a killer for my workflows, where I need Webpack features such as aliasing, etc.<p>That being said, I support the effort to make a simpler, superior bundler. My disagreement is more so with the marketing in the title. Though, those sort of claims are what drive clicks today."
      ],
      "relevant": "true"
    },
    {
      "id": 18937195,
      "title": "Gunk: Modern front end and syntax for Protocol Buffers",
      "search": [
        "Gunk: Modern front end and syntax for Protocol Buffers",
        "https://github.com/gunk/gunk",
        "Gunk is a modern frontend and syntax for Protocol Buffers. Quickstart | Installing | Syntax | Configuring | About | Releases Overview Gunk provides a modern project-based workflow along with a Go-derived syntax for defining types and services for use with Protocol Buffers. Gunk is designed to integrate cleanly with existing protoc based build pipelines, while standardizing workflows in a way that is familiar/accessible to Go developers. Quickstart Create a working directory for a project: $ mkdir -p ~/src/example && cd ~/src/example Install gunk and place the following Gunk definitions in example/util.gunk: package util // Util is a utility service. type Util interface { // Echo returns the passed message. Echo(Message) Message } // Message contains an echo message. type Message struct { // Msg is a message from a client. Msg string `pb:\"1\"` } Create the corresponding project configuration in example/.gunkconfig: [generate go] [generate js] import_style=commonjs binary Then, generate protocol buffer definitions/code: $ ls -A .gunkconfig util.gunk $ gunk generate $ ls -A all.pb.go all_pb.js .gunkconfig util.gunk As seen above, gunk generated the corresponding Go and JavaScript protobuf code using the options defined in the .gunkconfig. End-to-end Example A end-to-end example gRPC server implementation, using Gunk definitions is available for review. Debugging protoc commands Underlying commands executed by gunk can be viewed with the following: $ gunk generate -x protoc-gen-go protoc --js_out=import_style=commonjs,binary:/home/user/example --descriptor_set_in=/dev/stdin all.proto Installing The gunk command-line tool can be installed via Release, via Homebrew, via Scoop or via Go: Installing via Release Download a release for your platform Extract the gunk or gunk.exe file from the .tar.bz2 or .zip file Move the extracted executable to somewhere on your $PATH (Linux/macOS) or %PATH% (Windows) Installing via Homebrew (macOS) gunk is available in the gunk/gunk tap, and can be installed in the usual way with the brew command: # add tap $ brew tap gunk/gunk # install gunk $ brew install gunk Installing via Scoop (Windows) gunk can be installed using Scoop: # install scoop if not already installed iex (new-object net.webclient).downloadstring('https://get.scoop.sh') scoop install gunk Installing via Go gunk can be installed in the usual Go fashion: # install gunk $ go get -u github.com/gunk/gunk Protobuf Dependency and Caching The gunk command-line tool uses the protoc command-line tool. gunk can be configured to use protoc at a specified path. If it isn't available, gunk will download the latest protobuf release to the user's cache, for use. It's also possible to pin a specific version, see the section on protoc configuration. Protocol Types and Messages Gunk provides an alternate, Go-derived syntax for defining protocol buffers. As such, Gunk definitions are a subset of the Go programming language. Additionally, a special +gunk annotation is recognized by gunk, to allow the declaration of protocol buffer options: package message import \"github.com/gunk/opt/http\" // Message is a Echo message. type Message struct { // Msg holds a message. Msg string `pb:\"1\" json:\"msg\"` Code int `pb:\"2\" json:\"code\"` } // Util is a utility service. type Util interface { // Echo echoes a message. // // +gunk http.Match{ // Method: \"POST\", // Path: \"/v1/echo\", // Body: \"*\", // } Echo(Message) Message } Technically speaking, gunk is not actually strict subset of go, as gunk allows unused imports; it actually requires them for some features. See the example above; in pure go, this would not be a valid go code, as http is not used outside of the comment. Scalars Gunk's Go-derived syntax uses the canonical Go scalar types of the proto3 syntax, defined by the protocol buffer project: Proto3 Type Gunk Type double float64 float float32 int32 int int32 int32 int64 int64 uint32 uint uint32 uint32 uint64 uint64 bool bool string string bytes []byte Note: Variable-length scalars will be enabled in the future using a tag parameter. Messages Gunk's Go-derived syntax uses Go's struct type declarations for declaring messages, and require a pb:\"<field_number>\" tag to indicate the field number: type Message struct { FieldA string `pb:\"1\"` } type Envelope struct { Message Message `pb:\"1\" json:\"msg\"` } There are additional tags (for example, the json: tag above), that will be recognized by gunk format, and passed on to generators, where possible. Note: When using gunk format, a valid pb:\"<field_number>\" tag will be automatically inserted if not declared. Services Gunk's Go-derived syntax uses Go's interface syntax for declaring services: type SearchService interface { Search(SearchRequest) SearchResponse } The above is equivalent to the following protobuf syntax: service SearchService { rpc Search (SearchRequest) returns (SearchResponse); } Enums Gunk's Go-derived syntax uses Go const's for declaring enums: type MyEnum int const ( MYENUM MyEnum = iota MYENUM2 ) Note: values can also be fixed numeric values or a calculated value (using iota). Maps Gunk's Go-derived syntax uses Go map's for declaring map fields: type Project struct { ProjectID string `pb:\"1\" json:\"project_id\"` } type GetProjectResponse struct { Projects map[string]Project `pb:\"1\"` } Repeated Values Gunk's Go-derived syntax uses Go's slice syntax ([]) for declaring a repeated field: type MyMessage struct { FieldA []string `pb:\"1\"` } Message Streams Gunk's Go-derived syntax uses Go chan syntax for declaring streams: type MessageService interface { List(chan Message) chan Message } The above is equivalent to the following protobuf syntax: service MessageService { rpc List(stream Message) returns (stream Message); } Protocol Options Protocol buffer options are standard messages (ie, a struct), and can be attached to any service, message, enum, or other other type declaration in a Gunk file via the doccomment preceding the type, field, or service: // MyOption is an option. type MyOption struct { Name string `pb:\"1\"` } // +gunk MyOption { // Name: \"test\", // } type MyMessage struct { /* ... */ } Project Configuration Files Gunk uses a top-level .gunkconfig configuration file for managing the Gunk protocol definitons for a project: # Example .gunkconfig for Go, grpc-gateway, Python and JS [generate go] out=v1/go plugins=grpc [generate] out=v1/go command=protoc-gen-grpc-gateway logtostderr=true [generate python] out=v1/python [generate js] out=v1/js import_style=commonjs binary Project Search Path When gunk is invoked from the command-line, it searches the passed package spec (or current working directory) for a .gunkconfig file, and walks up the directory hierarchy until a .gunkconfig is found, or the project's root is encountered. The project root is defined as the top-most directory containing a .git subdirectory, or where a go.mod file is located. Format The .gunkconfig file format is compatible with Git config syntax, and in turn is compatible with the INI file format: [generate] command=protoc-gen-go [generate] out=v1/js protoc=js Global section import_path - see \"Converting Existing Protobuf Files\" strip_enum_type_names - with this option on, enums with their type prefixed will be renamed to the version without prefix. Note that this might produce invalid protobuf that stops compiling in 1.4.* protoc-gen-go, if the enum names clash. Section [protoc] The path where to check for (or where to download) the protoc binary can be configured. The version can also be pinned. Parameters version - the version of protoc to use. If unspecified, defaults to the latest release available. Otherwise, gunk will either download the specified version, or check that the version of protoc at the specified path matches what was configured. path - the path to check for the protoc binary. If unspecified, defaults appropriate user cache directory for the user's OS. If no file exists at the path, gunk will attempt to download protoc. Section [generate[ <type>]] Each [generate] or [generate <type>] section in a .gunkconfig corresponds to a invocation of the protoc-gen-<type> tool. Parameters Each name[=value] parameter defined within a [generate] section will be passed as a parameter to the protoc-gen-<type> tool, with the exception of the following special parameters that override the behavior of the gunk generate tool: command - overrides the protoc-gen-* command executable used by gunk generate. The executable must be findable on $PATH (Linux/macOS) or %PATH% (Windows), or may be the full path to the executable. If not defined, then command will be protoc-gen-<type>, when <type> is the value in [generate <type>]. protoc - overrides the <type> value, causing gunk generate to use the protoc value in place of <type>. out - overrides the output path of protoc. If not defined, output will be the same directory as the location of the .gunk files. plugin_version - specify version of plugin. The plugin is downloaded from github/maven, built in cache and used. It is not installed in $PATH. This currently works with the following plugins: protoc-gen-go protoc-gen-grpc-java protoc-gen-grpc-gateway protoc-gen-openapiv2 (protoc-gen-swagger support is deprecated) protoc-gen-swift (installing swift itself first is necessary) protoc-gen-grpc-swift (installing swift itself first is necessary) protoc-gen-ts (installing node and npm first is necessary) protoc-gen-grpc-python (cmake, gcc is necessary; takes ~10 minutes to clone build) It is recommended to use this function everywhere, for reproducible builds, together with version for protoc. json_tag_postproc - uses json tags defined in gunk file also for go-generated file fix_paths_postproc - for js and ts - by default, gunk generates wrong paths for other imported gunk packages, because of the way gunk moves files around. Works only if js also has import_style=commonjs option. All other name[=value] pairs specified within the generate section will be passed as plugin parameters to protoc and the protoc-gen-<type> generators. Short Form The following .gunkconfig: [generate go] [generate js] out=v1/js is equivalent to: [generate] command=protoc-gen-go [generate] out=v1/js protoc=js Different forms of invocation There are three different forms of gunkconfig sections that have three different semantics. [generate] command=protoc-gen-go [generate] protoc=go [generate go] The first one uses protoc-gen-go plugin directly, without using protoc. It also attempts to move files to the same directory as the gunk file. The second one uses protoc and does not attempt to move any files. Protoc attempts to load plugin from $PATH, if it is not one of the built-in protoc plugins; this will not work together with pinned version and other gunk features and is not recommended outside of built-in protoc generators. The third version is reccomended. It will try to detect whether language is one of built-in protoc generators, in that case behaves like the second way, otherwise behaves like the first. The built-in protoc generators are: cpp java python php ruby csharp objc js Third-Party Protobuf Options Gunk provides the +gunk annotation syntax for declaring protobuf options, and specially recognizes some third-party API annotations, such as Google HTTP options, including all builtin/standard protoc options for code generation: // +gunk java.Package(\"com.example.message\") // +gunk java.MultipleFiles(true) package message import ( \"github.com/gunk/opt/http\" \"github.com/gunk/opt/file/java\" ) type Util interface { // +gunk http.Match{ // Method: \"POST\", // Path: \"/v1/echo\", // Body: \"*\", // } Echo() } Further documentation on available options can be found at the Gunk options project. Formatting Gunk Files Gunk provides the gunk format command to format .gunk files (akin to gofmt): $ gunk format /path/to/file.gunk $ gunk format <pathspec> Converting Existing Protobuf Files Gunk provides the gunk convert command that will converting existing .proto files (or a directory) to the Go-derived Gunk syntax: $ gunk convert /path/to/file.proto $ gunk convert /path/to/protobuf/directory If your .proto is referencing another .proto from another directory, you can add import_path in the global section of your .gunkconfig. If you don't provide import_path it will only search in the root directory. import_path=relative/path/to/protobuf/directory The path to provide is relative from the .gunkconfig location. Furthermore, the referenced files must contain: option go_package=\"path/of/go/package\"; The resulting .gunk file will contain the import path as defined in go_package: import ( name \"path/of/go/package\" ) About Gunk is developed by the team at Brankas, and was designed to streamline API design and development. History From the beginning of the company, the Brankas team defined API types and services in .proto files, leveraging ad-hoc Makefile's, shell scripts, and other non-standardized mechanisms for generating Protocol Buffer code. As development exploded in 2017 (and beyond) with continued addition of backend microservices/APIs, more code repositories and projects, and team members, it became necessary to standardize tooling for the organization as well as reduce the cognitive load of developers (who for the most part were working almost exclusively with Go) when declaring gRPC and REST services. Naming The Gunk name has a cheeky, backronym \"Gunk Unified N-terface Kompiler\", however the name was chosen because it was possible to secure the GitHub gunk project name, was short, concise, and not used by other projects. Additionally, \"gunk\" is an apt description for the \"gunk\" surrounding protocol definition, generation, compilation, and delivery. Contributing Issues, Pull Requests, and other contributions are greatly welcomed and appreciated! Get started with building and running gunk: # clone source repository $ git clone https://github.com/gunk/gunk.git && cd gunk # force GO111MODULES $ export GO111MODULE=on # build and run $ go build && ./gunk Dependency Management Gunk uses Go modules for dependency management, and as such requires Go 1.11+. Please run go mod tidy before submitting any PRs: $ export GO111MODULE=on $ cd gunk && go mod tidy ",
        "I'm not seeing what makes this \"modern\". proto3 is only a few years old and nothing about it strikes me as unusually archaic. Protobuf in general isn't that much older than Go. I can see why Go-compatible syntax would be attractive to Go developers, so maybe that should be in the description rather than \"modern\"?",
        "Not sure why I’d want to define a language independent interchange format in a language specific way and remove all of the tooling help at the same time. Why is this better? A why section/motivations would help greatly."
      ],
      "relevant": "false"
    },
    {
      "id": 18855503,
      "title": "Master npm",
      "search": [
        "Master npm",
        "https://hashnode.com/post/master-npm-in-under-10-minutes-or-get-your-money-back-cjqmak392001i7vs2ufdlvcqb",
        "Weve all heard about Gulp, Grunt, Webpack or whatever tool that's cool at the time you're reading this article. All these tools help with automating your project's build tasks in one way or another. We all know that JavaScript is an interpreted language, so there is no actual code translation into bytecode done during this phase. However, it is also true that given JavaScripts ecosystem, depending on the project, there can be quite a lot of tasks to perform prior, during, and after preparing our code for deployment. These tasks could include things like transpiling your TypeScript files into the actual JavasScript that will get executed, or your SCSS files into CSS, maybe even compile all your images into a sprite sheet for better web performance. And that is just to name a few, there are tons more. With that being said, it is normal that many tools have been created during the past few years to help developers deal with the associated deployment pains. However, one particular tool thats been there since the beginning has been vastly ignored. Im referring to npm, and in this mastering npm tutorial, Im going to show you a set of tips and tricks you can use to cover most of your orchestration needs with it and become an npm master. Please see this is a beginner's guide and if you are just getting started you are in the right place. Let's start! Tip No.1 - Initializing New Packages To start you off with a simple one, you dont need generators for your new projects. Yes, of course, if youre planning on creating a new Express-based project, using Express own generator might be a good idea. But if youre starting something new, you might want to use npms own project generator. Yes, you read right, npm comes with a built-in generator, which helps you complete the basic information every npm project should have. This is the command you should use: your-folder> $ npm init Thats it, simple, direct, to the point and you get the following as a response: You basically get asked a set of questions to fill in your package.json file, which acts as a basic manifest for your project containing a basic description and some extra useful metadata about it e.g. the authors name, what kind of license its being published under, and so on. Note that some of the questions have an option between parenthesis. That means you can simply hit ENTER and that text will be entered by default, which comes extra handy when youre not yet sure how to answer some of the questions. Finally, at the end, you get a preview of how your new package.json file will look like. Then, hit ENTER again and itll be saved in your projects folder. Remember that anything and everything youve entered here can be changed in the future by editing the JSON file. Tip No.2 - Freezing Dependency Versions This is another simple, yet a forgotten feature. Did you know that if you install a new package without specifying its version, itll save it to your package.json file as version ^X.Y.Z? Meaning, at least, make sure you install version X.Y.Z of the package, but if there is a new one that matches X, then install that one (so it could match 1.0.0 and 1.9.9 as well). Maybe you dont see a big issue with that, but in my years of working with Node.js, I cant tell you how many times a poorly managed version change in one of my dependencies ended up preventing my code from working. It sounds crazy, but there is nothing out there that prevents OSS maintainers from making these types of mistakes. In fact, there was recently a problem with a hacker who gained access to the repository of a highly used package and after adding some malicious code, bumped the version up, so projects without version lock would install its new version without even asking for it. I believe this default behavior from npm comes from an innate trust in OSS maintainers, but as weve come to learn in the past, this is not a very good practice. And you could be fooled into thinking this is easily fixed by removing the caret or any other symbol from the version number. That would tell the package manager to download exactly that version, and no other. In that case, the problem comes from your dependencies, if theyve not done the same and version locked their own dependencies, then once you install them, youll run into the same problem. So, how do we fix this problem? Enter npm shrinkwrap $ npm shrinkwrap The above command will rename your package.json file into npm-shrinkwrap.json, and inside the dependencies element, youll notice every single dependency the project and its dependencies have. And from now on, that file will be used whenever you run npm install. To give you an example, for a single dependency such as express, you end up with a 350+ lines JSON file. So yeah, there are a lot of dependencies to deal with. Youll notice that in your new JSON file, youll have a version that starts with ~, which although not an exact match, will only allow updates on minor versions, so going back to the example, you could match 1.0.0 all the way to 1.0.X (with X being any valid number, of course). The point here is that minor version updates are only related to bug fixes that dont break backward compatibility. With that being said, the version bump is still something that needs to be done manually, so if you want to be 100% sure, you should remove those ~ from versions everywhere. Tip No.3 - Running Scripts Let's now move on to more interesting orchestration opportunities, shall we? Running scripts is one of the key feats you can achieve with npm that will give you a chance to start worrying about Grunt, Gulp or any other alternative. Thanks to this feature, you can even use npm as a language agnostic task runner! Its as simple as doing this: By adding the scripts element to your JSON file, you gain the ability to add extra functionality to npm. Out of the box, there are some commands that npm already expects such as start and test. So you can use those directly like this: $ npm start And the command you specify (node index.js in this example) will be executed. If on the other hand, you want to go ahead and add custom commands (which is the whole point of doing this), you can run them using npm run, like this: $ npm run <command-name> So, for example, on our screenshot, if we wanted to do the production preparations, we can do: $ npm run prod-pred And our prod-prep.sh script, is just a simple Bash script that could look like the following or as complex as you need it: #!/bin/bash echo \"== Doing the production preps ==\" echo \"Running tests...\" mocha test echo \"Linting the code...\" jshint **.js Its a very simple mechanic, but powerful at the same time, without the need for an extra task runner. We can even use hooks to automate scripts depending on what were trying to do, so lets look at that now. You can also get the list of available commands by typing: $ npm run With no arguments, npm will return the full list of commands, grouped by Lifecycle related and those available through npm run, like this: Lifecycle scripts included in test2-npm: test mocha test start node index.js available via `npm run-script`: prod-pred sh prod-prep.sh Tip No.4 - Running Scripts Before and After Other Scripts Although running scripts is one of the most powerful features of npm, giving it the ability to come much more than simply a package manager. It goes one step further by providing you with hooks. Long story short, every single command you can configure for your project will have a pre and post hook. Even those which you dont have full control over, such as install, uninstall, publish or update. So you can do things like: { \"name\": \"test\", \"version\": \"1.0.0\", \"description\": \"\", \"main\": \"index.js\", \"scripts\": { \"prepublish\": \"mocha test\", \"build\": \"sh build-project.sh\", \"postbuild\": \"rm -rf ./tmp\" }, \"author\": \"\", \"license\": \"ISC\", \"dependencies\": { \"mocha\": \"^5.2.0\" } } Notice how Ive added a pre-publish hook, to make sure all tests pass before publishing the package (that single line will make sure you dont forget about doing that ever again!). And Ive also added a simple post-build hook, showing you how you can add hooks to your own commands as well, in this case, it will remove any temporary folder or files created by a generic build process. The main takeaway here is that you can add hooks to existing and custom commands and the execution flow will look at their exit codes, meaning that if the tests fail for our first hook, the publish command will not be executed. Tip No.5 - Automatically Increasing Package Versions Another little-known feature of npm is that it allows you to automatically increase your packages version number. The versioning system used by npm is semver, so you have three variants to run whenever you need to bump up your version number: $ npm version patch $ npm version minor $ npm version major To make sure were all on the same page: semver works by having 3 numbers for each version, distributed like this: major.minor.patch And their meanings are: Major: references the main package version number. Changes to this number imply that non-backward compatible changes have been made, either to a feature or to the entire thing. Either way, rest assured that if you were using it and jump to the next major version, unless you update your own code, something will break. Minor: similar to the previous one, but a change in the minor version references a backward-compatible one. Meaning that most likely new features have been added to the package without breaking existing ones. If this number changes, it is recommended that you go through the documentation just to be sure. Patch: as already discussed, changes to this number on the version reference small fixes, usually bug fixes. Although it might seem like a small feature, you could pair it with scripts and hooks and come up with something like this: { \"name\": \"test\", \"version\": \"1.0.0\", \"description\": \"\", \"main\": \"index.js\", \"scripts\": { \"publish:fix\": \"npm publish\", \"publish:minor\": \"npm publish\", \"publish:major\": \"npm publish\", \"prepublish:fix\": \"npm version patch\", \"prepublish:minor\": \"npm version minor\", \"prepublish:major\": \"npm version major\" }, \"author\": \"\", \"license\": \"ISC\", \"dependencies\": { \"mocha\": \"^5.2.0\" } } And with that, you can simply choose what youre publishing and forget about the version number. For example: $ npm run publish:fix Tip No.6 - Using npx Sometimes your packages or others will add command line tools for you to use, Ive been using the example of mocha, for instance, which adds the mocha command that needs to be executed in order to run and verify the tests. Usually, these commands are either installed in the global cache or in the local node_modules folder, in which case theyre not readily accessible for you to use by simply typing their name. Since version 5.2, npm added the npx command, which simplifies that task for you. Instead of having to know where the binary is located (if at all), itll look for it in your $PATH and then (if not found) in your local node_modules/.bin . Not only that, but if it cant find it anywhere, itll install the package for you and then run the binary, all in one step. Using npx comes in handy when you need the power of a cli tool available from npms global registry, but you dont want to have it installed on your system. Tip No.7 - Adding npm Autocomplete to Your Bash This one is strictly for *nix based systems or Bash for Windows 10, but I think its pretty useful to have under your tool belt. One thing that is very useful when using Bash or any command line interface is the autocomplete feature they usually provide. The problem comes when youre using custom command line tools, they dont always provide auto-complete for their own arguments and (or) commands. Luckily for us, npm allows you to add this feature to your bash, simply with a single line of code: $ npm completion >> ~/.bashrc Warning: Make sure youre doing the double >, otherwise youd be replacing your .bashrc file content, instead of adding the required code for autocomplete. After that, you can either open a new terminal or run: $ source ~/.bashrc Thatll reload your Bash configuration and the autocomplete with it. Once this is done, you can test it by typing npm ins and hitting TAB. Note that the autocomplete feature also works for custom commands too, so you can also get that extra help if youre heavy on the scripts! Final tips In this npm tutorial, I covered several ways to improve your \"npm-fu\", such as using scripts and hooks to run custom actions, automating your version handling and locking dependencies versions. That being said, if you liked what you read, go check out npms official documentation, there are many things you can do with this tool that will probably help you get rid of some extra dependencies you might already have in your project. Leave a comment if any of these tips where useful to you or send me a message on Hashnode if you have any questions about them! ",
        "The article covers the basics, but not even close to mastering.<p>shrinkwrap should really not be used anymore - package-lock is mostly automatic, and much better than shrinkwrap for what you want to do in most projects.<p>If you want to talk about mastering npm, there are way more topics to cover, such as resolving issues when upgrading (`npm ls` helps here) and checking for vulnerabilities (`npm audit`).",
        "I'm not sure if any of this can be considered mastering NPM. In reality this is a uber-basic introduction to a handful of the most simple commands."
      ],
      "relevant": "false"
    },
    {
      "id": 21370525,
      "title": "Git repository summary on your terminal",
      "search": [
        "Git repository summary on your terminal",
        "https://github.com/o2sh/onefetch",
        "A command-line Git information tool written in Rust | | | Onefetch is a command-line Git information tool written in Rust that displays project information and code statistics for a local Git repository directly on your terminal. The tool is completely offline - no network access is required. By default, the repo's information is displayed alongside the dominant language's logo, but you can further configure onefetch to instead use an image - on supported terminals -, a text input or nothing at all. It automatically detects open source licenses from texts and provides the user with valuable information like code distribution, pending changes, number of dependencies (by package manager), top contributors (by number of commits), size on disk, creation date, LOC (lines of code), etc. Onefetch can be configured via command-line flags to display exactly what you want, the way you want it to: you can customize ASCII/Text formatting, disable info lines, ignore files & directories, output in multiple formats (Json, Yaml), etc. As of now, onefetch supports more than 50 different programming languages; if your language of choice isn't supported: Open up an issue and support will be added. Contributions are very welcome! See CONTRIBUTING for more info. More: [Wiki] [Installation] [Getting Started] ",
        "Reminds me of Ohloh (now OpenHub) but for your own projects and on command line.",
        "Patterns of commit activity might be shown using unicode sparklines and/or a color scale (foreground and/or background).<p>\"C++ (41.75 %)\" seems a lot of ink and space for \"C++ 42%\"."
      ],
      "relevant": "true"
    },
    {
      "id": 21676256,
      "title": "Open Source Webhook Server",
      "search": [
        "Open Source Webhook Server",
        "https://github.com/adnanh/webhook",
        "What is webhook? webhook is a lightweight configurable tool written in Go, that allows you to easily create HTTP endpoints (hooks) on your server, which you can use to execute configured commands. You can also pass data from the HTTP request (such as headers, payload or query variables) to your commands. webhook also allows you to specify rules which have to be satisfied in order for the hook to be triggered. For example, if you're using Github or Bitbucket, you can use webhook to set up a hook that runs a redeploy script for your project on your staging server, whenever you push changes to the master branch of your project. If you use Mattermost or Slack, you can set up an \"Outgoing webhook integration\" or \"Slash command\" to run various commands on your server, which can then report back directly to you or your channels using the \"Incoming webhook integrations\", or the appropriate response body. webhook aims to do nothing more than it should do, and that is: receive the request, parse the headers, payload and query variables, check if the specified rules for the hook are satisfied, and finally, pass the specified arguments to the specified command via command line arguments or via environment variables. Everything else is the responsibility of the command's author. Hookdoo If you don't have time to waste configuring, hosting, debugging and maintaining your webhook instance, we offer a SaaS solution that has all of the capabilities webhook provides, plus a lot more, and all that packaged in a nice friendly web interface. If you are interested, find out more at hookdoo website. If you have any questions, you can contact us at info@hookdoo.com If you need a way of inspecting, monitoring and replaying webhooks without the back and forth troubleshooting, give Hookdeck a try! Getting started Installation Building from source To get started, first make sure you've properly set up your Go 1.14 or newer environment and then run $ go build github.com/adnanh/webhook to build the latest version of the webhook. Using package manager Snap store Ubuntu If you are using Ubuntu linux (17.04 or later), you can install webhook using sudo apt-get install webhook which will install community packaged version. Debian If you are using Debian linux (\"stretch\" or later), you can install webhook using sudo apt-get install webhook which will install community packaged version (thanks @freeekanayaka) from https://packages.debian.org/sid/webhook Download prebuilt binaries Prebuilt binaries for different architectures are available at GitHub Releases. Configuration Next step is to define some hooks you want webhook to serve. webhook supports JSON or YAML configuration files, but we'll focus primarily on JSON in the following example. Begin by creating an empty file named hooks.json. This file will contain an array of hooks the webhook will serve. Check Hook definition page to see the detailed description of what properties a hook can contain, and how to use them. Let's define a simple hook named redeploy-webhook that will run a redeploy script located in /var/scripts/redeploy.sh. Make sure that your bash script has #!/bin/sh shebang on top. Our hooks.json file will now look like this: [ { \"id\": \"redeploy-webhook\", \"execute-command\": \"/var/scripts/redeploy.sh\", \"command-working-directory\": \"/var/webhook\" } ] NOTE: If you prefer YAML, the equivalent hooks.yaml file would be: - id: redeploy-webhook execute-command: \"/var/scripts/redeploy.sh\" command-working-directory: \"/var/webhook\" You can now run webhook using $ /path/to/webhook -hooks hooks.json -verbose It will start up on default port 9000 and will provide you with one HTTP endpoint http://yourserver:9000/hooks/redeploy-webhook Check webhook parameters page to see how to override the ip, port and other settings such as hook hotreload, verbose output, etc, when starting the webhook. By performing a simple HTTP GET or POST request to that endpoint, your specified redeploy script would be executed. Neat! However, hook defined like that could pose a security threat to your system, because anyone who knows your endpoint, can send a request and execute your command. To prevent that, you can use the \"trigger-rule\" property for your hook, to specify the exact circumstances under which the hook would be triggered. For example, you can use them to add a secret that you must supply as a parameter in order to successfully trigger the hook. Please check out the Hook rules page for detailed list of available rules and their usage. Multipart Form Data webhook provides limited support the parsing of multipart form data. Multipart form data can contain two types of parts: values and files. All form values are automatically added to the payload scope. Use the parse-parameters-as-json settings to parse a given value as JSON. All files are ignored unless they match one of the following criteria: The Content-Type header is application/json. The part is named in the parse-parameters-as-json setting. In either case, the given file part will be parsed as JSON and added to the payload map. Templates webhook can parse the hooks configuration file as a Go template when given the -template CLI parameter. See the Templates page for more details on template usage. Using HTTPS webhook by default serves hooks using http. If you want webhook to serve secure content using https, you can use the -secure flag while starting webhook. Files containing a certificate and matching private key for the server must be provided using the -cert /path/to/cert.pem and -key /path/to/key.pem flags. If the certificate is signed by a certificate authority, the cert file should be the concatenation of the server's certificate followed by the CA's certificate. TLS version and cipher suite selection flags are available from the command line. To list available cipher suites, use the -list-cipher-suites flag. The -tls-min-version flag can be used with -list-cipher-suites. CORS Headers If you want to set CORS headers, you can use the -header name=value flag while starting webhook to set the appropriate CORS headers that will be returned with each response. Interested in running webhook inside of a Docker container? You can use one of the following Docker images, or create your own (please read this discussion): almir/webhook roxedus/webhook thecatlady/webhook Examples Check out Hook examples page for more complex examples of hooks. Guides featuring webhook Plex 2 Telegram by @psyhomb Webhook & JIRA by @perfecto25 Trigger Ansible AWX job runs on SCM (e.g. git) commit by @jpmens Deploy using GitHub webhooks by @awea Setting up Automatic Deployment and Builds Using Webhooks by Will Browning Auto deploy your Node.js app on push to GitHub in 3 simple steps by Karolis Rusenas Automate Static Site Deployments with Salt, Git, and Webhooks by Linode Using Prometheus to Automatically Scale WebLogic Clusters on Kubernetes by Marina Kogan Github Pages and Jekyll - A New Platform for LACNIC Labs by Carlos Martnez Cagnazzo How to Deploy React Apps Using Webhooks and Integrating Slack on Ubuntu by Arslan Ud Din Shafiq Private webhooks by Thomas Adventures in webhooks by Drake GitHub pro tips by Spencer Lyon XiaoMi Vacuum + Amazon Button = Dash Cleaning by c0mmensal Set up Automated Deployments From Github With Webhook by Maxim Orlov VIDEO: Gitlab CI/CD configuration using Docker and adnanh/webhook to deploy on VPS - Tutorial #1 by Yes! Let's Learn Software Engineering Integrate automatic deployment in 20 minutes using webhooks + Nginx setup by Anksus ... Want to add your own? Open an Issue or create a PR :-) Community Contributions See the webhook-contrib repository for a collections of tools and helpers related to webhook that have been contributed by the webhook community. Need help? Check out existing issues to see if someone else also had the same problem, or open a new one. Support active development Sponsors DigitalOcean is a simple and robust cloud computing platform, designed for developers. BrowserStack is a cloud-based cross-browser testing tool that enables developers to test their websites across various browsers on different operating systems and mobile devices, without requiring users to install virtual machines, devices or emulators. Support this project by becoming a sponsor. Your logo will show up here with a link to your website. By contributing This project exists thanks to all the people who contribute. Contribute!. By giving money OpenCollective Backer OpenCollective Sponsor PayPal Patreon Faircode Flattr Thank you to all our backers! License The MIT License (MIT) Copyright (c) 2015 Adnan Hajdarevic adnanh@gmail.com Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. ",
        "Interesting, could be useful to interface some of my stuff with IFTTT or Integromat.",
        "I have been using this webhook server in prod for a few years and its been easy to setup/maintain ... you just define  github.com repo to publish `git push` or whatever then this webhook server listens to every git push my team makes to launch a code recompile/redeploy ...  foolproof and solid ... I highly recommend"
      ],
      "relevant": "false"
    },
    {
      "id": 21240641,
      "title": "Show HN: Hyperfine – a command-line benchmarking tool",
      "search": [
        "Show HN: Hyperfine – a command-line benchmarking tool",
        "https://github.com/sharkdp/hyperfine",
        " A command-line benchmarking tool. Demo: Benchmarking fd and find: Features Statistical analysis across multiple runs. Support for arbitrary shell commands. Constant feedback about the benchmark progress and current estimates. Warmup runs can be executed before the actual benchmark. Cache-clearing commands can be set up before each timing run. Statistical outlier detection to detect interference from other programs and caching effects. Export results to various formats: CSV, JSON, Markdown, AsciiDoc. Parameterized benchmarks (e.g. vary the number of threads). Cross-platform Usage Basic benchmark To run a benchmark, you can simply call hyperfine <command>.... The argument(s) can be any shell command. For example: Hyperfine will automatically determine the number of runs to perform for each command. By default, it will perform at least 10 benchmarking runs. To change this, you can use the -m/--min-runs option: hyperfine --min-runs 5 'sleep 0.2' 'sleep 3.2' Warmup runs and preparation commands If the program execution time is limited by disk I/O, the benchmarking results can be heavily influenced by disk caches and whether they are cold or warm. If you want to run the benchmark on a warm cache, you can use the -w/--warmup option to perform a certain number of program executions before the actual benchmark: hyperfine --warmup 3 'grep -R TODO *' Conversely, if you want to run the benchmark for a cold cache, you can use the -p/--prepare option to run a special command before each timing run. For example, to clear harddisk caches on Linux, you can run sync; echo 3 | sudo tee /proc/sys/vm/drop_caches To use this specific command with Hyperfine, call sudo -v to temporarily gain sudo permissions and then call: hyperfine --prepare 'sync; echo 3 | sudo tee /proc/sys/vm/drop_caches' 'grep -R TODO *' Parameterized benchmarks If you want to run a benchmark where only a single parameter is varied (say, the number of threads), you can use the -P/--parameter-scan option and call: hyperfine --prepare 'make clean' --parameter-scan num_threads 1 12 'make -j {num_threads}' This also works with decimal numbers. The -D/--parameter-step-size option can be used to control the step size: hyperfine --parameter-scan delay 0.3 0.7 -D 0.2 'sleep {delay}' This runs sleep 0.3, sleep 0.5 and sleep 0.7. Shell functions and aliases If you are using bash, you can export shell functions to directly benchmark them with hyperfine: $ my_function() { sleep 1; } $ export -f my_function $ hyperfine my_function If you are using a different shell, or if you want to benchmark shell aliases, you may try to put them in a separate file: echo 'my_function() { sleep 1 }' > /tmp/my_function.sh echo 'alias my_alias=\"sleep 1\"' > /tmp/my_alias.sh hyperfine 'source /tmp/my_function.sh; eval my_function' hyperfine 'source /tmp/my_alias.sh; eval my_alias' Export results Hyperfine has multiple options for exporting benchmark results: CSV, JSON, Markdown (see --help text for details). To export results to Markdown, for example, you can use the --export-markdown option that will create tables like this: Command Mean [s] Min [s] Max [s] Relative find . -iregex '.*[0-9]\\.jpg$' 2.275 0.046 2.243 2.397 9.79 0.22 find . -iname '*[0-9].jpg' 1.427 0.026 1.405 1.468 6.14 0.13 fd -HI '.*[0-9]\\.jpg$' 0.232 0.002 0.230 0.236 1.00 The JSON output is useful if you want to analyze the benchmark results in more detail. See the scripts/ folder for some examples. Installation On Ubuntu Download the appropriate .deb package from the Release page and install it via dpkg: wget https://github.com/sharkdp/hyperfine/releases/download/v1.12.0/hyperfine_1.12.0_amd64.deb sudo dpkg -i hyperfine_1.12.0_amd64.deb On Fedora On Fedora, hyperfine can be installed from the official repositories: On Alpine Linux On Alpine Linux, hyperfine can be installed from the official repositories: On Arch Linux On Arch Linux, hyperfine can be installed from the official repositories: On Funtoo Linux On Funtoo Linux, hyperfine can be installed from core-kit: emerge app-benchmarks/hyperfine On NixOS On NixOS, hyperfine can be installed from the official repositories: On Void Linux Hyperfine can be installed via xbps xbps-install -S hyperfine On macOS Hyperfine can be installed via Homebrew: Or you can install using MacPorts: sudo port selfupdate sudo port install hyperfine On FreeBSD Hyperfine can be installed via pkg: On OpenBSD With conda Hyperfine can be installed via conda from the conda-forge channel: conda install -c conda-forge hyperfine With cargo (Linux, macOS, Windows) Hyperfine can be installed via cargo: Make sure that you use Rust 1.46 or higher. From binaries (Linux, macOS, Windows) Download the corresponding archive from the Release page. Alternative tools Hyperfine is inspired by bench. Integration with other tools Chronologer is a tool that uses hyperfine to visualize changes in benchmark timings across your Git history. Make sure to check out the scripts folder in this repository for a set of tools to work with hyperfine benchmark results. Origin of the name The name hyperfine was chosen in reference to the hyperfine levels of caesium 133 which play a crucial role in the definition of our base unit of time the second. License hyperfine is dual-licensed under the terms of the MIT License and the Apache License 2.0. See the LICENSE-APACHE and LICENSE-MIT files for details. ",
        "I have submitted \"hyperfine\" 1.5 years ago when it just came out. Since then, the program has gained functionality (statistical outlier detection, result export, parametrized benchmarks) and maturity.<p>Old discussion: <a href=\"https://news.ycombinator.com/item?id=16193225\" rel=\"nofollow\">https://news.ycombinator.com/item?id=16193225</a><p>Looking forward to your feedback!",
        "Most -- nearly all -- benchmarking tools like this work from a normality assumption, i.e. assume that results follow the normal distribution, or is close to it. Some do this on blind faith, others argue from the CLT that \"with infinite samples, the mean is normally distributed, so surely it must be also with finite number of samples, at least a little?\"<p>In fact, performance numbers (latencies) often follow a heavy-tailed distribution. For these, you need a literal shitload of samples to get even a slightly normal mean. For these, the sample mean, the sample variance, the sample centiles -- they all severely underestimate the true values.<p>What's worse is when these tools start to remove \"outliers\". With a heavy-tailed distribution, the majority of samples don't contribute very much at all to the expectation. The strongest signal is found in the extreme values. The strongest signal is found in the stuff that is thrown out. The junk that's left is the noise, the stuff that doesn't tell you very much about what you're dealing with.<p>I stand firm in my belief that unless you can prove how CLT applies to your input distributions, you should not assume normality.<p>And if you don't know what you are doing, stop reporting means. Stop reporting centiles. Report the maximum value. That's a really boring thing to hear, but it is nearly always statistically  and analytically meaningful, so it is a good default."
      ],
      "relevant": "true"
    },
    {
      "id": 19032439,
      "title": "Announcing .NET Core 3 Preview 2",
      "search": [
        "Announcing .NET Core 3 Preview 2",
        "https://blogs.msdn.microsoft.com/dotnet/2019/01/29/announcing-net-core-3-preview-2/",
        "Richard January 29th, 2019 Today, we are announcing .NET Core 3 Preview 2. It includes new features in .NET Core 3.0 and C# 8, in addition to the large number of new features in Preview 1. ASP.NET Core 3.0 Preview 2 is alsoreleased today. C# 8 Preview 2 is part of .NET Core 3 SDK, and was also released last week with Visual Studio 2019 Preview 2. Download and get started with .NET Core 3 Preview 2 right now on Windows, macOS and Linux. In case you missed it, we made some big announcements with Preview 1, including adding support for Windows Forms and WPF with .NET Core, on Windows, and that both UI frameworks will be open source. We also announced that we would support Entity Framework 6 on .NET Core, which will come in a later preview. ASP.NET Core is also adding many features including Razor components. Please provide feedback on the release in the comments below or at dotnet/core #2263. You can see complete details of the release in the .NET Core 3 Preview 2 release notes. .NET Core 3 will be supported in Visual Studio 2019, Visual Studio for Mac and Visual Studio Code. Visual Studio 2019 Preview 2 was released last week and has support for C# 8. The Visual Studio Code C# Extension (in pre-release channel) was also just updated to support C# 8. C# 8 C# 8 is a major release of the language, as Mads describes in Do more with patterns in C# 8.0, Take C# 8.0 for a spin and Building C# 8.0. In this post, Ill cover a few favorites that are new in Preview 2. Using Declarations Are you tired of using statements that require indenting your code? No more! You can now write the following code, which attaches a using declaration to the scope of the current statement block and then disposes the object at the end of it. Switch Expressions Anyone who uses C# probably loves the idea of a switch statement, but not the syntax. C# 8 introduces switch expressions, which enable the following: terser syntax, returns a value since it is an expression, and fully integrated with pattern matching. The switch keyword is infix, meaning the keyword sits between the tested value (here, thats o) and the list of cases, much like expression lambdas. The following examples use the lambda syntax for methods, which integrates well with switch expressions but isnt required. You can see the syntax for switch expressions in the following example: There are two patterns at play in this example. o first matches with the Point type pattern and then with the property pattern inside the {curly braces}. The _ describes the discard pattern, which is the same as default for switch statements. You can go one step further, and rely on tuple deconstruction and parameter position, as you can see in the following example: In this example, you can see you do not need to define a variable or explicit type for each of the cases. Instead, the compiler can match the tuple being testing with the tuples defined for each of the cases. All of these patterns enable you to write declarative code that captures your intent instead of procedural code that implements tests for it. The compiler becomes responsible for implementing that boring procedural code and is guaranteed to always do it correctly. There will still be cases where switch statements will be a better choice than switch expressions and patterns can be used with both syntax styles. Async streams Async streams are another major improvement in C# 8. They have been changing with each preview and require that the compiler and the framework libraries match to work correctly. You need .NET Core 3.0 Preview 2 to use async streams if you want to develop with either Visual Studio 2019 Preview 2 or the latest preview of the C# extension for Visual Studio Code. If you are using .NET Core 3.0 Preview 2 at the commandline, then everything will work as expected. IEEE Floating-point improvements Floating point APIs are in the process of being updated to comply with IEEE 754-2008 revision. The goal of this floating point project is to expose all required operations and ensure that they are behaviorly compliant with the IEEE spec. Parsing and formatting fixes: Correctly parse and round inputs of any length. Correctly parse and format negative zero. Correctly parse Infinity and NaN by performing a case-insensitive check and allowing an optional preceding + where applicable. New Math APIs: BitIncrement/BitDecrement corresponds to the nextUp and nextDown IEEE operations. They return the smallest floating-point number that compares greater or lesser than the input (respectively). For example, Math.BitIncrement(0.0) would return double.Epsilon. MaxMagnitude/MinMagnitude corresponds to the maxNumMag and minNumMag IEEE operations, they return the value that is greater or lesser in magnitude of the two inputs (respectively). For example, Math.MaxMagnitude(2.0, -3.0) would return -3.0. ILogB corresponds to the logB IEEE operation which returns an integral value, it returns the integral base-2 log of the input parameter. This is effectively the same as floor(log2(x)), but done with minimal rounding error. ScaleB corresponds to the scaleB IEEE operation which takes an integral value, it returns effectively x * pow(2, n), but is done with minimal rounding error. Log2 corresponds to the log2 IEEE operation, it returns the base-2 logarithm. It minimizes rounding error. FusedMultiplyAdd corresponds to the fma IEEE operation, it performs a fused multiply add. That is, it does (x * y) + z as a single operation, there-by minimizing the rounding error. An example would be FusedMultiplyAdd(1e308, 2.0, -1e308) which returns 1e308. The regular (1e308 * 2.0) - 1e308 returns double.PositiveInfinity. CopySign corresponds to the copySign IEEE operation, it returns the value of x, but with the sign of y. .NET Platform Dependent Intrinsics Weve added APIs that allow access to certain perf-oriented CPU instructions, such as the SIMD or Bit Manipulation instruction sets. These instructions can help achieve big performance improvements in certain scenarios, such as processing data efficiently in parallel. In addition to exposing the APIs for your programs to use, we have begun using these instructions to accelerate the .NET libraries too. The following CoreCLR PRs demonstrate a few of the intrinsics, either via implementation or use: Implement simple SSE2 hardware instrinsics Implement the SSE hardware intrinsics Arm64 Base HW Intrinsics Use TZCNT and LZCNT for Locate{First|Last}Found{Byte|Char} For more information, take a look at .NET Platform Dependent Intrinsics, which defines an approach for defining this hardware infrastructure, allowing Microsoft, chip vendors or any other company or individual to define hardware/chip APIs that should be exposed to .NET code. Introducing a fast in-box JSON Writer & JSON Document Following the introduction of the JSON reader in preview1, weve added System.Text.Json.Utf8JsonWriter and System.Text.Json.JsonDocument. As described in our System.Text.Json roadmap, we plan to provide a POCO serializer and deserializer next. Utf8JsonWriter The Utf8JsonWriter provides a high-performance, non-cached, forward-only way to write UTF-8 encoded JSON text from common .NET types like String, Int32, and DateTime. Like the reader, the writer is a foundational, low-level type, that can be leveraged to build custom serializers. Writing a JSON payload using the new Utf8JsonWriter is 30-80% faster than using the writer from Json.NET and does not allocate. Here is a sample usage of the Utf8JsonWriter that can be used as a starting point: The Utf8JsonWriter accepts IBufferWriter<byte> as the output location to synchronously write the json data into and you, as the caller, need to provide a concrete implementation. The platform does not currently include an implementation of this interface, but we plan to provide one that is backed by a resizable byte array. That implementation would enable synchronous writes, which could then be copied to any stream (either synchronously or asynchronously). If you are writing JSON over the network and include the System.IO.Pipelines package, you can leverage the Pipe-based implementation of the interface called PipeWriter to skip the need to copy the JSON from an intermediary buffer to the actual output. You can take inspiration from this sample implementation of IBufferWriter<T>. The following is a skeleton array-backed concrete implementation of the interface: JsonDocument In preview2, weve also added System.Text.Json.JsonDocument which was built on top of the Utf8JsonReader. The JsonDocument provides the ability to parse JSON data and build a read-only Document Object Model (DOM) that can be queried to support random access and enumeration. The JSON elements that compose the data can be accessed via the JsonElement type which is exposed by the JsonDocument as a property called RootElement. The JsonElement contains the JSON array and object enumerators along with APIs to convert JSON text to common .NET types. Parsing a typical JSON payload and accessing all its members using the JsonDocument is 2-3x faster than Json.NET with very little allocations for data that is reasonably sized (i.e. < 1 MB). Here is a sample usage of the JsonDocument and JsonElement that can be used as a starting point: GPIO Support for Raspberry Pi We added initial support for GPIO with Preview 1. As part of Preview 2, we have released two NuGet packages that you can use for GPIO programming. System.Device.Gpio Iot.Device.Bindings The GPIO Packages includes APIs for GPIO, SPI, I2C and PWM devices. The IoT bindings package includes device bindings for various chips and sensors, the same ones available at dotnet/iot src/devices. Updated serial port APIs were announced as part of Preview 1. They are not part of these packages but are available as part of the .NET Core platform. Local dotnet tools Local dotnet tools have been improved in Preview 2. Local tools are similar to dotnet global tools but are associated with a particular location on disk. This enables per-project and per-repository tooling. You can read more about them in the .NET Core 3.0 Preview 1 post. In this preview, we have added 2 new commands: dotnet new tool-manifest dotnet tool list To add local tools, you need to add a manifest that will define the tools and versions available. The dotnet new tool-manifest command automates creation of the manifest required by local tools. After this file is created, you can add tools to it via dotnet tool install <packageId>. The command dotnet tool list lists local tools and their corresponding manifest. The command dotnet tool list -g lists global tools. There was a change in .NET Core Local Tools between .NET Core 3.0 Preview 1 and .NET Core 3.0 Preview 2. If you tried out local tools in Preview 1 by running a command like dotnet tool restore or dotnet tool install, you need to delete your local tools cache folder before local tools will work correctly in Preview 2. This folder is located at: On mac, Linux: rm -r $HOME/.dotnet/toolResolverCache On Windows: rmdir /s %USERPROFILE%\\.dotnet\\toolResolverCache If you do not delete this folder, you will receive an error. Assembly Unloadability Assembly unloadability is a new capability of AssemblyLoaderContext. This new feature is largely transparent from an API perspective, exposed with just a few new APIs. It enables a loader context to be unloaded, releasing all memory for instantiated types, static fields and for the assembly itself. An application should be able to load and unload assemblies via this mechanism forever without experiencing a memory leak. We expect this new capability to be used for the following scenarios: Plugin scenarios where dynamic plugin loading and unloading is required. Dynamically compiling, running and then flushing code. Useful for web sites, scripting engines, etc. Loading assemblies for introspection (like ReflectionOnlyLoad), although MetadataLoadContext (released in Preview 1) will be a better choice in many cases. More information: Design doc Using Unloadability doc Assembly unloading requires significant care to ensure that all references to managed objects from outside a loader context are understood and managed. When the loader context is requested to be unloaded, any outside references need to have been unreferenced so that the loader context is self-consistent only to itself. Assembly unloadability was provided in the .NET Framework by Application Domains (AppDomains), which are not supported with .NET Core. AppDomains had both benefits and limitations compared to this new model. We consider this new loader context-based model to be more flexible and higher performance when compared to AppDomains. Windows Native Interop Windows offers a rich native API, in the form of flat C APIs, COM and WinRT. Weve had support for P/Invoke since .NET Core 1.0, and have been adding the ability to CoCreate COM APIs and Activate WinRT APIs as part of the .NET Core 3.0 release. We have had many requests for these capabilities, so we know that they will get a lot of use. Late last year, we announced that we had managed to automate Excel from .NET Core. That was a fun moment. You can now try this same demo yourself with the Excel demo sample. Under the covers, this demo is using COM interop features like NOPIA, object equivalence and custom marshallers. Managed C++ and WinRT interop are coming in a later preview. We prioritized getting COM interop built first. WPF and Windows Forms The WPF and Windows Forms teams opened up their repositories, dotnet/wpf and dotnet/winforms, respectively, on December 4th, the same day .NET Core 3.0 Preview 1 was released. Much of the last month, beyond holidays, has been spent interacting with the community, merging PRs, and responding to issues. In the background, theyve been integrating WPF and Windows Forms into the .NET Core build system, including adopting the Arcade SDK. Arcade is an MSBuild SDK that exposes functionality which is needed to build the .NET Platform. The WPF team will be publishing more of the WPF source code over the coming months. The same teams have been making a final set of changes in .NET Framework 4.8. These same changes have also been added to the .NET Core versions of WPF and Windows Forms. Visual Studio support Desktop development on .NET Core 3 requires Visual Studio 2019. We added the WPF and Windows Forms templates to the New Project Dialog to make it easier starting your new applications without using the command line. The WPF and Windows Forms designer teams are continuing to work on an updated designer for .NET Core, which will be part of a Visual Studio 2019 Update. MSIX Deployment for Desktop apps MSIX is a new Windows app package format. It can be used to deploy .NET Core 3 desktop applications to Windows 10. The Windows Application Packaging Project, available in Visual Studio 2019 preview 2, allows you to create MSIX packages with self-contained .NET Core applications. Note: The .NET Core project file must specify the supported runtimes in the <RuntimeIdentifiers> property: <RuntimeIdentifiers>win-x86;win-x64</RuntimeIdentifiers> Install .NET Core 3.0 Previews on Linux with Snap Snap is the preferred way to install and try .NET Core previews on Linux distributions that support Snap. At present, only X64 builds are supported with Snap. Well look at supporting ARM builds, too. After configuring Snap on your system, run the following command to install the .NET Core SDK 3.0 Preview SDK. sudo snap install dotnet-sdk --beta --classic When .NET Core in installed using the Snap package, the default .NET Core command is dotnet-sdk.dotnet, as opposed to just dotnet. The benefit of the namespaced command is that it will not conflict with a globally installed .NET Core version you may have. This command can be aliased to dotnet with: sudo snap alias dotnet-sdk.dotnet dotnet Some distros require an additional step to enable access to the SSL certificate. See our Linux Setup for details. Platform Support .NET Core 3 will be supported on the following operating systems: Windows Client: 7, 8.1, 10 (1607+) Windows Server: 2012 R2 SP1+ macOS: 10.12+ RHEL: 6+ Fedora: 26+ Ubuntu: 16.04+ Debian: 9+ SLES: 12+ openSUSE: 42.3+ Alpine: 3.8+ Chip support follows: x64 on Windows, macOS, and Linux x86 on Windows ARM32 on Windows and Linux ARM64 on Linux For Linux, ARM32 is supported on Debian 9+ and Ubuntu 16.04+. For ARM64, it is the same as ARM32 with the addition of Alpine 3.8. These are the same versions of those distros as is supported for X64. We made a conscious decision to make supported platforms as similar as possible between X64, ARM32 and ARM64. Docker images for .NET Core 3.0 are available at microsoft/dotnet on Docker Hub. We are in the process of adopting Microsoft Container Registry (MCR). We expect that the final .NET Core 3.0 images will only be published to MCR. Closing Take a look at the .NET Core 3.0 Preview 1 post if you missed that. It includes a broader description of the overall release including the initial set of features, which are also included and improved on in Preview 2. Thanks to everyone that installed .NET Core 3.0 Preview 1. We appreciate you trying out the new version and for your feedback. Please share any feedback you have about Preview 2. ",
        "C# pattern matching capabilities are starting to remind me of OCaml tutorials in the university. Sweet.",
        "<i>using var options = Parse(args);</i><p>That's going to tidy up a ton of my code, flatter is definitely better."
      ],
      "relevant": "false"
    },
    {
      "id": 19108787,
      "title": "Why are we templating YAML?",
      "search": [
        "Why are we templating YAML?",
        "https://leebriggs.co.uk/blog/2019/02/07/why-are-we-templating-yaml.html",
        "Published Feb 7, 2019 by Lee Briggs #kubernetes #configuration mgmt #jsonnet #helm #kr8 I was at cfgmgmtcamp 2019 in Ghent, and did a talk which I think was well received about the need for some Kubernetes configuration management as well as the solution we built for it at $work, kr8. I made a statement during the talk which ignited some fairly fierce discussion both online, and at the conference: \"If you're starting to template yaml, ask yourself the question: why am I not *generating* json?\" - @briggsl spitting straight fire at #cfgmgmtcamp eric sorenson (@ahpook) February 5, 2019 To put this into my own words: At some point, we decided it was okay for us to template yaml. When did this happen? How is this acceptable? After some conversation, I figured it was probably best to back up my claims in some way. This blog post is going to try to do that. The configuration problem Once the applications and infrastructure youre going to manage grows past a certain size, you inevitably end up in some form of configuration complexity hell. If youre only deploying 1 or maybe 2 things, you can write a yaml configuration file and be done with it. However once you grow beyond that, you need to figure out how to manage this complexity. Its incredibly likely that the reason you have multiple configuration files is because the $thing that uses that config is slightly different from its companions. Examples of this include: Applications deployed in different environments, like dev, stg and prod Applications deployed in different regions, like Europe or North American Obviously, not all the configuration is different here, but its likely the configuration differs enough that you want to be able to differentiate between the two. This configuration complexity has been well known for Operators (System Administrators, DevOps engineers, whatever you want to call them) for some years now. An entire discpline grew up around this in Configuration Management, and each tool solved this problem in their own way, but ultimately, they used YAML to get the job done. My favourite method has always been hiera which comes bundled with Puppet. Having the ability to hierarchically look up the variables of specific config needs is incredibly powerful and flexible, and has generally meant you dont actually need to do any templating of yaml at all, except perhaps for embedding Puppet facts into the yaml. Did we go backwards? Then, as our industries needs moved above the operating system and into cloud computing, we had a whole new data plane to configure. The tooling to configure this changed, and tools like CloudFormation and Helm appeared. These tools are excellent configuration tools, but I firmly believe we (as an industry) got something really, really wrong when we designed them. To examine that, lets take a look at example of a helm chart taking a custom parameter Helm Charts Helm charts can take external parameters defined by an values.yaml file which you specify when rendering the chart. A simple example might look like this: Lets say my external parameter is simple - its a string. Itd look a bit like this: image: \"{{ .Values.image }}\" Thats not so bad right? You just specify a value for image in your values.yaml and youre on your way. The real problem starts to get highlighted when you want to do more complicated and complex things. In this particular example, youre doing okay because you know you have to specify an image for a Kubernetes deployment. However, what if youre working with something like an optional field? Well, then it gets a little more unwieldy: {{- with .resourceGroup }} resourceGroup: {{ . }} {{- end }} Optional values just make things ugly in templating languages, and you cant just leave the value blank, so you have to resort to ugly loops and conditionals that are probably going to bite you later. Lets say you need to go a step further, and you need to push an array or map into the config. With helm, youd do something like this. {{- with .Values.podAnnotations }} annotations: {{ toYaml . | indent 8 }} {{- end }} Firstly, lets ignore the madness of having a templating function toYaml to convert yaml to yaml and focus more on the whitespace issue here. YAML has strict requirements and whitespace implementation rules. The following, for example, is not valid or complete yaml: something: nothing hello: goodbye Generally, if youre handwriting something, this isnt necessarily a problem because you just hit backspace twice and its fixed. However, if youre generating YAML using a templating system, you cant do that - and if youre operating above 5 or 10 configuration files, you probably want to be generating your config rather than writing it. So, in the above example, you want to embed the values of .Values.podAnnotations under the annotations field, which is indented already. So youre having to not only indent your values, but indent them correctly. What makes this even more confusing is that the go parser doesnt actually know anything about YAML at all, so if you try to keep the syntax clean and indent the templates like this: {{- with .Values.podAnnotations }} annotations: {{ toYaml . | indent 6 }} {{- end }} You actually cant do that, because the templating system gets confused. This is a singular example of the complexity and difficulty you end up facing when generating config data in YAML, but when you really start to do more complex work, it really starts to become obvious that this isnt the way to go. Needless to say, this isnt what I want to spend my time doing. If fiddling around with whitespace requirements in a templating system doing something its not really designed for is what suits you, then Im not going to stop you. I also dont want to spend my time writing configuration in JSON without comments and accidentally missing commas all over the shop. We (as an industry) decided a long time ago that shit wasnt going to work and thats why YAML exists. So what should we do instead? Thats where jsonnet comes in. JSON, Jsonnet & YAML Before we actually talk about Jsonnet, its worth reminding people of a very important (but oft forgotten point). YAML is a superset of JSON and converting between the two is trivial. Many applications and programming languages will parse JSON and YAML natively, and many can convert between the two very simple. For example, in Python: python -c 'import json, sys, yaml ; y=yaml.safe_load(sys.stdin.read()) ; print(json.dumps(y))' So with that in mind, lets talk about Jsonnet. Welcome to the church of Jsonnet Jsonnet is a relatively new, little known (outside the Kubernetes community?) language that calls itself a data templating language. Its definitely a good exercise to read and consume the Jsonnet design rationale page to get an idea why it exists, but if I was going to define in a nutshell what its purpose is - its to generate JSON config. So, how does it help, exactly? Well, lets take our earlier example - we want to generate some JSON config specifying a parameter (ie, the image string). We can do that very very easily with Jsonnet using external variables. Firstly, lets define some Jsonnet: { image: std.extVar('image'), } Then, we can generate it using the Jsonnet command line tool, passing in the external variable as we need to: jsonnet image.jsonnet -V image=\"my-image\" { \"image\": \"my-image\" } Easy! Optional fields Before, I noted that if you wanted to define an optional field, with YAML templating you had to define if statements for everything. With Jsonnet, youre just defining code! // define a variable - yes, jsonnet also has comments local rg = null; { image: std.extVar('image'), // if the variable is null, this will be blank [if rg != null then 'resourceGroup']: rg, } The output here, because our variable is null, means that we never actually populate resourceGroup. If you specify a value, it will appear: jsonnet image.jsonnet -V image=\"my-image\" { \"image\": \"my-image\" } Maps and parameters Okay, now lets look at our previous annotation example. We want to define some pod annotations, which takes a YAML map as its input. You want this map to be configurable by specifying external data, and obviously doing that on the command line sucks (youd be very unlikely to specify this with Helm on the command line, for example) so generally youd use Jsonnet imports to this. Im going to specify this config as a variable and then load that variable into the annotation: local annotations = { 'nginx.ingress.kubernetes.io/app-root': '/', 'nginx.ingress.kubernetes.io/enable-cors': true, }; { metadata: { // annotations are nested under the metadata of a pod annotations: annotations, }, } This might just be my bias towards Jsonnet talking, but this is so dramatically easier than faffing about with indentation that I cant even begin to describe it. Additional goodies The final thing I wanted to quickly explore, which is something that I feel cant really be done with Helm and other yaml templating tools, is the concept of manipulating existing objects in config. Lets take our example above with the annotations, and look at the result file: { \"metadata\": { \"annotations\": { \"nginx.ingress.kubernetes.io/app-root\": \"/\", \"nginx.ingress.kubernetes.io/enable-cors\": true } } } Now, lets say for example I wanted to append a set of annotations to this annotations map. In any templating system, Id probably have to rewrite the whole map. Jsonnet makes this trivial. I can simply use the + operator to add something to this. Heres a (poor) example: local annotations = { 'nginx.ingress.kubernetes.io/app-root': '/', 'nginx.ingress.kubernetes.io/enable-cors': true, }; { metadata: { annotations: annotations, }, } + { // this adds another JSON object metadata+: { // I'm using the + operator, so we'll append to the existing metadata annotations+: { // same as above something: 'nothing', }, }, } The end result is this: { \"metadata\": { \"annotations\": { \"nginx.ingress.kubernetes.io/app-root\": \"/\", \"nginx.ingress.kubernetes.io/enable-cors\": true, \"something\": \"nothing\" } } } Obviously, in this case, its more code to this, but as your example get more complex, it becomes extremely useful to be able to manipulate objects this way. Kr8 We use all of these methods in kr8 to make creating and manipulating configuration for multiple Kubernetes clusters easy and simple. I highly recommend you check it out if any of the concepts youve found here have found you nodding your head. ",
        "I know I'm in a minority, but I really dislike YAML... I recently did a lot of Ansible and boy, at the beginning, I was just struggling a lot. Syntactic whitespace kills me.<p>I don't like it in Python either, but for some reason, when I write Python, it's a lot easier. Maybe YAML is just a bit more complex (and Python has better IDE support..?)",
        "My belief is that we've been slowly building up to using general purpose languages, one small step at a time, throughout the infrastructure as code, DevOps, and SRE journeys this past 10 years. INI files, XML, JSON, and YAML aren't sufficiently expressive -- lacking for loops, conditionals, variable references, and any sort of abstraction -- so, of course, we add templates to it. But as the author (IMHO rightfully) points out, we just end up with a funky, poor approximation of a language.<p>I think this approach is a byproduct of thinking about infrastructure and configuration -- and the cloud generally -- as an \"afterthought,\" not a core part of an application's infrastructure. Containers, Kubernetes, serverless, and more hosted services all change this, and Chef, Puppet, and others laid the groundwork to think differently about what the future looks like. More developers today than ever before need to think about how to build and configure cloud software.<p>We started the Pulumi project to solve this very problem, so I'm admittedly biased, and I hope you forgive the plug -- I only mention it here because I think it contributes to the discussion. Our approach is to simply use general purpose languages like TypeScript, Python, and Go, while still having infrastructure as code. An important thing to realize is that infrastructure as code is based on the idea of a <i>goal state</i>. Using a full blown language to generate that goal state generally doesn't threaten the repeatability, determinism, or robustness of the solution, provided you've got an engine handling state management, diffing, resource CRUD, and so on. We've been able to apply this universally across AWS, Azure, GCP, <i>and</i> Kubernetes, often mixing their configuration in the same program.<p>Again, I'm biased and want to admit that, however if you're sick of YAML, it's definitely worth checking out. We'd love your feedback:<p>- Project website: <a href=\"https://pulumi.io/\" rel=\"nofollow\">https://pulumi.io/</a><p>- All open source on GitHub: <a href=\"https://github.com/pulumi/pulumi\" rel=\"nofollow\">https://github.com/pulumi/pulumi</a><p>- Example of abstractions: <a href=\"https://blog.pulumi.com/the-fastest-path-to-deploying-kubernetes-on-aws-with-eks-and-pulumi\" rel=\"nofollow\">https://blog.pulumi.com/the-fastest-path-to-deploying-kubern...</a><p>- Example of serverless as event handlers: <a href=\"https://blog.pulumi.com/lambdas-as-lambdas-the-magic-of-simple-serverless-functions\" rel=\"nofollow\">https://blog.pulumi.com/lambdas-as-lambdas-the-magic-of-simp...</a><p>Pulumi may not be <i>the</i> solution for everyone, but I'm fairly optimistic that this is where we're all heading.<p>Joe"
      ],
      "relevant": "false"
    },
    {
      "id": 19382838,
      "title": "Panic's Next Code Editor (Coda Successor)",
      "search": [
        "Panic's Next Code Editor (Coda Successor)",
        "https://panic.com/next/",
        "If we're being honest, Mac apps are a bit of a lost art. There are great reasons to make cross-platform apps to start, they're cross-platform but it's just not who we are. Founded as a Mac software company in 1997, our joy at Panic comes from building things that feel truly, well, Mac-like. Long ago, we created Coda, an all-in-one Mac web editor that broke new ground. But when we started work on Nova, we looked at where the web was today, and where we needed to be. It was time for a fresh start. A powerful editor. A themeable interface. Flexible workflows. Useful tools. Robust extensions. And lots of settings. The Editor. It all starts with our first-class text-editor. It's new, hyper-fast, and flexible, with all the features you want: smart autocomplete, multiple cursors, a Minimap, editor overscroll, tag pairs and brackets, and way, way more. Autocomplete with Fuzzy Matching Minimap Issues Multiple Cursors Git Status For the curious, Nova has built-in support for CoffeeScript, CSS, Diff, ERB, Haml, HTML, INI, JavaScript, JSON, JSX, Less, Lua, Markdown, Perl, PHP, Python, Ruby, Sass, SCSS, Smarty, SQL, TSX, TypeScript, XML, and YAML. It's also very expandable, with a robust API and a built-in extension browser. (Here's a little editor story for fun. During beta we found some bugs in Apple's text layout engine that we just could not fix. Our solution? Writing our own text layout manager from scratch. Not only did this fix the bugs, but it also boosted our editor's performance. We're not messing around!) But even the best text engine in the world means nothing unless you actually enjoy spending your time in the app. So, how does Nova look? The Interface. It's beautiful. And clean. And fun. You can make Nova look exactly the way you want, while still feeling Mac-like. Bright, dark, cyberpunk, it's all you. Plus, themes are CSS-like and easy to write. Nova can even automatically change your theme when your Mac switches from light to dark mode. The Workflows. Nova doesn't just help you code. It helps your code run. You can easily create build and run tasks for your projects. We didn't have them in Coda, but boy do we have them now. They're custom scripts that can be triggered at any time by toolbar buttons or keyboard shortcuts. Imagine building content, and with the single click of a button watching as Nova fires up your local server, grabs the appropriate URL, and opens a browser for you, instantly. Just think of the time you'll save. Nova supports separate Build, Run, and Clean tasks. It can open a report when run. And the scripts can be written in a variety of languages. The Tools. Now, this is important. Editing text is just part of what Nova does. We've bundled in extremely useful tools to help you get your work done quickly and efficiently. They're all fast and native too, of course. The New Tab button doesn't just open a fresh document. although it does that, too. Click it to quickly access a feature-packed Transmit file browser, or a super-convenient Prompt terminal, all right inside Nova. Meanwhile, Nova's sidebar is packed with power. The sidebar can also be split to show multiple tools at once, on the left and/or right side of your editor. And you can drag your favorite tools into the sidebar dock at the top for one-click access. Nova also has Git source control tools built-in. Clone. Click-to-clone. Initialize a repo. Fetch and pull. Stage and unstage. Commit. Push. You know the drill. (We don't have built-in diff yet, but it's on our list!) Git status is available both in the editor and the sidebar. And a useful \"Show Last Change for Line\" pop-up explains commits. The Extensions. Nova has a robust extensions API. A Nova extension can do lots of things, like add support for new languages, extend the sidebar, draw beautiful new themes and syntax colors, validate different code, and much more. Even better, extensions are written in JavaScript, so anyone can write them. And Nova includes built-in extension templates for fast development. Check out some of this weeks popular extensions Text Tools 3.0.5 biati Sort, Transform, Filter, Delete Duplicates, Encode, Decode, Expand and Shrink Se... CSS Validator 0.9.2 Panic Integrates the W3C CSS online valiator. Prettier 2.3.0 Alexander Weiss Code formatter using prettier Beautify 1.4.1 Patrick A. Vuarnoz Format Javascript, JSON, CSS, SCSS, LESS, HTML and XML using JS-Beautify. JSON 1.1.1 Cameron Little Advanced JSON support for Nova TypeScript 2.4.0 Cameron Little Advanced TypeScript and JavaScript language support for Nova Browse Extensions The Settings. People have strong editor opinions. And we're here to help. Nova has a whole host of settings. We have easily customizable key bindings. We have custom, quickly-switchable workspace layouts. And we have loads of editor tweaks, from matching brackets to overscroll. (And if there's something you need to work that Nova doesn't have, just let us know! Nova is always changing, always growing.) Click around to see Nova's preferences! And So Much More. Command Palette Project Launcher with Custom Artwork Multiple Sidebars & Sidebar Splits Separate Editor & Window Themes Automatic Theme Changes Global & Project Clips Project-Wide Indexing Intelligent, Extendable Autocomplete Powerful Open Quickly Git Source Control Sidebar Preview Tabs Built-in Static Web Server Remote Publishing via FTP, SFTP, WebDAV, & Clouds Local & Remote Terminals Markdown Preview Customizable In-App Key Bindings Panic Sync for Servers & Keys Robust Extension API In-App Extension Library nova Command Line Tool Reopen Recently Closed Files Small or Large Sidebar Dock Sizes Project-specific Sidebar Layouts Remote-Bound Workspaces Quick Tab Overview Customizable Event Behaviors Deep-Filtering Files Sidebar Ignored Files in Sidebar Drag-to-Split Easily Merged JSON Project Settings Files Sidebar Navigation Controls Single-Click to Open Files Find & Replace in Project Powerful Find & Replace Wildcards Find Scopes Ignore Specific Files when Indexing Remote Files Sidebar Server Preferences Staged Publishing List Multiple Publishing Destinations per Project Save & Publish Rich Editor Typography & Styling Customizable Line Height Text Glow Support in Themes Multiple Insertion Point Styles Type & Function Separators Customizable Editor Overscroll Source Control Change Annotations Automatic Link Detection Spell Checking Powerful Clip Wildcard Tokens Expandable Issue Line Annotations Hierarchical Symbols List Jump To Definition Dictionary Define Popover EditorConfig Support Customizable Markdown Stylesheets Rainbow Bracket Nesting Rainbow Indentation Guides Matching Tag Highlighting Identifier Highlighting Automatic Closing-Bracket Insertion Bracket Wrapping Automatic Tag Closing Customizable Wrap Indentation Quickly Add Cursors for Successive Lines Project Issues Sidebar Git Branch & Switch Git Commit, Fetch, Push, & Pull Image, Audio, & Video Media Viewers Extension Updating Without Restarting IDE Task Output Reports Remote Tasks on Unix, Linux, Windows, & PowerShell Custom Task Environment Variables Automatic Parsing of Task Output Into Issues Auditory and Visual Terminal Beeps Customizable Terminal Tab Titles Terminal Key-Binding Escape Sequences Terminal \"Option as Meta Key\" Terminal URL and file detection Terminal Mouse Events RSA, ECDSA, & ED25519 Keys Dual-Pane File Browser Tabs Cloud Provider Files Support Transfer Transcripts Robust Transfer Settings & Rules External Preview in Browser with Live Reload Non-Interruptive Updates Install Updates On Quit Coda 2 Import & Migration Assistant Transmit 5 Import No-Fuss Analytics & Privacy Settings And Now You Know Why This Took Us a Few Years For Extensions: Develop Extensions In-App Safe, Sandboxed Environment Robust JavaScript API Project & Global Settings Rapidly Reload and Test Live Filesystem, Network, & Subprocess Access Debug Console Editor & Project Actions Linters & Validators Custom Language Grammars Expressive Completion Providers Build & Run Task Templates Custom Sidebars Syntax Inspector Language Server Protocol Support Custom Themes Workspace Notifications Text Parsing & Encoding Utilities Secure Credential Storage Submit Extensions Easily with Validation ",
        "Why would you deliberately skew the text block like they did?  The page reads like a printout from a busted printer.<p>Definitely not the worst weird hipster web-design I've seen, but it just invokes an uncomfortable feeling of something being broken or off.  Not a good thing for a product announcement.",
        "the only way I see this having any chance is if they make it able to load textmate / vscode / sublime text extensions"
      ],
      "relevant": "false"
    },
    {
      "id": 21674729,
      "title": "HTTPie – A user-friendly CLI HTTP client",
      "search": [
        "HTTPie – A user-friendly CLI HTTP client",
        "https://github.com/jakubroztocil/httpie",
        "HTTPie (pronounced aitch-tee-tee-pie) is a command-line HTTP client. Its goal is to make CLI interaction with web services as human-friendly as possible. HTTPie is designed for testing, debugging, and generally interacting with APIs & HTTP servers. The http & https commands allow for creating and sending arbitrary HTTP requests. They use simple and natural syntax and provide formatted and colorized output. Getting started Installation instructions Full documentation Features Expressive and intuitive syntax Formatted and colorized terminal output Built-in JSON support Forms and file uploads HTTPS, proxies, and authentication Arbitrary request data Custom headers Persistent sessions wget-like downloads See all features Examples Hello World: Custom HTTP method, HTTP headers and JSON data: $ http PUT pie.dev/put X-API-Token:123 name=John Build and print a request without sending it using offline mode: $ http --offline pie.dev/post hello=offline Use GitHub API to post a comment on an Issue with authentication: $ http -a USERNAME POST https://api.github.com/repos/httpie/httpie/issues/83/comments body='HTTPie is awesome! :heart:' See more examples Community & support Visit the HTTPie website for full documentation and useful links. Join our Discord server is to ask questions, discuss features, and for general API chat. Tweet at @httpie on Twitter. Use StackOverflow to ask questions and include a httpie tag. Create GitHub Issues for bug reports and feature requests. Subscribe to the HTTPie newsletter for occasional updates. Contributing Have a look through existing Issues and Pull Requests that you could help with. If you'd like to request a feature or report a bug, please create a GitHub Issue using one of the templates provided. See contribution guide ",
        "Truly one of the best cli tools out there. The api is concise and flexible, the --help flag is easy to read. But most importantly, there's <i>sane</i> defaults.<p><i>defaults to GET request</i><p><pre><code>   http google.com\n</code></pre>\n<i>when json body exists it becomes a POST request</i><p><pre><code>   http google.com user=65</code></pre>",
        "HTTPie is amazing, but I grew tired of it being \"slow\", slower than some of my services response times at least.<p>Migrated to Curlie [1], `alias http=curlie`, and been happy with it since. Same API, better performance and access to full `curl` flags.<p>[1] <a href=\"https://curlie.io\" rel=\"nofollow\">https://curlie.io</a>"
      ],
      "relevant": "true"
    },
    {
      "id": 21454153,
      "title": "Stripe CLI",
      "search": [
        "Stripe CLI",
        "https://stripe.com/blog/stripe-cli",
        "Building and testing a Stripe integration can require frequent switching between the terminal, your code editor, and the Dashboard. Today, were excited to launch the Stripe command-line interface (CLI). It lets you interact with Stripe right from the terminal and makes it easier to build, test, and manage your integration.To start, the CLI will let you test webhooks, tail real-time API logs, and create or update API objects. Heres a preview of some of the features: Simplify webhook setup and testingStripe sends a variety ofwebhooks, which let you listen and respond to specific events programmatically. Runstripe listenwith the CLI to forward webhooks to your local web server during developmentno third-party tunneling tools required. You can also trigger and test specific webhook events withstripe trigger.Debug faster with real-time logsWhile integrating, it can be useful to look at logs to fix any issues. You can now usestripe logs tailto stream API request logs in real time in the terminal in addition to viewing these logs from the Dashboard. Quickly inspect parameters or JSON responses and debug errors as they happen.Speed up common tasks and workflowsYou can now create, retrieve, update, or delete any Stripe object directly from the CLI in both test and live mode. For example, you can usestripe customers createand specify parameters for properties inline.Since you can pipe results into other commands, this can be a simple and powerful way to automate tasks. Heres an example:The above command uses the CLI to list live subscriptions that are past due, pipes the JSON response tojqto extract the customer name and email, and exports the data in CSV format.To see a full list of supported commands, runstripe helpor visitthe docsto learn more.Getting startedThe Stripe CLI natively supportsmacOS, Windows, and Linux. You can also pull ourDocker imageto use in automated testing or a continuous integration setup.Were just getting started and well be adding a lot more features to the CLI. If you have feedback or feature requests, join the conversation onGitHub. You can also runstripe feedbackto share your ideas for which use cases we should tackle next. ",
        "Stripe is by far the best developer experience I’ve had in my career working with third party APIs/services. The attention to detail is just second to none, and documentation is a big part of this.",
        "In the Twilio world, I can do something like this, via the command line and their API:<p>- activate new phone number<p>- send SMS from that number<p>- deactivate number<p>I have not ever done <i>that particular workflow</i>, but I could.<p>Who is the Twilio of payments/fintech wherein I could perform a workflow like this:<p>- generate new CC number in my name<p>- set transaction limit(s) and expiry 10 days from now<p>- (go use that CC in real life)<p>- deactivate the CC<p>or maybe:<p>- disable existing card<p>- reenable existing card<p>I know I could do this if I wrote the API myself and had a big, complicated, sticky relationship with a bank ... but does someone let me do things like this if I am just an end-user (like Twilio does) ?"
      ],
      "relevant": "false"
    }
  ]
}
