{
  "docs": [
    {
      "id": 20559240,
      "title": "Learn a little jq, Awk and sed",
      "search": [
        "Learn a little jq, Awk and sed",
        "https://letterstoanewdeveloper.com/2019/07/29/learn-a-little-jq-awk-and-sed/",
        "Dear new developer, You are probably going to be dealing with text files sometime during your development career. These could be plain text, csv, or json. They may have data you want to get out, or log files you want to examine. You may be transforming from one format to another. Now, if this is a regular occurrence, you may want to build a script or a program around this problem (or use a third party service which aggregates everything together). But sometimes these files are one offs. Or you use them once in a blue moon. And it can take a little while to write a script, look at the libraries, and put it all together. Another alternative is to learn some of the unix tools available on the command line. Here are three that I consider table stakes. awk This is a multi purpose line processing utility. I often want to grab lines of a log file and figure out what is going on. Heres a few lines of a log file: 54.147.20.92 - - [26/Jul/2019:20:21:04 -0600] \"GET /wordpress HTTP/1.1\" 301 241 \"-\" \"Slackbot 1.0 (+https://api.slack.com/robots)\" 185.24.234.106 - - [26/Jul/2019:20:20:50 -0600] \"GET /wordpress/archives/date/2004/02 HTTP/1.1\" 200 87872 \"http://www.mooreds.com\" \"DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)\" 185.24.234.106 - - [26/Jul/2019:20:20:50 -0600] \"GET /wordpress/archives/date/2004/08 HTTP/1.1\" 200 81183 \"http://www.mooreds.com\" \"DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)\" If I want to see only the ip addresses (assuming these are all in a file called logs.txt), Id run something like: $ awk '{print $1}' logs.txt 54.147.20.92 185.24.234.106 185.24.234.106 Theres lots more, but you can see that youd be able to slice and dice delimited data pretty easily. Heres a great article which dives in further. sed This is another line utility. You can use it for all kinds of things, but I primarily use it to do search and replace on a file. Suppose you had the same log file, but you wanted to anonymize the the ip address and the user agent. Perhaps youre going to ship them off for long term storage or something. You can easily remove this with a couple of sed commands. $ sed 's/^[^ ]*//' logs.txt |sed 's/\"[^\"]*\"$//' - - [26/Jul/2019:20:21:04 -0600] \"GET /wordpress HTTP/1.1\" 301 241 \"-\" - - [26/Jul/2019:20:20:50 -0600] \"GET /wordpress/archives/date/2004/02 HTTP/1.1\" 200 87872 \"http://www.mooreds.com\" - - [26/Jul/2019:20:20:50 -0600] \"GET /wordpress/archives/date/2004/08 HTTP/1.1\" 200 81183 \"http://www.mooreds.com\" Yes, it looks like line noise, but this is the power of regular expressions. Theyre in every language (though with slight variations) and worth learning. sed gives you the power of regular expressions at the command line for processing files. I dont have a great sed tutorial Ive found, but googling shows a number. jq If you work on the command line with modern software at all, you have encountered json. Its used for configuration files and data transmission. Sometimes you get an array of json and you just want to pick out certain attributes of it. Tools like sed and awk fail at this, because they are used to newlines separating records, not curly braces and commas. Sure, you could use regular expressions to parse simple json, and there are times when Ive done this. But a far better tool is jq. Im not as savvy with this as with the others, but have used it whenever Im dealing with an API that delivers json (which is most modern ones). I can pull the API down with curl (another great tool) and parse it out with jq. I can put these all in a script and have the exploration be repeatable. I did this a few months ago when I was doing some exploration of an elastic search system. I crafted the queries with curl and then used jq to parse out the results so that I could make some sense of this. Yes, I could have done this with a real programming language, but it would have taken longer. I could also have used a gui tool like postman, but then it would not have been replicable. sed and awk should be on every system you run across; jq is non standard, but easy to install. Its worth spending some time getting to know these tools. So next time you are processing a text file and need to extract just a bit of it, reach for sed and awk. Next time you get a hairy json file and you are peering at it, look at jq. I think youll be happy with the result. Sincerely, Dan Published July 29, 2019October 17, 2020 ",
        "When I first saw someone using zsh (omz), I was awe-struck.<p>Same thing happens to the person sitting next to me when I pipe an output to jq.",
        "however, I find jq not so friendly piping its output to other programs."
      ],
      "relevant": "true"
    },
    {
      "id": 20811829,
      "title": "Curl exercises",
      "search": [
        "Curl exercises",
        "https://jvns.ca/blog/2019/08/27/curl-exercises/",
        "Recently Ive been interested in how people learn things. I was reading Kathy Sierras great book Badass: Making Users Awesome. It talks about the idea of deliberate practice. The idea is that you find a small micro-skill that can be learned in maybe 3 sessions of 45 minutes, and focus on learning that micro-skill. So, as an exercise, I was trying to think of a computer skill that I thought could be learned in 3 45-minute sessions. I thought that making HTTP requests with curl might be a skill like that, so here are some curl exercises as an experiment! whats curl? curl is a command line tool for making HTTP requests. I like it because its an easy way to test that servers or APIs are doing what I think, but its a little confusing at first! Heres a drawing explaining curls most important command line arguments (which is page 6 of my Bite Size Networking zine). You can click to make it bigger. fluency is valuable With any command line tool, I think having fluency is really helpful. Its really nice to be able to just type in the thing you need. For example recently I was testing out the Gumroad API and I was able to just type in: curl https://api.gumroad.com/v2/sales \\ -d \"access_token=<SECRET>\" \\ -X GET -d \"before=2016-09-03\" and get things working from the command line. 21 curl exercises These exercises are about understanding how to make different kinds of HTTP requests with curl. Theyre a little repetitive on purpose. They exercise basically everything I do with curl. To keep it simple, were going to make a lot of our requests to the same website: https://httpbin.org. httpbin is a service that accepts HTTP requests and then tells you what request you made. Request https://httpbin.org Request https://httpbin.org/anything. httpbin.org/anything will look at the request you made, parse it, and echo back to you what you requested. curls default is to make a GET request. Make a POST request to https://httpbin.org/anything Make a GET request to https://httpbin.org/anything, but this time add some query parameters (set value=panda). Request googles robots.txt file (www.google.com/robots.txt) Make a GET request to https://httpbin.org/anything and set the header User-Agent: elephant. Make a DELETE request to https://httpbin.org/anything Request https://httpbin.org/anything and also get the response headers Make a POST request to https://httpbin.org/anything with the JSON body {\"value\": \"panda\"} Make the same POST request as the previous exercise, but set the Content-Type header to application/json (because POST requests need to have a content type that matches their body). Look at the json field in the response to see the difference from the previous one. Make a GET request to https://httpbin.org/anything and set the header Accept-Encoding: gzip (what happens? why?) Put a bunch of a JSON in a file and then make a POST request to https://httpbin.org/anything with the JSON in that file as the body Make a request to https://httpbin.org/image and set the header Accept: image/png. Save the output to a PNG file and open the file in an image viewer. Try the same thing with different Accept: headers. Make a PUT request to https://httpbin.org/anything Request https://httpbin.org/image/jpeg, save it to a file, and open that file in your image editor. Request https://www.twitter.com. Youll get an empty response. Get curl to show you the response headers too, and try to figure out why the response was empty. Make any request to https://httpbin.org/anything and just set some nonsense headers (like panda: elephant) Request https://httpbin.org/status/404 and https://httpbin.org/status/200. Request them again and get curl to show the response headers. Request https://httpbin.org/anything and set a username and password (with -u username:password) Download the Twitter homepage (https://twitter.com) in Spanish by setting the Accept-Language: es-ES header. Make a request to the Stripe API with curl. (see https://stripe.com/docs/development for how, they give you a test API key). Try making exactly the same request to https://httpbin.org/anything. ",
        "I really like using HTTPie (<a href=\"https://httpie.org\" rel=\"nofollow\">https://httpie.org</a>). Much friendlier syntax then curl, formats the output...<p>Also works great with fx (<a href=\"https://github.com/antonmedv/fx\" rel=\"nofollow\">https://github.com/antonmedv/fx</a>). Just pipe the output from HTTPie and you get easy json processing.",
        "This is more SysAdmin related, but one power-curl function I use atleast 30 times a day is this alias I have in my .bash_aliases<p>This will output the HTTP status code for a given URL.<p><pre><code>     alias hstat=\"curl -o /dev/null --silent --head --write-out '%{http_code}\\n'\" $1  \n</code></pre>\nExample:<p><pre><code>  $ hstat google.com\n  301\n</code></pre>\nI also use curl as an 'uptime monitor' by adding onto that code section.  (a file with a list of URLs, and \"if http status code !=200\" then email me.)<p>There are variations on this all over the place but I really depend on it and I like it."
      ],
      "relevant": "true"
    },
    {
      "id": 21363121,
      "title": "An Illustrated Guide to Useful Command Line Tools",
      "search": [
        "An Illustrated Guide to Useful Command Line Tools",
        "https://www.wezm.net/technical/2019/10/useful-command-line-tools/",
        "Published on Sat, 26 October 2019 Inspired by a similar post by Ben Boyter this a list of useful command line tools that I use. Its not a list of every tool I use. These are tools that are new or typically not part of a standard POSIX command line environment. This post is a living document and will be updated over time. It should be obvious that I have a strong preference for fast tools without a large runtime dependency like Python or node.js. Most of these tools are portable to *BSD, Linux, macOS. Many also work on Windows. For OSes that ship up to date software many are available via the system package repository. Last updated: 31 Oct 2019 About my CLI environment: I use the zsh shell, Pragmata Pro font, and base16 default dark color scheme. My prompt is generated by promptline. Table of Contents Alacritty Terminal emulator alt Find alternate files bat cat with syntax highlighting bb System monitor chars Unicode character search dot Dot files manager dust Disk usage analyser eva Calculator exa Replacement for ls fd Replacement for find hexyl Hex viewer hyperfine Benchmarking tool jq awk/XPath for JSON mdcat Render Markdown in the terminal pass Password manager Podman Docker alternative Restic Encrypted backup tool ripgrep Fast, intelligent grep shotgun Take screenshots skim Fuzzy finder slop Graphical region selection Syncthing Decentralised file synchronisation tig TUI for git titlecase Convert text to title case Universal Ctags Maintained ctags fork watchexec Run commands in response to file system changes z Jump to directories zola Static site compiler Changelog The changelog for this page Alacritty Alacritty is fast terminal emulator. Whilst not strictly a command line tool, it does host everything I do in the command line. It is the terminal emulator in use in all the screenshots on this page. Homepage alt alt is a tool for finding the alternate to a file. E.g. the header for an implementation or the test for an implementation. I use it paired with Neovim to easily toggle between tests and implementation. $ alt app/models/page.rb spec/models/page_spec.rb Homepage bat bat is an alternative to the common (mis)use of cat to print a file to the terminal. It supports syntax highlighting and git integration. Homepage bb bb is system monitor like top. It shows overall CPU and memory usage as well as detailed information per process. Homepage chars chars shows information about Unicode characters matching a search term. Homepage dot dot is a dotfiles manager. It maintains a set of symlinks according to a mappings file. I use it to manage my dotfiles. Homepage dust dust is an alternative du -sh. It calculates the size of a directory tree, printing a summary of the largest items. Homepage exa exa is a replacement for ls with sensible defaults and added features like a tree view, git integration, and optional icons. I have ls aliased to exa in my shell. Homepage eva eva is a command line calculator similar to bc, with syntax highlighting and persistent history. Homepage fd fd is an alternative to find and has a more user friendly command line interface and respects ignore files, like .gitignore. The combination of its speed and ignore file support make it excellent for searching for files in git repositories. Homepage hexyl hexyl is a hex viewer that uses Unicode characters and colour to make the output more readable. Homepage hyperfine hyperfine command line benchmarking tool. It allows you to benchmark commands with warmup and statistical analysis. Homepage jq jq is kind of like awk for JSON. It lets you transform and extract information from JSON documents. Homepage mdcat mdcat renders Markdown files in the terminal. In supported terminals (not Alacritty) links are clickable (without the url being visible like in a web browser) and images are rendered. Homepage pass pass is a password manager that uses GPG to store the passwords. I use it with the passff Firefox extension and Pass for iOS on my phone. Homepage Podman podman is an alternative to Docker that does not require a daemon. Containers are run as the user running Podman so files written into the host dont end up owned by root. The CLI is largely compatible with the docker CLI. Homepage Restic restic is a backup tool that performs client side encryption, de-duplication and supports a variety of local and remote storage backends. Homepage ripgrep ripgrep (rg) recursively searches file trees for content in files matching a regular expression. Its extremely fast, and respects ignore files and binary files by default. Homepage shotgun shotgun is a tool for taking screenshots on X.org based environments. All the screenshots in this post were taken with it. It pairs well with slop. $ shotgun $(slop -c 0,0,0,0.75 -l -f \"-i %i -g %g\") eva.png Homepage skim skim is a fuzzy finder. It can be used to fuzzy match input fed to it. I use it with Neovim and zsh for fuzzy matching file names. Homepage slop slop (Select Operation) presents a UI to select a region of the screen or a window and prints the region to stdout. Works well with shotgun. $ slop -c 0,0,0,0.75 -l -f \"-i %i -g %g\" -i 8389044 -g 1464x1008+291+818 Homepage Syncthing Syncthing is a decentralised file synchronisation tool. Like Dropbox but self hosted and without the need for a central third-party file store. Homepage tig tig is a ncurses TUI for git. Its great for reviewing and staging changes, viewing history and diffs. Homepage titlecase titlecase is a little tool I wrote to format text using a title case format described by John Gruber. It correctly handles punctuation, and words like iPhone. I use it to obtain consistent titles on all my blog posts. $ echo 'an illustrated guide to useful command line tools' | titlecase An Illustrated Guide to Useful Command Line Tools I typically use it from within Neovim where selected text is piped through it in-place. This is done by creating a visual selection and then typing: :!titlecase. Homepage Universal Ctags Universal Ctags is a fork of exuberant ctags that is actively maintained. ctags is used to generate a tags file that vim and other tools can use to navigate to the definition of symbols in files. $ ctags --recurse src Homepage watchexec watchexec is a file and directory watcher that can run commands in response to file-system changes. Handy for auto running tests or restarting a development web server when source files change. # run command on file change $ watchexec -w content cobalt build # kill and restart server on file change $ watchexec -w src -s SIGINT -r 'cargo run' Homepage z z tracks your most used directories and allows you to jump to them with a partial name. Homepage zola zola is a full-featured very fast static site compiler. Homepage Changelog 31 Oct 2019 Add bb, and brief descriptions to the table of contents 28 Oct 2019 Add hyperfine Comments Comments on Lobsters Comments on Hacker News Previous Post: What I Learnt Building a Lobsters TUI in Rust ",
        "I quite like this list, there are a number of utilities here that I already use on a daily basis. There are also a few utilities that I like that weren't on this list, or some alternatives to what was shown. Some off the top of my head:<p>- nnn[0] (C) - A terminal file manager, similar to ranger. It allows you to navigate directories, manipulate files, analyze disk usage, and fuzzy open files.<p>- ncdu[1] (C) - ncurses disk analyzer. Similar to du -sh, but allows for directory navigation as well.<p>- z.lua[2] (Lua) - It's an alternative to the z utility mentioned in the article. I haven't done the benchmarks, but they claim to be faster than z. I'm mostly just including it because it's what I use.<p>[0] <a href=\"https://github.com/jarun/nnn\" rel=\"nofollow\">https://github.com/jarun/nnn</a>\n[1] <a href=\"https://dev.yorhel.nl/ncdu\" rel=\"nofollow\">https://dev.yorhel.nl/ncdu</a>\n[2] <a href=\"https://github.com/skywind3000/z.lua\" rel=\"nofollow\">https://github.com/skywind3000/z.lua</a>",
        "An observation: I have always taken the meaning of the word \"illustrated\" to specifically refer to non-lexical graphics. Searching the dictionary definition of the word, I find a looser definition that applies to \"examples\" intended to aid an explanation.<p>This is a similar cognitive dissonance to when I first learned that \"Visual Basic\" and \"Visual Studio\" meant that the syntax of the displayed code was highlighted, not graphically represented in a non-lexical way."
      ],
      "relevant": "true"
    },
    {
      "id": 21572308,
      "title": "Run an Internet Speed Test from the Command Line",
      "search": [
        "Run an Internet Speed Test from the Command Line",
        "https://www.putorius.net/speed-test-command-line.html",
        "We have all used tools like speedtest.net to test upload and download speeds. Whether it was to test the WiFi in that coffee shop (I use my own tether, never unknown hot spots), preparing for a LAN party (do people still do that?), or just a step in troubleshooting, we have all been there. For one reason or another you simply think you are being cheated of bandwidth, so you want independent verification of your speeds. This typically means opening a browser and going to a website to test your connection. But what if you want to run a speed test on a remote server? In this article we will discuss running an internet speed test from the Linux command line, and skipping the browser. There is something about the raw efficiency of the command line that I am really attracted to. As I discussed in the article 5 Command Line Tool to Break Your Dependence on the GUI, I try my best to stay away from the browser. It usually creates an unnecessary distraction. The internet is designed to grab your attention like a laser pointer does to a cat. So lets get started, and figure out one more way to stay away from the GUI. Different Speed Test Packages There are a few different tools you can use to run a speed test from the command line. To make things even more confusing the two most popular share the same exact name, but both use the speedtest.net service. Unofficial Speedtest-CLI Python Script The first one is an independently written Python script that is simple to install and use. It is available in the default repositories for some popular Linux distributions. Pros: Easy to installWide AvailabilityFull list of serversCan specify upload test, download test, or both Cons: Minimal output format optionsNo verbose output option Jump to Installing speedtest-cli Python Script or How to Use speedtest-cli Python Script. Official Ookla Speedtest CLI The second tool is built by Ookla, the people who bring you the speedtest.net website and service. Installing it requires you to add a repo for your package manager. But the maintainers offer simple instructions for installation. Pros: Official release from OoklaMore robust formatting optionsOutput easier to read, better layoutVerbose output availableHas repo making it easy to get updates Cons: Use limited to nearby serversCannot specify download or upload only Jump to Official Ookla Speedtest CLI The Speedtest-cli Python Script This is an easy way to get started running a speed test on the Linux command line. Installing the Speedtest-cli Python Script Simply use your package manager to install the package. Install on Fedora using DNF sudo dnf install speedtest-cli Ubuntu or Debian using APT sudo apt-get install speedtest-cli CentOS/Red Hat 7 / 8 Unfortunately, CentOS does not offer the rpm in their repos. It can still be easily installed. Change to /usr/bin directory to make command available to all users: cd /usr/bin Install dependencies: sudo yum install -y python wget Fetch script from github: sudo wget https://raw.githubusercontent.com/sivel/speedtest-cli/master/speedtest.py -O speedtest-cli Make script executable: sudo chmod +x speedtest-cli Or just copy and paste the whole thing below as a single line: cd /usr/bin; sudo yum install -y python wget && wget https://raw.githubusercontent.com/sivel/speedtest-cli/master/speedtest.py -O speedtest-cli && sudo chmod +x speedtest-cli How to Use the Python Script to Run a Speed Test The most basic usage is to simply run the command. It will automatically select the best server based on ping responses. Speedtest-cli Python Script Options There are several options available to change the default behavior. Here we will outline the most popular options. List Available Speed Test Servers You can use the list option to find a list of available servers to run your test against. At the time of writing this list is pretty extensive with 8829 possible servers. NOTE: The servers are sorted by distance, closest first. [[emailprotected] ~]$ speedtest-cli --list Retrieving speedtest.net configuration 4847) Hotwire Fision (Philadelphia, PA, United States) [10.92 km] 10979) School District of Philadelphia (Philadelphia, PA, United States) [10.92 km] ...OUTPUT TRUNCATED... Specify Specific Server to Test Against Once you have found the server you want to test against, you can use the server <SERVER ID> to select it. The server ID is the first column in the output of the list option above. [[emailprotected] ~]$ speedtest-cli --server 4847 Retrieving speedtest.net configuration Testing from Verizon Fios (x.x.x.x) Retrieving speedtest.net server list Selecting best server based on ping Hosted by School District of Philadelphia (Philadelphia, PA) [10.92 km]: 25.033 ms Testing download speed.. Download: 384.07 Mbit/s Testing upload speed Upload: 417.93 Mbit/s Only Test Upload or Download Speeds The option is actually designed to exclude a test. But since there are only two options it is effectively the same as selecting only one. To run only the download test, you exclude the upload, and vice versa. [[emailprotected] ~]$ speedtest-cli --no-upload Retrieving speedtest.net configuration Testing from Verizon Fios (x.x.x.x) Retrieving speedtest.net server list Selecting best server based on ping Hosted by KamaTera INC (New Jersey, NJ) [62.36 km]: 19.785 ms Testing download speed.. Download: 600.89 Mbit/s Skipping upload test Format Output in JSON or CSV You can specify the output format in JSON or CSV. You also have the opton to use CSV with a custom delimiter. This is handy if you are going to use the output in some other script or application. [[emailprotected] ~]$ speedtest-cli --json {\"download\": 597726146.0529929, \"upload\": 562476134.8046777, \"ping\": 17.004, \"server\": {\"url\": \"http://speedtest.us-ny2.kamatera.com:8080/speedtest/upload.php\", \"lat\": \"40.0583\", \"lon\": \"-74.4057\", \"name\": \"New Jersey, NJ\", \"country\": \"United States\", \"cc\": \"US\", \"sponsor\": \"KamaTera INC\", \"id\": \"11612\" ...OUTPUT TRUNCATED... Using CSV with a custom delimiter. The default delimiter is a comma, which is implied by the name CSV. Here we use the csv-delimiter option to change the delimiter to a pipe character. [[emailprotected] ~]$ speedtest-cli --csv --csv-delimiter \"|\" 11612|KamaTera INC|New Jersey, NJ|2019-11-17T14:51:53.636981Z|62.35865439150934|8.546|588013638.8767571|512001168.48230773||x.x.x.x The Official Ookla Speedtest CLI The official Speedtest CLI (Command Line Interface) from Ookla is a little more robust. It has all of the options of the python script and more. There are also several output formats not available with the unofficial python script. Ooklas speedtest is also a little easier on the eyes. It spreads the information out which makes it easier to read and displays a neat little progress bar. A URL you can use to share the results is also displayed by default. Installing the Official Speedtest CLI Install Speedtest CLI on Ubuntu / Debian: The Speedtest CLI from Ookla is supported on Ubuntu (xenial & bionic) and Debian (jessie, stretch, buster). $ sudo apt-get install gnupg1 apt-transport-https dirmngr $ export INSTALL_KEY=379CE192D401AB61 $ export DEB_DISTRO=$(lsb_release -sc) $ sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys $INSTALL_KEY $ echo \"deb https://ookla.bintray.com/debian ${DEB_DISTRO} main\" | sudo tee /etc/apt/sources.list.d/speedtest.list $ sudo apt-get update $ sudo apt-get install speedtest Install Speedtest CLI on Fedora / Redhat / CentOS: Fedora has moved on to DNF for package management, but is still compatible with YUM. These instructions were tested on Fedora 31, CentOS 7 and Red Hat 8. $ sudo yum install wget $ wget https://bintray.com/ookla/rhel/rpm -O bintray-ookla-rhel.repo $ sudo mv bintray-ookla-rhel.repo /etc/yum.repos.d/ $ sudo yum install speedtest How to Use the Official Speedtest CLI Once installed you can simply call the utility by typing speedtest at the command line. This will give you all the default information that you would see on the web version of speedtest.net. Official Speedtest CLI Options The options available in the official release are more robust. Here we will outline the popular options and how to use them. List Available Speed Test Servers Using the -L (servers) option will give you a list of servers available to run a test against. This option will only show you servers that are nearby. What exactly determines nearby is undefined. But for me it looks like they are staying in the tri-state area (PA, NJ, DE). [[emailprotected] ~]$ speedtest -L Closest servers: ID Name Location Country 4847 Hotwire Fision Philadelphia, PA United States 10979 School District of Philadelphia Philadelphia, PA United States 9840 Comcast New Castle, DE United States 11612 KamaTera INC New Jersey, NJ United States ...OUTPUT TRUNCATED... Optionally, you can use the -o (host) option and specify the FQDN of the server instead of the ID. But oddly, I dont see a way to get the FQDN of the servers on the list. I am guessing this option is available for using a custom server. I havent found a way to list all servers. If you are looking to test against a server on the other side of the country, you will have to find it another way. Select Specific Server to Run Speed Test Against You can use the -s (server-id) option to select a server to use from the list. You must supply the server ID with this option. The server ID is the number in the first column of the list output above. [[emailprotected] ~]$ speedtest -s 4847 Speedtest by Ookla Server: Hotwire Fision - Philadelphia, PA (id = 4847) Change Unit Used for Speed Output The -u (unit) option can display the speed output in many different formats. Decimal prefix, bits per second: bps, kbps, Mbps, Gbps Decimal prefix, bytes per second: B/s, kB/s, MB/s, GB/s Binary prefix, bits per second: kibps, Mibps, Gibps Binary prefix, bytes per second: kiB/s, MiB/s, GiB/s [[emailprotected] ~]$ speedtest -u MiB/s Speedtest by Ookla Server: Hotwire Fision - Philadelphia, PA (id = 4847) ISP: Verizon Fios Latency: 10.67 ms ( 0.95 ms jitter) Download: 63.45 MiB/s (data used: 700.2 MiB) ...OUTPUT TRUNCATED... Output Formatting Options The Ookla Speedtest CLI offers decent options for output formats. Human Readable DefaultCSV Comma Separated ValueTSV Tab Separated ValueJSON JavaScript Object NotationJSONL JSON LinesJSON-PRETTY JSON Pretty Printed Here is an example using json-pretty. [[emailprotected] ~]$ speedtest -f json-pretty { \"type\": \"result\", \"timestamp\": \"2019-11-17T16:42:06Z\", \"ping\": { \"jitter\": 0.29899999999999999, \"latency\": 17.474 }, \"download\": { \"bandwidth\": 92184614, \"bytes\": 491967724, \"elapsed\": 5303 }, \"upload\": { \"bandwidth\": 45010100, \"bytes\": 313859035, \"elapsed\": 6714 }, ...OUTPUT TRUNCATED... Conclusion Running a speed test from the command line may not be something that is needed on a daily basis for most people. However, it may prove useful in some troubleshooting situations. In this article we cover how to run a speed test from the command line using two similar tools. The unofficial python script and the official Ookla Speedtest CLI. We discussed installing, using and setting options for each one. This should be enough to get you started. For more information on these tools, visit their respective home pages found in the resources section below. Resources and Links: Ookla Speedtest CLISpeedtest-cli (Python Version) on GitHub ",
        "Note that some ISPs prioritise traffic to known speedtest targets, turning off traffic shaping rules that might otherwise slow bulk transfers. When this happens it means you are testing the likely maximum throughput of your connection not necessarily the throughput you will see more generally.<p>This is why Netflix started fast.com - because it draws data from the same distribution points as their video streaming apps it means you can't prioritise the speedtest without also doing so for the video traffic or (more likely) you can't de-prioritise the video traffic without also getting bad scores in that particular speedtest. From Netflix's point of view it is an answer to people contacting support with \"my speedtest results are fine, the problem must be your servers\" when they are experiencing video lag/drops and other such problems and the issue is due to ISP traffic shaping or the ISP simply not having enough backhaul bandwidth.<p>A more reliable test might be taking part in a busy public torrent: that way you are testing against arbitrary locations so your ISP can't be setting different shaping rules for them. Just remember to throttle upstream when testing downstream and vice-versa or saturation in the other direction will slow control packets that will in turn give you lower results for the one you are testing. This may fall into another trap though: unless you limit the number of active streams it may be an unrealistic test as more generally most processes use a small number of streams (or just a single one), and if you limit the number of streams too much you might get a lower result because each swarm member you connect to may be fairly saturated and sharing its bandwidth amongst many connections.",
        "speedtest-cli is <i>garbage</i> if you have >100Mbps Speeds. The dev refuses to acknowledge this: <a href=\"https://github.com/sivel/speedtest-cli/issues/226\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/226</a><p>Also not just me:\n<a href=\"https://github.com/sivel/speedtest-cli/issues/649\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/649</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/648\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/648</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/641\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/641</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/616\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/616</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/601\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/601</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/588\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/588</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/546\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/546</a>"
      ],
      "relevant": "true"
    },
    {
      "id": 21858952,
      "title": "JSON on the Command Line with Jq",
      "search": [
        "JSON on the Command Line with Jq",
        "https://shapeshed.com/jq-json/",
        "HomePostsContactLast updated Saturday, Nov 16, 2019A series of how to examples on using jq, a command-line JSON processorEstimated reading time: 4 minutesTable of contentsHow to pretty print JSONHow to use pipes with jqHow to find a key and valueHow to find items in an arrayHow to combine filtersHow to transform JSONHow to transform data values within JSONHow to remove keys from JSONHow to map valuesHow to wrangle JSON how you wantFurther readingjq is a fantastic command-line JSON processor. It plays nice with UNIX pipes and offers extensive functionality for interrogating, manipulating and working with JSON file.How to pretty print JSONjq can do a lot but probably the highest frequency use for most users is to pretty print JSON either from a file or after a network call. Suppose that we have a file names.json containing the following json.[{\"id\": 1, \"name\": \"Arthur\", \"age\": \"21\"},{\"id\": 2, \"name\": \"Richard\", \"age\": \"32\"}] jq can pretty print this file using the . filter. This takes the entire input and sends it to standard output. Unless told not to jq will pretty print making JSON readable.jq '.' names.json [ { \"id\": 1, \"name\": \"Arthur\", \"age\": \"21\" }, { \"id\": 2, \"name\": \"Richard\", \"age\": \"32\" } ] How to use pipes with jqBecause jq is UNIX friendly it is possible to pipe data in and out of it. This can be useful for using jq as a filter or interacting with other tools. In the following pipeline cat pipes the file into jq and this is piped onto less. This can be very useful for viewing large JSON files.cat names.json | jq '.' | less How to find a key and valueTo find a key and value jq can filter based on keys and return the value. Suppose we have the following simple JSON document saved as dog.json.{ \"name\": \"Buster\", \"breed\": \"Golden Retriever\", \"age\": \"4\", \"owner\": { \"name\": \"Sally\" }, \"likes\": [ \"bones\", \"balls\", \"dog biscuits\" ] } jq can retrieve values from this document by passing key names.jq '.name' \"Buster\" Multiple keys can by passed separated by commas.jq '.breed,.age' \"Golden Retriever\" \"4\" To search for nested objects chain values using the dot operator just as you would in JavaScript.jq '.owner.name' \"Sally\" How to find items in an arrayTo search for items in arrays use bracket syntax with the index starting at 0.jq '.likes[0]' \"bones\" Multiple elements of an array may also be returned.echo '[\"a\",\"b\",\"c\",\"d\",\"e\"]' | jq '.[2:4]' [ \"c\", \"d\" ] How to combine filtersjq can combine filters to search within a selection. For the following JSON document suppose that the names need to be filtered.[ { \"id\": 1, \"name\": \"Arthur\", \"age\": \"21\" }, { \"id\": 2, \"name\": \"Richard\", \"age\": \"32\" } ] This can be achieved with a pipe with the jq filter.jq '.[] | .name' names.json \"Arthur\" \"Richard\" How to transform JSONjq can be used for more than just reading values from a JSON object. It can also transform JSON into new data structures. Returning to the dog.json example earlier a new array can be created containing the name and likes as follows.jq '[.name, .likes[]]' dog.json [ \"Buster\", \"Bones\", \"balls\", \"dog biscuits\" ] This can be very useful in data transform pipelines to shift JSON data from one structure to another.How to transform data values within JSONjq can also operate on data within JSON objects. Suppose the following JSON file exists and is saved as inventory.json.{ \"eggs\": 2, \"cheese\": 1, \"milk\": 1 } jq can be used to perform basic arithmetic on number values.jq '.eggs + 1' inventory.json 3 How to remove keys from JSONjq can remove keys from JSON objects. This outputs the JSON object without the deleted key. Suppose the following JSON is saved as cheeses.json.{ \"maroilles\": \"stinky\", \"goat\": \"mild\" } jq can remove keys as follows leaving just wonderful stinky cheese.jq 'del(.goat)' cheeses.json { \"maroilles\": \"stinky\" } How to map valuesjq can map values and perform an operation on each one. In the following example each item in an array is mapped and has two subtracted.echo '[12,14,15]' | jq 'map(.-2)' [ 10, 12, 13 ] How to wrangle JSON how you wantjq has many more advanced features to help manipulating and wrangling JSON however you want to. For more run man jq.Further readingjq project pagejq manualjq is sed for JSONbash that JSON (jq)Parsing JSON with jqHave an update or suggestion for this article? You can edit it here and send me a pull request.TagsRecent Posts ",
        "Thread from 2016: <a href=\"https://news.ycombinator.com/item?id=13090604\" rel=\"nofollow\">https://news.ycombinator.com/item?id=13090604</a><p>2015: <a href=\"https://news.ycombinator.com/item?id=9446980\" rel=\"nofollow\">https://news.ycombinator.com/item?id=9446980</a><p>2014: <a href=\"https://news.ycombinator.com/item?id=7895076\" rel=\"nofollow\">https://news.ycombinator.com/item?id=7895076</a><p>2013: <a href=\"https://news.ycombinator.com/item?id=5734683\" rel=\"nofollow\">https://news.ycombinator.com/item?id=5734683</a><p>2012: <a href=\"https://news.ycombinator.com/item?id=4985250\" rel=\"nofollow\">https://news.ycombinator.com/item?id=4985250</a><p><a href=\"https://news.ycombinator.com/item?id=4679933\" rel=\"nofollow\">https://news.ycombinator.com/item?id=4679933</a><p>(for the curious)",
        "Every time I try to do something involved with jq (i.e. more than a couple commands deep) and inevitably end up wandering through the manual, I get a persistent feeling that jq could be cleaned up quite a bit. That it either has structure throughout that I fail to grasp, or that it's really rather disorganized and convoluted in places. This is exacerbated by me never being able to find anything in the manual without skimming half of it each single time.<p>E.g.:<p>Filtering and transforming objects by reaching a few levels deeper in them seems to be way easier to slap together in Python's list comprehensions or some functional notation. Stuff like [{ * * x, qwe: x.qwe*2} for x in a.b if x.y.z == 1].<p>Add nested lists to the above, and I can spend another fifteen minutes writing a one-liner (i.e. `a` is a list of objects with lists in some field).<p>I once had to deal with a conditional structure where a field might be one object or a list of them. Hoo boy.<p>Every time I want to check the number of entries my filters print out, I need to wrap them in an array. Repeat this a couple dozen times during writing a complex query. Weirdly, this is where strict structure gets in the way―and while I understand the reasoning, I feel that something could be done for such basic need. (Like, aggregate filters should be able to aggregate instead of acting on individual items? Maybe it's already in the language, but I'm not in the mood to go over the manual again.)<p>Variables seem to be bolted on as an afterthought, or at least the syntax doesn't exactly accommodate them. Meanwhile, they're necessary to implement some filters omitted in the language. Compare that to Lisp's simple `(let)`. IIRC the manual also says that jq has some semblance of functions, but I'm afraid to think about them with this syntax.<p>I like the idea a lot, but execution not so much. Frankly I'll probably end up throwing together a script in Lumo or something, that will accept Lisp expressions and feed JSON structure to them. (I'd use Fennel, but JSON has actual null while Lua... doesn't.)<p>Btw, I have pretty much the same sentiment about Git. Git structures, great. Git tools, oy vey. Maybe I need Lisp for Git, or at least Python for Git."
      ],
      "relevant": "true"
    },
    {
      "id": 20062064,
      "title": "Semantic: Parsing, analyzing, and comparing source code across many languages",
      "search": [
        "Semantic: Parsing, analyzing, and comparing source code across many languages",
        "https://github.com/github/semantic",
        "semantic is a Haskell library and command line tool for parsing, analyzing, and comparing source code. In a hurry? Check out our documentation of example uses for the semantic command line tool. Table of Contents Usage Language support Development Technology and architecture Licensing Usage Run semantic --help for complete list of up-to-date options. Parse Usage: semantic parse [--sexpression | (--json-symbols|--symbols) | --proto-symbols | --show | --quiet] [FILES...] Generate parse trees for path(s) Available options: --sexpression Output s-expression parse trees (default) --json-symbols,--symbols Output JSON symbol list --proto-symbols Output protobufs symbol list --show Output using the Show instance (debug only, format subject to change without notice) --quiet Don't produce output, but show timing stats -h,--help Show this help text Language support Language Parse AST Symbols Stack graphs Ruby JavaScript TypeScript Python Go PHP Java JSON JSX TSX CodeQL Haskell Used for code navigation on github.com. Supported Partial support Under development - N/A Development semantic requires at least GHC 8.10.1 and Cabal 3.0. We strongly recommend using ghcup to sandbox GHC versions, as GHC packages installed through your OS's package manager may not install statically-linked versions of the GHC boot libraries. semantic currently builds only on Unix systems; users of other operating systems may wish to use the Docker images. We use cabal's Nix-style local builds for development. To get started quickly: git clone git@github.com:github/semantic.git cd semantic script/bootstrap cabal v2-build all cabal v2-run semantic:test cabal v2-run semantic:semantic -- --help You can also use the Bazel build system for development. To learn more about Bazel and why it might give you a better development experience, check the build documentation. git clone git@github.com:github/semantic.git cd semantic script/bootstrap-bazel bazel build //... stack as a build tool is not officially supported; there is unofficial stack.yaml support available, though we cannot make guarantees as to its stability. Technology and architecture Architecturally, semantic: Generates per-language Haskell syntax types based on tree-sitter grammar definitions. Reads blobs from a filesystem or provided via a protocol buffer request. Returns blobs or performs analysis. Renders output in one of many supported formats. Throughout its lifestyle, semantic has leveraged a number of interesting algorithms and techniques, including: Myers' algorithm (SES) as described in the paper An O(ND) Difference Algorithm and Its Variations RWS as described in the paper RWS-Diff: Flexible and Efficient Change Detection in Hierarchical Data. Open unions and data types la carte. An implementation of Abstracting Definitional Interpreters extended to work with an la carte representation of syntax terms. Contributions Contributions are welcome! Please see our contribution guidelines and our code of conduct for details on how to participate in our community. Licensing Semantic is licensed under the MIT license. ",
        "Looks very interesting - would benefit from showing some examples and/or use cases",
        "The late 20th century, early 00's version:<p><a href=\"http://www.program-transformation.org/Transform/CodeCrawler\" rel=\"nofollow\">http://www.program-transformation.org/Transform/CodeCrawler</a><p>(And MOOSE)"
      ],
      "relevant": "false"
    },
    {
      "id": 21912838,
      "title": "Zwitterion: a web dev server that lets you import anything",
      "search": [
        "Zwitterion: a web dev server that lets you import anything",
        "https://github.com/lastmjs/zwitterion",
        "Zwitterion A web dev server that lets you import anything* * If by anything you mean: JavaScript ES2015+, TypeScript, JSON, JSX, TSX, AssemblyScript, Rust, C, C++, WebAssembly, and in the future anything that compiles to JavaScript or WebAssembly. Zwitterion is designed to be an instant replacement for your current web development static file server. Production deployments are also possible through the static build. For example, you can write stuff like the following and it just works: ./index.html: <!DOCTYPE html> <html> <head> <script type=\"module\" src=\"app.ts\"></script> </head> <body> This is the simplest developer experience I've ever had! </body> </html> ./app.ts: import { getHelloWorld } from './hello-world.ts'; const helloWorld: string = getHelloWorld(); console.log(helloWorld); ./hello-world.ts: export function getHelloWorld(): string { return 'Why hello there world!'; } Really, it just works. Zwitterion lets you get back to the good old days of web development. Just write your source code in any supported language and run it in the browser. Also...Zwitterion is NOT a bundler. It eschews bundling for a simpler experience. Current Features ES2015+ TypeScript JSON JSX TSX AssemblyScript Rust (basic support) C (basic support) C++ (basic support) WebAssembly Text Format (Wat) WebAssembly (Wasm) Bare imports (import * as stuff from 'library'; instead of import * as stuff from '../node_modules/library/index.js';) Single Page Application routing (by default the server returns index.html on unhandled routes) Static build for production deployment Upcoming Features More robust Rust integration (i.e. automatic local Rust installation during npm installation) More robust C integration More robust C++ integration Import maps HTTP2 optimizations Documentation Examples Installation and Basic Use Production Use Languages JavaScript TypeScript JSON JSX TSX AssemblyScript Rust C C++ WebAssembly Text Format (Wat) WebAssembly (Wasm) Command Line Options Special Considerations Under the Hood Installation and Basic Use Local Installation and Use Install Zwitterion in the directory that you would like to serve files from: Run Zwitterion by accessing its executable directly from the terminal: node_modules/.bin/zwitterion or from an npm script: { ... \"scripts\": { \"start\": \"zwitterion\" } ... } Global Installation and Use Install Zwitterion globally to use across projects: npm install -g zwitterion Run Zwitterion from the terminal: or from an npm script: { ... \"scripts\": { \"start\": \"zwitterion\" } ... } Production Use It is recommended to use Zwitterion in production by creating a static build of your project. A static build essentially runs all relevant files through Zwitterion, and copies those and all other files in your project to a dist directory. You can take this directory and upload it to a Content Delivery Network (CDN), or another static file hosting service. You may also use a running Zwitterion server in production, but for performance and potential security reasons it is not recommended. To create a static build, run Zwitterion with the --build-static option. You will probably need to add the application/javascript MIME type to your hosting provider for your TypeScript, AssemblyScript, Rust, Wasm, and Wat files. From the terminal: zwitterion --build-static From an npm script: { ... \"scripts\": { \"build-static\": \"zwitterion --build-static\" } ... } The static build will be located in a directory called dist, in the same directory that you ran the --build-static command from. Languages JavaScript JavaScript is the language of the web. You can learn more here. Importing JavaScript ES2015+ is straightforward and works as expected. Simply use import and export statements without any modifications. It is recommended to use explicit file extensions: ./app.js: import { helloWorld } from './hello-world.js'; console.log(helloWorld()); ./hello-world.js: export function helloWorld() { return 'Hello world!'; } JavaScript transpilation is done by the TypeScript compiler. By default, the TypeScript compiler's compilerOptions are set to the following: { \"module\": \"ES2015\", \"target\": \"ES2015\" } You can override these options by creating a .json file with your own compilerOptions and telling Zwitterion where to locate it with the --tsc-options-file command line option. The available options can be found here. Options are specified as a JSON object. For example: tsc-options.json: Tell Zwitterion where to locate it: zwitterion --tsc-options-file tsc-options.json TypeScript TypeScript is a typed superset of JavaScript. You can learn more here. Importing TypeScript is straightforward and works as expected. Simply use import and export statements without any modifications. It is recommended to use explicit file extensions: ./app.ts: import { helloWorld } from './hello-world.ts'; console.log(helloWorld()); ./hello-world.ts: export function helloWorld(): string { return 'Hello world!'; } By default, the TypeScript compiler's compilerOptions are set to the following: { \"module\": \"ES2015\", \"target\": \"ES2015\" } You can override these options by creating a .json file with your own compilerOptions and telling Zwitterion where to locate it with the --tsc-options-file command line option. The available options can be found here. Options are specified as a JSON object. For example: tsc-options.json: Tell Zwitterion where to locate it: zwitterion --tsc-options-file tsc-options.json JSON JSON is provided as a default export. It is recommended to use explicit file extensions: ./app.js: import helloWorld from './hello-world.json'; console.log(helloWorld); ./hello-world.json: JSX Importing JSX is straightforward and works as expected. Simply use import and export statements without any modifications. It is recommended to use explicit file extensions: ./app.js: import { helloWorldElement } from './hello-world.jsx'; ReactDOM.render( helloWorldElement, document.getElementById('root') ); ./hello-world.jsx: export const hellowWorldElement = <h1>Hello, world!</h1>; JSX transpilation is done by the TypeScript compiler. By default, the TypeScript compiler's compilerOptions are set to the following: { \"module\": \"ES2015\", \"target\": \"ES2015\" } You can override these options by creating a .json file with your own compilerOptions and telling Zwitterion where to locate it with the --tsc-options-file command line option. The available options can be found here. Options are specified as a JSON object. For example: tsc-options.json: Tell Zwitterion where to locate it: zwitterion --tsc-options-file tsc-options.json TSX Importing TSX is straightforward and works as expected. Simply use import and export statements without any modifications. It is recommended to use explicit file extensions: ./app.js: import { helloWorldElement } from './hello-world.tsx'; ReactDOM.render( helloWorldElement, document.getElementById('root') ); ./hello-world.tsx: const helloWorld: string = 'Hello, world!'; export const hellowWorldElement = <h1>{ helloWorld }</h1>; TSX transpilation is done by the TypeScript compiler. By default, the TypeScript compiler's compilerOptions are set to the following: { \"module\": \"ES2015\", \"target\": \"ES2015\" } You can override these options by creating a .json file with your own compilerOptions and telling Zwitterion where to locate it with the --tsc-options-file command line option. The available options can be found here. Options are specified as a JSON object. For example: tsc-options.json: Tell Zwitterion where to locate it: zwitterion --tsc-options-file tsc-options.json AssemblyScript AssemblyScript is a new language that compiles a strict subset of TypeScript to WebAssembly. You can learn more about it in The AssemblyScript Book. Zwitterion assumes that AssemblyScript files have the .as file extension. This is a Zwitterion-specific extension choice, as the AssemblyScript project has not yet chosen its own official file extension. You can follow that discussion here. Zwitterion will follow the official extension choice once it is made. Importing AssemblyScript is nearly identical to importing JavaScript or TypeScript. The key difference is that the default export of your entry AssemblyScript module is a function that returns a promise. This function takes as its one parameter an object containing imports to the AssemblyScript module. Passing values to and from functions exported from AssemblyScript modules should be straightforward, but there are some limitations. Zwitterion uses as-bind under the hood to broker values to and from AssemblyScript modules. Look there if you need more information. You can import AssemblyScript from JavaScript or TypeScript files like this: ./app.js: import addModuleInit from './add.as'; runAssemblyScript(); async function runAssemblyScript() { const adddModule = await addModuleInit(); console.log(addModule.add(1, 1)); } ./add.as: export function add(x: i32, y: i32): i32 { return x + y; } If you want to pass in imports from outside of the AssemblyScript environment, you create a file with export declarations defining the types of the imports. You then pass your imports in as an object to the AssemblyScript module init function. The name of the property that defines your imports for a module must be the exact filename of the file exporting the import declarations. For example: ./app.js: import addModuleInit from './add.as'; runAssemblyScript(); async function runAssemblyScript() { const adddModule = await addModuleInit({ 'env.as': { log: console.log } }); console.log(addModule.add(1, 1)); } ./env.as: export declare function log(x: number): void; ./add.as: import { log } from './env.as'; export function add(x: i32, y: i32): i32 { log(x + y); return x + y; } You can also import AssemblyScript from within AssemblyScript files, like so: ./add.as: import { subtract } from './subtract.as'; export function add(x: i32, y: i32): i32 { return subtract(x + y, 0); } ./subtract.as: export function subtract(x: i32, y: i32): i32 { return x - y; } By default, no compiler options have been set. The available options can be found here. You can add options by creating a .json file with an array of option names and values, and telling Zwitterion where to locate it with the --asc-options-file command line option. For example: ./asc-options.json: [ \"--optimizeLevel\", \"3\", \"--runtime\", \"none\", \"--shrinkLevel\", \"2\" ] Tell Zwitterion where to locate it: zwitterion --asc-options-file asc-options.json Rust Rust is a low-level language focused on performance, reliability, and productivity. Learn more here. Rust support is currently very basic (i.e. no wasm-bindgen support). You must have Rust installed on your machine. You can find instructions for installing Rust here. It is a goal of Zwitterion to automatically install a local version of the necessary Rust tooling when Zwitterion is installed, but that is currently a work in progress. Importing Rust is nearly identical to importing JavaScript or TypeScript. The key difference is that the default export of your entry Rust module is a function that returns a promise. This function takes as its one parameter an object containing imports to the Rust module. You can import Rust from JavaScript or TypeScript files like this: ./app.js import addModuleInit from './add.rs'; runRust(); async function runRust() { const addModule = await addModuleInit(); console.log(addModule.add(5, 5)); } ./add.rs #![no_main] #[no_mangle] pub fn add(x: i32, y: i32) -> i32 { return x + y; } C C support is currently very basic. You must have Emscripten installed on your machine. You can find instructions for installing Emscripten here. It is a goal of Zwitterion to automatically install a local version of the necessary C tooling when Zwitterion is installed, but that is currently a work in progress. Importing C is nearly identical to importing JavaScript or TypeScript. The key difference is that the default export of your entry C module is a function that returns a promise. This function takes as its one parameter an object containing imports to the C module. You can import C from JavaScript or TypeScript files like this: ./app.js import addModuleInit from './add.c'; runC(); async function runC() { const addModule = await addModuleInit(); console.log(addModule.add(5, 5)); } ./add.c int add(int x, int y) { return x + y; } C++ C++ support is currently very basic. You must have Emscripten installed on your machine. You can find instructions for installing Emscripten here. It is a goal of Zwitterion to automatically install a local version of the necessary C++ tooling when Zwitterion is installed, but that is currently a work in progress. Importing C++ is nearly identical to importing JavaScript or TypeScript. The key difference is that the default export of your entry C++ module is a function that returns a promise. This function takes as its one parameter an object containing imports to the C++ module. You can import C++ from JavaScript or TypeScript files like this: ./app.js import addModuleInit from './add.cpp'; runCPP(); async function runCPP() { const addModule = await addModuleInit(); console.log(addModule.add(5, 5)); } ./add.cpp extern \"C\" { int add(int x, int y) { return x + y; } } WebAssembly Text Format (Wat) Wat is a textual representation of the Wasm binary format. It allows Wasm to be more easily written by hand. Learn more here. Importing Wat is nearly identical to importing JavaScript or TypeScript. The key difference is that the default export of your entry Wat module is a function that returns a promise. This function takes as its one parameter an object containing imports to the Wat module. You can import Wat from JavaScript or TypeScript files like this: ./app.js import addModuleInit from './add.wat'; runWat(); async function runWat() { const addModule = await addModuleInit(); console.log(addModule.add(5, 5)); } ./add.wat (module (func $add (param $x i32) (param $y i32) (result i32) (i32.add (get_local $x) (get_local $y)) ) (export \"add\" (func $add)) ) WebAssembly (Wasm) Wasm is a binary instruction format built to be efficient, safe, portable, and open. Learn more here. Importing Wasm is nearly identical to importing JavaScript or TypeScript. The key difference is that the default export of your entry Wasm module is a function that returns a promise. This function takes as its one parameter an object containing imports to the Wasm module. You can import Wasm from JavaScript or TypeScript files like this: ./app.js import addModuleInit from './add.wasm'; runWasm(); async function runWasm() { const addModule = await addModuleInit(); console.log(addModule.add(5, 5)); } ./add.wasm Imagine this is a compiled Wasm binary file with a function called `add` Command Line Options Port Specify the server's port: Build Static Create a static build of the current working directory. The output will be in a directory called dist in the current working directory: Exclude A comma-separated list of paths, relative to the current directory, to exclude from the static build: Include A comma-separated list of paths, relative to the current directory, to include in the static build SPA Root A path to a file, relative to the current directory, to serve as the SPA root. It will be returned for the root path and when a file cannot be found: Disable SPA Disable the SPA redirect to index.html: Headers File A path to a JSON file, relative to the current directory, for custom HTTP headers: --headers-file [headersFile] Custom HTTP headers are specified as a JSON object with the following shape: type CustomHTTPHeaders = { [regexp: string]: HTTPHeaders; } type HTTPHeaders = { [key: string]: string; } For example: ./headers.json { \"^service-worker.ts$\": { \"Service-Worker-Allowed\": \"/\" } } TSC Options File A path to a JSON file, relative to the current directory, for tsc compiler options: --tsc-options-file [tscOptionsFile] The available options can be found here. Options are specified as a JSON object. For example: tsc-options.json: ASC Options File A path to a JSON file, relative to the current directory, for asc compiler options: --asc-options-file [ascOptionsFile] By default, no compiler options have been set. The available options can be found here. Options are specified as an array of option names and values. For example: ./asc-options.json: [ \"--optimizeLevel\", \"3\", \"--runtime\", \"none\", \"--shrinkLevel\", \"2\" ] Special Considerations Third-party Packages Third-party packages must be authored as if they were using Zwitterion. Essentially this means they should be authored in standard JavaScript or TypeScript, and AssemblyScript, Rust, C, and C++ must be authored according to their WebAssembly documentation. Notable exceptions will be explained in this documentation. CommonJS (the require syntax), JSON, HTML, or CSS ES Module imports, and other non-standard features that bundlers commonly support are not suppored in source code. Root File It's important to note that Zwitterion assumes that the root file (the file found at /) of your web application is always an index.html file. ES Module script elements Zwitterion depends on native browser support for ES modules (import/export syntax). You must add the type=\"module\" attribute to script elements that reference modules, for example: <script type=\"module\" src=\"amazing-module.ts\"></script> Performance It's important to note that Zwitterion does not bundle files nor engage in tree shaking. This may impact the performance of your application. HTTP2 and ES modules may help with performance, but at this point in time signs tend to point toward worse performance. Zwitterion has plans to improve performance by automatically generating HTTP2 server push information from the static build, and looking into tree shaking, but it is unclear what affect this will have. Stay tuned for more information about performance as Zwitterion matures. With all of the above being said, the performance implications are unclear. Measure for yourself. Read the following for more information on bundling versus not bundling with HTTP2: https://medium.com/@asyncmax/the-right-way-to-bundle-your-assets-for-faster-sites-over-http-2-437c37efe3ff https://stackoverflow.com/questions/30861591/why-bundle-optimizations-are-no-longer-a-concern-in-http-2 http://engineering.khanacademy.org/posts/js-packaging-http2.htm https://blog.newrelic.com/2016/02/09/http2-best-practices-web-performance/ https://mattwilcox.net/web-development/http2-for-front-end-web-developers https://news.ycombinator.com/item?id=9137690 https://www.sitepoint.com/file-bundling-and-http2/ https://medium.freecodecamp.org/javascript-modules-part-2-module-bundling-5020383cf306 https://css-tricks.com/musings-on-http2-and-bundling/ Under the Hood Zwitterion is simple. It is more or less a static file server, but it rewrites requested files in memory as necessary to return to the client. For example, if a TypeScript file is requested from the client, Zwitterion will retrieve the text of the file, compile it to JavaScript, and then return the compiled text to the client. The same thing is done for JavaScript files. In fact, nearly the same process will be used for any file extension that we want to support in the future. For example, in the future, if a C file is requested it will be read into memory, the text will be compiled to WebAssembly, and the WebAssembly will be returned to the client. All of this compilation is done server-side and hidden from the user. To the user, it's just a static file server. ",
        "Can anyone point me to a workflow where the cognitive load is similar to how it was in the good old days?<p>i.e. some <i>thing</i> that does everything for me apart from the bit where I write application and presentation code? That mostly just works and doesn't require me to understand the whole stack.<p>Because when I switch to other languages/environments I can most remain blissfully unaware of the plumbing that keeps the thing running. I learned all I needed to learn about pip and virtualenv in under an hour. My C# editor just lets me edit code and hit play.<p>Is there any chance of getting to this state of nirvana for web development?",
        "It doesn't appear to support CSS files (or other flavors like SCSS), so calling it a \"webpack killer\" seems extra. Also, a lot of people need polyfills for node modules written for NodeJS that are used in browser; for example, Buffer. Webpack takes care of all of those nuances for you. I wouldn't call this a killer for my workflows, where I need Webpack features such as aliasing, etc.<p>That being said, I support the effort to make a simpler, superior bundler. My disagreement is more so with the marketing in the title. Though, those sort of claims are what drive clicks today."
      ],
      "relevant": "false"
    },
    {
      "id": 21370525,
      "title": "Git repository summary on your terminal",
      "search": [
        "Git repository summary on your terminal",
        "https://github.com/o2sh/onefetch",
        "A command-line Git information tool written in Rust | | | Onefetch is a command-line Git information tool written in Rust that displays project information and code statistics for a local Git repository directly on your terminal. The tool is completely offline - no network access is required. By default, the repo's information is displayed alongside the dominant language's logo, but you can further configure onefetch to instead use an image - on supported terminals -, a text input or nothing at all. It automatically detects open source licenses from texts and provides the user with valuable information like code distribution, pending changes, number of dependencies (by package manager), top contributors (by number of commits), size on disk, creation date, LOC (lines of code), etc. Onefetch can be configured via command-line flags to display exactly what you want, the way you want it to: you can customize ASCII/Text formatting, disable info lines, ignore files & directories, output in multiple formats (Json, Yaml), etc. As of now, onefetch supports more than 50 different programming languages; if your language of choice isn't supported: Open up an issue and support will be added. Contributions are very welcome! See CONTRIBUTING for more info. More: [Wiki] [Installation] [Getting Started] ",
        "Reminds me of Ohloh (now OpenHub) but for your own projects and on command line.",
        "Patterns of commit activity might be shown using unicode sparklines and/or a color scale (foreground and/or background).<p>\"C++ (41.75 %)\" seems a lot of ink and space for \"C++ 42%\"."
      ],
      "relevant": "true"
    },
    {
      "id": 19427332,
      "title": "Show HN: Drogon – A C++14/17 based high performance HTTP application framework",
      "search": [
        "Show HN: Drogon – A C++14/17 based high performance HTTP application framework",
        "https://github.com/an-tao/drogon",
        "English | | Overview Drogon is a C++14/17-based HTTP application framework. Drogon can be used to easily build various types of web application server programs using C++. Drogon is the name of a dragon in the American TV series \"Game of Thrones\" that I really like. Drogon is a cross-platform framework, It supports Linux, macOS, FreeBSD, OpenBSD, HaikuOS, and Windows. Its main features are as follows: Use a non-blocking I/O network lib based on epoll (kqueue under macOS/FreeBSD) to provide high-concurrency, high-performance network IO, please visit the TFB Tests Results for more details; Provide a completely asynchronous programming mode; Support Http1.0/1.1 (server side and client side); Based on template, a simple reflection mechanism is implemented to completely decouple the main program framework, controllers and views. Support cookies and built-in sessions; Support back-end rendering, the controller generates the data to the view to generate the Html page. Views are described by CSP template files, C++ codes are embedded into Html pages through CSP tags. And the drogon command-line tool automatically generates the C++ code files for compilation; Support view page dynamic loading (dynamic compilation and loading at runtime); Provide a convenient and flexible routing solution from the path to the controller handler; Support filter chains to facilitate the execution of unified logic (such as login verification, Http Method constraint verification, etc.) before handling HTTP requests; Support https (based on OpenSSL); Support WebSocket (server side and client side); Support JSON format request and response, very friendly to the Restful API application development; Support file download and upload; Support gzip, brotli compression transmission; Support pipelining; Provide a lightweight command line tool, drogon_ctl, to simplify the creation of various classes in Drogon and the generation of view code; Support non-blocking I/O based asynchronously reading and writing database (PostgreSQL and MySQL(MariaDB) database); Support asynchronously reading and writing sqlite3 database based on thread pool; Support Redis with asynchronous reading and writing; Support ARM Architecture; Provide a convenient lightweight ORM implementation that supports for regular object-to-database bidirectional mapping; Support plugins which can be installed by the configuration file at load time; Support AOP with build-in joinpoints. Support C++ coroutines A very simple example Unlike most C++ frameworks, the main program of the drogon application can be kept clean and simple. Drogon uses a few tricks to decouple controllers from the main program. The routing settings of controllers can be done through macros or configuration file. Below is the main program of a typical drogon application: #include <drogon/drogon.h> using namespace drogon; int main() { app().setLogPath(\"./\") .setLogLevel(trantor::Logger::kWarn) .addListener(\"0.0.0.0\", 80) .setThreadNum(16) .enableRunAsDaemon() .run(); } It can be further simplified by using configuration file as follows: #include <drogon/drogon.h> using namespace drogon; int main() { app().loadConfigFile(\"./config.json\").run(); } Drogon provides some interfaces for adding controller logic directly in the main() function, for example, user can register a handler like this in Drogon: app().registerHandler(\"/test?username={name}\", [](const HttpRequestPtr& req, std::function<void (const HttpResponsePtr &)> &&callback, const std::string &name) { Json::Value json; json[\"result\"]=\"ok\"; json[\"message\"]=std::string(\"hello,\")+name; auto resp=HttpResponse::newHttpJsonResponse(json); callback(resp); }, {Get,\"LoginFilter\"}); While such interfaces look intuitive, they are not suitable for complex business logic scenarios. Assuming there are tens or even hundreds of handlers that need to be registered in the framework, isn't it a better practice to implement them separately in their respective classes? So unless your logic is very simple, we don't recommend using above interfaces. Instead, we can create an HttpSimpleController as follows: /// The TestCtrl.h file #pragma once #include <drogon/HttpSimpleController.h> using namespace drogon; class TestCtrl:public drogon::HttpSimpleController<TestCtrl> { public: virtual void asyncHandleHttpRequest(const HttpRequestPtr& req, std::function<void (const HttpResponsePtr &)> &&callback) override; PATH_LIST_BEGIN PATH_ADD(\"/test\",Get); PATH_LIST_END }; /// The TestCtrl.cc file #include \"TestCtrl.h\" void TestCtrl::asyncHandleHttpRequest(const HttpRequestPtr& req, std::function<void (const HttpResponsePtr &)> &&callback) { //write your application logic here auto resp = HttpResponse::newHttpResponse(); resp->setBody(\"<p>Hello, world!</p>\"); resp->setExpiredTime(0); callback(resp); } Most of the above programs can be automatically generated by the command line tool drogon_ctl provided by drogon (The command is drogon_ctl create controller TestCtrl). All the user needs to do is add their own business logic. In the example, the controller returns a Hello, world! string when the client accesses the http://ip/test URL. For JSON format response, we create the controller as follows: /// The header file #pragma once #include <drogon/HttpSimpleController.h> using namespace drogon; class JsonCtrl : public drogon::HttpSimpleController<JsonCtrl> { public: virtual void asyncHandleHttpRequest(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback) override; PATH_LIST_BEGIN //list path definitions here; PATH_ADD(\"/json\", Get); PATH_LIST_END }; /// The source file #include \"JsonCtrl.h\" void JsonCtrl::asyncHandleHttpRequest(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback) { Json::Value ret; ret[\"message\"] = \"Hello, World!\"; auto resp = HttpResponse::newHttpJsonResponse(ret); callback(resp); } Let's go a step further and create a demo RESTful API with the HttpController class, as shown below (Omit the source file): /// The header file #pragma once #include <drogon/HttpController.h> using namespace drogon; namespace api { namespace v1 { class User : public drogon::HttpController<User> { public: METHOD_LIST_BEGIN //use METHOD_ADD to add your custom processing function here; METHOD_ADD(User::getInfo, \"/{id}\", Get); //path is /api/v1/User/{arg1} METHOD_ADD(User::getDetailInfo, \"/{id}/detailinfo\", Get); //path is /api/v1/User/{arg1}/detailinfo METHOD_ADD(User::newUser, \"/{name}\", Post); //path is /api/v1/User/{arg1} METHOD_LIST_END //your declaration of processing function maybe like this: void getInfo(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback, int userId) const; void getDetailInfo(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback, int userId) const; void newUser(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback, std::string &&userName); public: User() { LOG_DEBUG << \"User constructor!\"; } }; } // namespace v1 } // namespace api As you can see, users can use the HttpController to map paths and parameters at the same time. This is a very convenient way to create a RESTful API application. In addition, you can also find that all handler interfaces are in asynchronous mode, where the response is returned by a callback object. This design is for performance reasons because in asynchronous mode the drogon application can handle a large number of concurrent requests with a small number of threads. After compiling all of the above source files, we get a very simple web application. This is a good start. For more information, please visit the wiki or DocsForge Contributions Every contribution is welcome. Please refer to the contribution guidelines for more information. ",
        "2 small things.<p>a) Show me the code to start with, don't send me digging for it. A SimpleController example as the very first thing would help give a feel for the project, and makes me more likely to consider the project.<p>b) If there's an easier way (like the drogon_ctl utility at the start of Quickstart [0]), show that first, and the more detailed way second.<p>Other than that, it looks great. I've used libmicrohttpd a few times, so a bit less of an overhead always looks great.<p>[0] <a href=\"https://github.com/an-tao/drogon/wiki/quick-start\" rel=\"nofollow\">https://github.com/an-tao/drogon/wiki/quick-start</a>",
        "This is cool, and I like it. Very Haskell like, which is a compliment in my book.<p>But one thing that surprises me is that folks are essentially sleeping on HTTP/2. HTTP/2 is just a hell of a lot better in most every dimension. It's better for handshake latency, it's better for bandwidth in most cases, it's better for eliminating excess SSL overhead and also, it's kinda easier to write client libraries for, because it's so much simpler (although the parallel and concurrent nature of connections will challenge a lot of programmers).<p>It's not bad to see a new contender in this space, but it's surprising that it isn't http/2 first. Is there a good reason for this? It's busted through 90% support on caniuse, so it's hard to make an argument that adoption holds it back."
      ],
      "relevant": "false"
    },
    {
      "id": 21196177,
      "title": "I wrote a command-line Ruby program to manage EC2 instances for me",
      "search": [
        "I wrote a command-line Ruby program to manage EC2 instances for me",
        "https://www.codewithjason.com/wrote-command-line-ruby-program-manage-ec2-instances/",
        "Why I did this Heroku is great, but not in 100% of cases When I want to quickly deploy a Rails application, my go-to choice is Heroku. Im a big fan of the idea that I can just run heroku create and have a production application online in just a matter of seconds. Unfortunately, Heroku isnt always a desirable option. If Im just messing around, I dont usually want to pay for Heroku features, but I also dont always want my dynos to fall asleep after 30 minutes like on the free tier. (Im aware that there are ways around this but I dont necessarily want to deal with the hassle of all that.) Also, sometimes I want finer control than what Heroku provides. I want to be closer to the metal with the ability to directly manage my EC2 instances, RDS instances, and other AWS services. Sometimes I desire this for cost reasons. Sometimes I just want to learn what I think is the valuable developer skill of knowing how to manage AWS infrastructure. Unfortunately, using AWS by itself isnt very easy. Setting up Rails on bare EC2 is a time-consuming and brain-consuming hassle Getting a Rails app standing up on AWS is pretty hard and time-consuming. Im actually not even going to get into Rails-related stuff in this post because even the small task of getting an EC2 instance up and runningwithout no additional software installed on that instanceis a lot harder than I think it should be, and theres a lot to discuss and improve just inside that step. Just to briefly illustrate what a pain in the ass it is to get an EC2 instance launched and to SSH into it, here are the steps. The steps that follow are the command-line steps. I find the AWS GUI console steps roughly equally painful. 1. Use the AWS CLI create-key-pair command to create a key pair. This step is necessary for later when I want to SSH into my instance. 2. Think of a name for the key pair and save it somewhere. Thinking of a name might seem like a trivially small hurdle, but every tiny bit of mental friction adds up. I dont want to have to think of a name, and I dont want to have to think about where to put the file (even if that means just remembering that I want to put the key in ~/.ssh, which is the most likely case. 3. Use the run-instances command, using an AMI ID (AMI == Amazon Machine Image) and passing in my key name. Now I have to go look up the run-instances (because I sure as hell dont remember it) and, look up my AMI ID, and remember what my key name is. (If you dont know what an AMI ID is, thats what determines whether the instance will be Ubuntu, Amazon Linux, Windows, etc.) 4. Use the describe-instances command to find out the public DNS name of the instance I just launched. This means I either have to search the JSON response of describe-instances for the PublicDnsName entry or apply a filter. Just like with every AWS CLI command, Id have to go look up the exact syntax for this. 5. Run the ssh command, passing in my instances DNS and the path to my key. This step is probably the easiest, although it took me a long time to commit the exact ssh -i syntax to memory. For the record, the command is ssh -i ~/.ssh/my_key.pem ubuntu@mypublicdns.com. Its a small pain in the ass to have to look up the public DNS for my instance again and remember whether my EC2 user is going to be ubuntu or ec2-user (it depends on what AMI I used). My goals for my AWS command-line tool All this fuckery was a big hassle so I decided to write my own command-line tool to manage EC2 instances. I call the tool Exosuit. You can actually try it out yourself by following these instructions. There were four specific capabilities I wanted Exosuit to have. Launch an instance By running bin/exo launch, it should launch an EC2 instance for me. It should assume I want Ubuntu. It should let me know when the instance is ready, and what its instance ID and public DNS are. SSH into an instance I should be able to run bin/exo ssh, get prompted for which instance I want to SSH into, and then get SSHd into that instance. List all running instances I should be able to run bin/exo instances to see all my running instances. It should show the instance ID and public DNS for each. Terminate instances I should be able to run bin/exo terminate which will show me all my instance IDs and allow me to select one or more of them for termination. How I did it Side note: when I first wrote this, I forgot that the AWS SDK for Ruby existed, so I reinvented some wheels. Whoops. After I wrote this I refactored the project to use AWS SDK instead of shell out to AWS CLI. For brevity Ill focus on the bin/exo launch command. Using the AW CLI run-instances command The AWS CLI command for launching an instance looks like this: aws ec2 run-instances \\ --count 1 \\ --image-id ami-05c1fa8df71875112 \\ --instance-type t2.micro \\ --key-name normal-quiet-carrot \\ --profile personal Hopefully most of these flags are self-explanatory. You might wonder where the key name of normal-quiet-carrot came from. When the bin/exo launch command is run, Exosuit asks Is there a file defined at .exosuit/config.yml that contains a key pair name and path? If not, create that file, create a new key pair with a random phrase for a name, and save the name and path to that file. Heres what my .exosuit/config.yml looks like: --- aws_profile_name: personal key_pair: name: normal-quiet-carrot path: \"~/.ssh/normal-quiet-carrot.pem\" The aws_profile_name is something that I imagine most users arent likely to need. I personally happen to have multiple AWS accounts, so its necessary for me to send a --profile flag when using AWS CLI commands so AWS knows which account of mine to use. If a profile isnt specified in .exosuit/config.yml, Exosuit will just leave the --profile flag off and everything will still work fine. Abstracting the run-instances command Once I had coded Exosuit to construct a few different AWS CLI commands (e.g. run-instances, terminate-instances), I noticed that things were getting a little repetitive. Most troubling, I had to always remember to include the --profile flag (just as I would if I were typing all this on the command line manually), and I didnt always remember to do so. In those cases my command would get sent to the wrong account. Thats bad. So I created an abstraction called AWSCommand. Heres what a usage of it looks like: command = AWSCommand.new( :run_instances, count: 1, image_id: IMAGE_ID, instance_type: INSTANCE_TYPE, key_name: key_pair.name ) JSON.parse(command.run) You can probably see the resemblance it bears to the bare run-instances usage. Note the conspicuous absence of the profile flag, which is now automatically included every single time. Listening for launch success One of my least favorite things about manually launching EC2 instances is having to check periodically to see when theyve started running. So I wanted Exosuit to tell me when my EC2 instance was running. I achieved this by writing a loop that hits AWS once per second, checking the state of my new instance each time. module Exosuit def self.launch_instance response = Instance.launch(self.key_pair) instance_id = response['Instances'][0]['InstanceId'] print \"Launching instance #{instance_id}...\" while true sleep(1) print '.' instance = Instance.find(instance_id) if instance && instance.running? puts break end end puts 'Instance is now running' puts \"Public DNS: #{instance.public_dns_name}\" end end You might wonder what Instance.find and instance.running? do. The Instance.find method will run the aws ec2 describe-instances command, parse the JSON response, then grab the relevant JSON data for whatever instance_id I passed to it. The return value is an instance of the Instance class. When an instance of Instance is instantiated, an instance variable gets set (pardon all the instances) with all the JSON data for that instance that was returned by the AWS CLI. The instance.running? method simply looks at that JSON data (which has since been converted to a Ruby hash) and checks to see what the value of ['State']['Name'] is. Heres an abbreviated version of the Instance class for reference. module Exosuit class Instance def initialize(info) @info = info end def state @info['State']['Name'] end def running? state == 'running' end end end (By the way, all the Exosuit code is available on GitHub if youd like to take a look.) Success notification As you can see from the code a couple snippets above, Exosuit lets me know once my instances has entered a running state. At this point I can run bin/exo ssh, bin/exo instances or bin/exo terminate to mess with my instance(s) as I please. Demo video Heres a small sample of Exosuit in action: Try it out yourself If youd like to try out Exosuit, just visit the Getting Started with Exosuit guide. If you think this idea is cool and useful, please let me know by opening a GitHub issue for a feature youd like to see, or tweeting at me, or simply starring the project on GitHub so I can gage interest. I hope you enjoyed this explanation and I look forward to sharing the next steps I take with this project. ",
        "Dude, thank you so much. This looks like exactly what I need for a project I'm on."
      ],
      "relevant": "false"
    },
    {
      "id": 19108787,
      "title": "Why are we templating YAML?",
      "search": [
        "Why are we templating YAML?",
        "https://leebriggs.co.uk/blog/2019/02/07/why-are-we-templating-yaml.html",
        "Published Feb 7, 2019 by Lee Briggs #kubernetes #configuration mgmt #jsonnet #helm #kr8 I was at cfgmgmtcamp 2019 in Ghent, and did a talk which I think was well received about the need for some Kubernetes configuration management as well as the solution we built for it at $work, kr8. I made a statement during the talk which ignited some fairly fierce discussion both online, and at the conference: \"If you're starting to template yaml, ask yourself the question: why am I not *generating* json?\" - @briggsl spitting straight fire at #cfgmgmtcamp eric sorenson (@ahpook) February 5, 2019 To put this into my own words: At some point, we decided it was okay for us to template yaml. When did this happen? How is this acceptable? After some conversation, I figured it was probably best to back up my claims in some way. This blog post is going to try to do that. The configuration problem Once the applications and infrastructure youre going to manage grows past a certain size, you inevitably end up in some form of configuration complexity hell. If youre only deploying 1 or maybe 2 things, you can write a yaml configuration file and be done with it. However once you grow beyond that, you need to figure out how to manage this complexity. Its incredibly likely that the reason you have multiple configuration files is because the $thing that uses that config is slightly different from its companions. Examples of this include: Applications deployed in different environments, like dev, stg and prod Applications deployed in different regions, like Europe or North American Obviously, not all the configuration is different here, but its likely the configuration differs enough that you want to be able to differentiate between the two. This configuration complexity has been well known for Operators (System Administrators, DevOps engineers, whatever you want to call them) for some years now. An entire discpline grew up around this in Configuration Management, and each tool solved this problem in their own way, but ultimately, they used YAML to get the job done. My favourite method has always been hiera which comes bundled with Puppet. Having the ability to hierarchically look up the variables of specific config needs is incredibly powerful and flexible, and has generally meant you dont actually need to do any templating of yaml at all, except perhaps for embedding Puppet facts into the yaml. Did we go backwards? Then, as our industries needs moved above the operating system and into cloud computing, we had a whole new data plane to configure. The tooling to configure this changed, and tools like CloudFormation and Helm appeared. These tools are excellent configuration tools, but I firmly believe we (as an industry) got something really, really wrong when we designed them. To examine that, lets take a look at example of a helm chart taking a custom parameter Helm Charts Helm charts can take external parameters defined by an values.yaml file which you specify when rendering the chart. A simple example might look like this: Lets say my external parameter is simple - its a string. Itd look a bit like this: image: \"{{ .Values.image }}\" Thats not so bad right? You just specify a value for image in your values.yaml and youre on your way. The real problem starts to get highlighted when you want to do more complicated and complex things. In this particular example, youre doing okay because you know you have to specify an image for a Kubernetes deployment. However, what if youre working with something like an optional field? Well, then it gets a little more unwieldy: {{- with .resourceGroup }} resourceGroup: {{ . }} {{- end }} Optional values just make things ugly in templating languages, and you cant just leave the value blank, so you have to resort to ugly loops and conditionals that are probably going to bite you later. Lets say you need to go a step further, and you need to push an array or map into the config. With helm, youd do something like this. {{- with .Values.podAnnotations }} annotations: {{ toYaml . | indent 8 }} {{- end }} Firstly, lets ignore the madness of having a templating function toYaml to convert yaml to yaml and focus more on the whitespace issue here. YAML has strict requirements and whitespace implementation rules. The following, for example, is not valid or complete yaml: something: nothing hello: goodbye Generally, if youre handwriting something, this isnt necessarily a problem because you just hit backspace twice and its fixed. However, if youre generating YAML using a templating system, you cant do that - and if youre operating above 5 or 10 configuration files, you probably want to be generating your config rather than writing it. So, in the above example, you want to embed the values of .Values.podAnnotations under the annotations field, which is indented already. So youre having to not only indent your values, but indent them correctly. What makes this even more confusing is that the go parser doesnt actually know anything about YAML at all, so if you try to keep the syntax clean and indent the templates like this: {{- with .Values.podAnnotations }} annotations: {{ toYaml . | indent 6 }} {{- end }} You actually cant do that, because the templating system gets confused. This is a singular example of the complexity and difficulty you end up facing when generating config data in YAML, but when you really start to do more complex work, it really starts to become obvious that this isnt the way to go. Needless to say, this isnt what I want to spend my time doing. If fiddling around with whitespace requirements in a templating system doing something its not really designed for is what suits you, then Im not going to stop you. I also dont want to spend my time writing configuration in JSON without comments and accidentally missing commas all over the shop. We (as an industry) decided a long time ago that shit wasnt going to work and thats why YAML exists. So what should we do instead? Thats where jsonnet comes in. JSON, Jsonnet & YAML Before we actually talk about Jsonnet, its worth reminding people of a very important (but oft forgotten point). YAML is a superset of JSON and converting between the two is trivial. Many applications and programming languages will parse JSON and YAML natively, and many can convert between the two very simple. For example, in Python: python -c 'import json, sys, yaml ; y=yaml.safe_load(sys.stdin.read()) ; print(json.dumps(y))' So with that in mind, lets talk about Jsonnet. Welcome to the church of Jsonnet Jsonnet is a relatively new, little known (outside the Kubernetes community?) language that calls itself a data templating language. Its definitely a good exercise to read and consume the Jsonnet design rationale page to get an idea why it exists, but if I was going to define in a nutshell what its purpose is - its to generate JSON config. So, how does it help, exactly? Well, lets take our earlier example - we want to generate some JSON config specifying a parameter (ie, the image string). We can do that very very easily with Jsonnet using external variables. Firstly, lets define some Jsonnet: { image: std.extVar('image'), } Then, we can generate it using the Jsonnet command line tool, passing in the external variable as we need to: jsonnet image.jsonnet -V image=\"my-image\" { \"image\": \"my-image\" } Easy! Optional fields Before, I noted that if you wanted to define an optional field, with YAML templating you had to define if statements for everything. With Jsonnet, youre just defining code! // define a variable - yes, jsonnet also has comments local rg = null; { image: std.extVar('image'), // if the variable is null, this will be blank [if rg != null then 'resourceGroup']: rg, } The output here, because our variable is null, means that we never actually populate resourceGroup. If you specify a value, it will appear: jsonnet image.jsonnet -V image=\"my-image\" { \"image\": \"my-image\" } Maps and parameters Okay, now lets look at our previous annotation example. We want to define some pod annotations, which takes a YAML map as its input. You want this map to be configurable by specifying external data, and obviously doing that on the command line sucks (youd be very unlikely to specify this with Helm on the command line, for example) so generally youd use Jsonnet imports to this. Im going to specify this config as a variable and then load that variable into the annotation: local annotations = { 'nginx.ingress.kubernetes.io/app-root': '/', 'nginx.ingress.kubernetes.io/enable-cors': true, }; { metadata: { // annotations are nested under the metadata of a pod annotations: annotations, }, } This might just be my bias towards Jsonnet talking, but this is so dramatically easier than faffing about with indentation that I cant even begin to describe it. Additional goodies The final thing I wanted to quickly explore, which is something that I feel cant really be done with Helm and other yaml templating tools, is the concept of manipulating existing objects in config. Lets take our example above with the annotations, and look at the result file: { \"metadata\": { \"annotations\": { \"nginx.ingress.kubernetes.io/app-root\": \"/\", \"nginx.ingress.kubernetes.io/enable-cors\": true } } } Now, lets say for example I wanted to append a set of annotations to this annotations map. In any templating system, Id probably have to rewrite the whole map. Jsonnet makes this trivial. I can simply use the + operator to add something to this. Heres a (poor) example: local annotations = { 'nginx.ingress.kubernetes.io/app-root': '/', 'nginx.ingress.kubernetes.io/enable-cors': true, }; { metadata: { annotations: annotations, }, } + { // this adds another JSON object metadata+: { // I'm using the + operator, so we'll append to the existing metadata annotations+: { // same as above something: 'nothing', }, }, } The end result is this: { \"metadata\": { \"annotations\": { \"nginx.ingress.kubernetes.io/app-root\": \"/\", \"nginx.ingress.kubernetes.io/enable-cors\": true, \"something\": \"nothing\" } } } Obviously, in this case, its more code to this, but as your example get more complex, it becomes extremely useful to be able to manipulate objects this way. Kr8 We use all of these methods in kr8 to make creating and manipulating configuration for multiple Kubernetes clusters easy and simple. I highly recommend you check it out if any of the concepts youve found here have found you nodding your head. ",
        "I know I'm in a minority, but I really dislike YAML... I recently did a lot of Ansible and boy, at the beginning, I was just struggling a lot. Syntactic whitespace kills me.<p>I don't like it in Python either, but for some reason, when I write Python, it's a lot easier. Maybe YAML is just a bit more complex (and Python has better IDE support..?)",
        "My belief is that we've been slowly building up to using general purpose languages, one small step at a time, throughout the infrastructure as code, DevOps, and SRE journeys this past 10 years. INI files, XML, JSON, and YAML aren't sufficiently expressive -- lacking for loops, conditionals, variable references, and any sort of abstraction -- so, of course, we add templates to it. But as the author (IMHO rightfully) points out, we just end up with a funky, poor approximation of a language.<p>I think this approach is a byproduct of thinking about infrastructure and configuration -- and the cloud generally -- as an \"afterthought,\" not a core part of an application's infrastructure. Containers, Kubernetes, serverless, and more hosted services all change this, and Chef, Puppet, and others laid the groundwork to think differently about what the future looks like. More developers today than ever before need to think about how to build and configure cloud software.<p>We started the Pulumi project to solve this very problem, so I'm admittedly biased, and I hope you forgive the plug -- I only mention it here because I think it contributes to the discussion. Our approach is to simply use general purpose languages like TypeScript, Python, and Go, while still having infrastructure as code. An important thing to realize is that infrastructure as code is based on the idea of a <i>goal state</i>. Using a full blown language to generate that goal state generally doesn't threaten the repeatability, determinism, or robustness of the solution, provided you've got an engine handling state management, diffing, resource CRUD, and so on. We've been able to apply this universally across AWS, Azure, GCP, <i>and</i> Kubernetes, often mixing their configuration in the same program.<p>Again, I'm biased and want to admit that, however if you're sick of YAML, it's definitely worth checking out. We'd love your feedback:<p>- Project website: <a href=\"https://pulumi.io/\" rel=\"nofollow\">https://pulumi.io/</a><p>- All open source on GitHub: <a href=\"https://github.com/pulumi/pulumi\" rel=\"nofollow\">https://github.com/pulumi/pulumi</a><p>- Example of abstractions: <a href=\"https://blog.pulumi.com/the-fastest-path-to-deploying-kubernetes-on-aws-with-eks-and-pulumi\" rel=\"nofollow\">https://blog.pulumi.com/the-fastest-path-to-deploying-kubern...</a><p>- Example of serverless as event handlers: <a href=\"https://blog.pulumi.com/lambdas-as-lambdas-the-magic-of-simple-serverless-functions\" rel=\"nofollow\">https://blog.pulumi.com/lambdas-as-lambdas-the-magic-of-simp...</a><p>Pulumi may not be <i>the</i> solution for everyone, but I'm fairly optimistic that this is where we're all heading.<p>Joe"
      ],
      "relevant": "true"
    },
    {
      "id": 18937195,
      "title": "Gunk: Modern front end and syntax for Protocol Buffers",
      "search": [
        "Gunk: Modern front end and syntax for Protocol Buffers",
        "https://github.com/gunk/gunk",
        "Gunk is a modern frontend and syntax for Protocol Buffers. Quickstart | Installing | Syntax | Configuring | About | Releases Overview Gunk provides a modern project-based workflow along with a Go-derived syntax for defining types and services for use with Protocol Buffers. Gunk is designed to integrate cleanly with existing protoc based build pipelines, while standardizing workflows in a way that is familiar/accessible to Go developers. Quickstart Create a working directory for a project: $ mkdir -p ~/src/example && cd ~/src/example Install gunk and place the following Gunk definitions in example/util.gunk: package util // Util is a utility service. type Util interface { // Echo returns the passed message. Echo(Message) Message } // Message contains an echo message. type Message struct { // Msg is a message from a client. Msg string `pb:\"1\"` } Create the corresponding project configuration in example/.gunkconfig: [generate go] [generate js] import_style=commonjs binary Then, generate protocol buffer definitions/code: $ ls -A .gunkconfig util.gunk $ gunk generate $ ls -A all.pb.go all_pb.js .gunkconfig util.gunk As seen above, gunk generated the corresponding Go and JavaScript protobuf code using the options defined in the .gunkconfig. End-to-end Example A end-to-end example gRPC server implementation, using Gunk definitions is available for review. Debugging protoc commands Underlying commands executed by gunk can be viewed with the following: $ gunk generate -x protoc-gen-go protoc --js_out=import_style=commonjs,binary:/home/user/example --descriptor_set_in=/dev/stdin all.proto Installing The gunk command-line tool can be installed via Release, via Homebrew, via Scoop or via Go: Installing via Release Download a release for your platform Extract the gunk or gunk.exe file from the .tar.bz2 or .zip file Move the extracted executable to somewhere on your $PATH (Linux/macOS) or %PATH% (Windows) Installing via Homebrew (macOS) gunk is available in the gunk/gunk tap, and can be installed in the usual way with the brew command: # add tap $ brew tap gunk/gunk # install gunk $ brew install gunk Installing via Scoop (Windows) gunk can be installed using Scoop: # install scoop if not already installed iex (new-object net.webclient).downloadstring('https://get.scoop.sh') scoop install gunk Installing via Go gunk can be installed in the usual Go fashion: # install gunk $ go get -u github.com/gunk/gunk Protobuf Dependency and Caching The gunk command-line tool uses the protoc command-line tool. gunk can be configured to use protoc at a specified path. If it isn't available, gunk will download the latest protobuf release to the user's cache, for use. It's also possible to pin a specific version, see the section on protoc configuration. Protocol Types and Messages Gunk provides an alternate, Go-derived syntax for defining protocol buffers. As such, Gunk definitions are a subset of the Go programming language. Additionally, a special +gunk annotation is recognized by gunk, to allow the declaration of protocol buffer options: package message import \"github.com/gunk/opt/http\" // Message is a Echo message. type Message struct { // Msg holds a message. Msg string `pb:\"1\" json:\"msg\"` Code int `pb:\"2\" json:\"code\"` } // Util is a utility service. type Util interface { // Echo echoes a message. // // +gunk http.Match{ // Method: \"POST\", // Path: \"/v1/echo\", // Body: \"*\", // } Echo(Message) Message } Technically speaking, gunk is not actually strict subset of go, as gunk allows unused imports; it actually requires them for some features. See the example above; in pure go, this would not be a valid go code, as http is not used outside of the comment. Scalars Gunk's Go-derived syntax uses the canonical Go scalar types of the proto3 syntax, defined by the protocol buffer project: Proto3 Type Gunk Type double float64 float float32 int32 int int32 int32 int64 int64 uint32 uint uint32 uint32 uint64 uint64 bool bool string string bytes []byte Note: Variable-length scalars will be enabled in the future using a tag parameter. Messages Gunk's Go-derived syntax uses Go's struct type declarations for declaring messages, and require a pb:\"<field_number>\" tag to indicate the field number: type Message struct { FieldA string `pb:\"1\"` } type Envelope struct { Message Message `pb:\"1\" json:\"msg\"` } There are additional tags (for example, the json: tag above), that will be recognized by gunk format, and passed on to generators, where possible. Note: When using gunk format, a valid pb:\"<field_number>\" tag will be automatically inserted if not declared. Services Gunk's Go-derived syntax uses Go's interface syntax for declaring services: type SearchService interface { Search(SearchRequest) SearchResponse } The above is equivalent to the following protobuf syntax: service SearchService { rpc Search (SearchRequest) returns (SearchResponse); } Enums Gunk's Go-derived syntax uses Go const's for declaring enums: type MyEnum int const ( MYENUM MyEnum = iota MYENUM2 ) Note: values can also be fixed numeric values or a calculated value (using iota). Maps Gunk's Go-derived syntax uses Go map's for declaring map fields: type Project struct { ProjectID string `pb:\"1\" json:\"project_id\"` } type GetProjectResponse struct { Projects map[string]Project `pb:\"1\"` } Repeated Values Gunk's Go-derived syntax uses Go's slice syntax ([]) for declaring a repeated field: type MyMessage struct { FieldA []string `pb:\"1\"` } Message Streams Gunk's Go-derived syntax uses Go chan syntax for declaring streams: type MessageService interface { List(chan Message) chan Message } The above is equivalent to the following protobuf syntax: service MessageService { rpc List(stream Message) returns (stream Message); } Protocol Options Protocol buffer options are standard messages (ie, a struct), and can be attached to any service, message, enum, or other other type declaration in a Gunk file via the doccomment preceding the type, field, or service: // MyOption is an option. type MyOption struct { Name string `pb:\"1\"` } // +gunk MyOption { // Name: \"test\", // } type MyMessage struct { /* ... */ } Project Configuration Files Gunk uses a top-level .gunkconfig configuration file for managing the Gunk protocol definitons for a project: # Example .gunkconfig for Go, grpc-gateway, Python and JS [generate go] out=v1/go plugins=grpc [generate] out=v1/go command=protoc-gen-grpc-gateway logtostderr=true [generate python] out=v1/python [generate js] out=v1/js import_style=commonjs binary Project Search Path When gunk is invoked from the command-line, it searches the passed package spec (or current working directory) for a .gunkconfig file, and walks up the directory hierarchy until a .gunkconfig is found, or the project's root is encountered. The project root is defined as the top-most directory containing a .git subdirectory, or where a go.mod file is located. Format The .gunkconfig file format is compatible with Git config syntax, and in turn is compatible with the INI file format: [generate] command=protoc-gen-go [generate] out=v1/js protoc=js Global section import_path - see \"Converting Existing Protobuf Files\" strip_enum_type_names - with this option on, enums with their type prefixed will be renamed to the version without prefix. Note that this might produce invalid protobuf that stops compiling in 1.4.* protoc-gen-go, if the enum names clash. Section [protoc] The path where to check for (or where to download) the protoc binary can be configured. The version can also be pinned. Parameters version - the version of protoc to use. If unspecified, defaults to the latest release available. Otherwise, gunk will either download the specified version, or check that the version of protoc at the specified path matches what was configured. path - the path to check for the protoc binary. If unspecified, defaults appropriate user cache directory for the user's OS. If no file exists at the path, gunk will attempt to download protoc. Section [generate[ <type>]] Each [generate] or [generate <type>] section in a .gunkconfig corresponds to a invocation of the protoc-gen-<type> tool. Parameters Each name[=value] parameter defined within a [generate] section will be passed as a parameter to the protoc-gen-<type> tool, with the exception of the following special parameters that override the behavior of the gunk generate tool: command - overrides the protoc-gen-* command executable used by gunk generate. The executable must be findable on $PATH (Linux/macOS) or %PATH% (Windows), or may be the full path to the executable. If not defined, then command will be protoc-gen-<type>, when <type> is the value in [generate <type>]. protoc - overrides the <type> value, causing gunk generate to use the protoc value in place of <type>. out - overrides the output path of protoc. If not defined, output will be the same directory as the location of the .gunk files. plugin_version - specify version of plugin. The plugin is downloaded from github/maven, built in cache and used. It is not installed in $PATH. This currently works with the following plugins: protoc-gen-go protoc-gen-grpc-java protoc-gen-grpc-gateway protoc-gen-openapiv2 (protoc-gen-swagger support is deprecated) protoc-gen-swift (installing swift itself first is necessary) protoc-gen-grpc-swift (installing swift itself first is necessary) protoc-gen-ts (installing node and npm first is necessary) protoc-gen-grpc-python (cmake, gcc is necessary; takes ~10 minutes to clone build) It is recommended to use this function everywhere, for reproducible builds, together with version for protoc. json_tag_postproc - uses json tags defined in gunk file also for go-generated file fix_paths_postproc - for js and ts - by default, gunk generates wrong paths for other imported gunk packages, because of the way gunk moves files around. Works only if js also has import_style=commonjs option. All other name[=value] pairs specified within the generate section will be passed as plugin parameters to protoc and the protoc-gen-<type> generators. Short Form The following .gunkconfig: [generate go] [generate js] out=v1/js is equivalent to: [generate] command=protoc-gen-go [generate] out=v1/js protoc=js Different forms of invocation There are three different forms of gunkconfig sections that have three different semantics. [generate] command=protoc-gen-go [generate] protoc=go [generate go] The first one uses protoc-gen-go plugin directly, without using protoc. It also attempts to move files to the same directory as the gunk file. The second one uses protoc and does not attempt to move any files. Protoc attempts to load plugin from $PATH, if it is not one of the built-in protoc plugins; this will not work together with pinned version and other gunk features and is not recommended outside of built-in protoc generators. The third version is reccomended. It will try to detect whether language is one of built-in protoc generators, in that case behaves like the second way, otherwise behaves like the first. The built-in protoc generators are: cpp java python php ruby csharp objc js Third-Party Protobuf Options Gunk provides the +gunk annotation syntax for declaring protobuf options, and specially recognizes some third-party API annotations, such as Google HTTP options, including all builtin/standard protoc options for code generation: // +gunk java.Package(\"com.example.message\") // +gunk java.MultipleFiles(true) package message import ( \"github.com/gunk/opt/http\" \"github.com/gunk/opt/file/java\" ) type Util interface { // +gunk http.Match{ // Method: \"POST\", // Path: \"/v1/echo\", // Body: \"*\", // } Echo() } Further documentation on available options can be found at the Gunk options project. Formatting Gunk Files Gunk provides the gunk format command to format .gunk files (akin to gofmt): $ gunk format /path/to/file.gunk $ gunk format <pathspec> Converting Existing Protobuf Files Gunk provides the gunk convert command that will converting existing .proto files (or a directory) to the Go-derived Gunk syntax: $ gunk convert /path/to/file.proto $ gunk convert /path/to/protobuf/directory If your .proto is referencing another .proto from another directory, you can add import_path in the global section of your .gunkconfig. If you don't provide import_path it will only search in the root directory. import_path=relative/path/to/protobuf/directory The path to provide is relative from the .gunkconfig location. Furthermore, the referenced files must contain: option go_package=\"path/of/go/package\"; The resulting .gunk file will contain the import path as defined in go_package: import ( name \"path/of/go/package\" ) About Gunk is developed by the team at Brankas, and was designed to streamline API design and development. History From the beginning of the company, the Brankas team defined API types and services in .proto files, leveraging ad-hoc Makefile's, shell scripts, and other non-standardized mechanisms for generating Protocol Buffer code. As development exploded in 2017 (and beyond) with continued addition of backend microservices/APIs, more code repositories and projects, and team members, it became necessary to standardize tooling for the organization as well as reduce the cognitive load of developers (who for the most part were working almost exclusively with Go) when declaring gRPC and REST services. Naming The Gunk name has a cheeky, backronym \"Gunk Unified N-terface Kompiler\", however the name was chosen because it was possible to secure the GitHub gunk project name, was short, concise, and not used by other projects. Additionally, \"gunk\" is an apt description for the \"gunk\" surrounding protocol definition, generation, compilation, and delivery. Contributing Issues, Pull Requests, and other contributions are greatly welcomed and appreciated! Get started with building and running gunk: # clone source repository $ git clone https://github.com/gunk/gunk.git && cd gunk # force GO111MODULES $ export GO111MODULE=on # build and run $ go build && ./gunk Dependency Management Gunk uses Go modules for dependency management, and as such requires Go 1.11+. Please run go mod tidy before submitting any PRs: $ export GO111MODULE=on $ cd gunk && go mod tidy ",
        "I'm not seeing what makes this \"modern\". proto3 is only a few years old and nothing about it strikes me as unusually archaic. Protobuf in general isn't that much older than Go. I can see why Go-compatible syntax would be attractive to Go developers, so maybe that should be in the description rather than \"modern\"?",
        "Not sure why I’d want to define a language independent interchange format in a language specific way and remove all of the tooling help at the same time. Why is this better? A why section/motivations would help greatly."
      ],
      "relevant": "false"
    },
    {
      "id": 18964961,
      "title": "A command-line installer for Windows",
      "search": [
        "A command-line installer for Windows",
        "https://scoop.sh/",
        "A command-line installer for Windows Scoop installs the tools you know and love Get comfortable on the Windows command line Looking for familiar Unix tools? Tired of Powershells Verb-Noun verbosity? Scoop helps you get the programs you need, with a minimal amount of point-and-clicking. Say goodbye to permission pop-ups Scoop installs programs to your home directory by default. So you dont need admin permissions to install programs, and you wont see UAC popups every time you need to add or remove a program. Scoop reads the README for you Not sure whether you need 32-bit or 64-bit? Cant remember that command you have to type after you install to get that other thing you need? Scoop has you covered. Just scoop install and youll be ready to work in no time. Demo Installs in seconds Make sure PowerShell 5 (or later, include PowerShell Core) and .NET Framework 4.5 (or later) are installed. Then run: Invoke-Expression (New-Object System.Net.WebClient).DownloadString('https://get.scoop.sh') # or shorter iwr -useb get.scoop.sh | iex Note: if you get an error you might need to change the execution policy (i.e. enable Powershell) with Set-ExecutionPolicy RemoteSigned -scope CurrentUser ",
        "Reading the headline made of think of installing Windows via the command-line (say via imagex). Which is actually very useful in certain tricky situations when you mess up an install.<p>Once I didn't have enough space on a flash drive to install Windows via an ISO...but I did have a liveboot ubuntu system and 2 spare drives. Interestingly enough imagex and WIM tools were ported to linux, and it was a surprisingly easy experience.",
        "I struggle with all the alternate 'installation managers' for Windows. Microsoft has already done this with their MSI system (Although even they have basically abandoned it in favour of the APPX stuff). MSI was a good format, and could be installed silently with the right switches (/q/b), support alternate installation options via MST files, and even patches via MSP files. Even the specific DLL dependencies ('DLL Hell') got fixed via manifest files/attributes. Microsoft published the MSI specification early on, and encouraged everyone to support it.<p>However, the core problem with Windows software is there isn't a primary software repository for all approved and tested software, like there is for *nix platforms. Ideally that's what needs to be fixed first before yet more client-end installers get created.<p>To fix this, based on the current landscape, all third-party client-end installers need to support all existing third-party repositories.  Even better, everyone agrees on a standard JSON format (or whatever) for their repository manifest and all third-party installers understand that. Then just like Linux et al. all you have to do is add the new repository URL to your installers config, and all the packages advertised within are immediately available.<p>It seems really simple to fix - but people have to co-operate."
      ],
      "relevant": "false"
    },
    {
      "id": 20950146,
      "title": "Varlink – A plain-text, type-safe, discoverable, self-documenting interface",
      "search": [
        "Varlink – A plain-text, type-safe, discoverable, self-documenting interface",
        "https://varlink.org",
        "Varlink is an interface description format and protocol that aims to make services accessible to both humans and machines in the simplest feasible way. A varlink interface combines the classic UNIX command line options, STDIN/OUT/ERROR text formats, man pages, service metadata and provides the equivalent over a single file descriptor, a.k.a. FD3. Varlink is plain-text, type-safe, discoverable, self-documenting, remotable, testable, easy to debug. Varlink is accessible from any programming environment. See the Ideals page for more. And everybody likes Screenshots. Interface A varlink interface has a reverse-domain name and specifies which methods the interface implements. Each method has named and typed input and output parameters. Complex types can be aliased with the type keyword to allow reusing them and to make method signatures easier to read. The interface also specifies the errors that may be returned from its method calls. Everything can be documented by adding a comment immediately before it. The documentation is provided to clients as structured data on a well-known service interface. See the Interface Definition about the varlink syntax and how to parse an interface file. # Interface to jump a spacecraft to another point in space. # The FTL Drive is the propulsion system to achieve # faster-than-light travel through space. A ship making a # properly calculated jump can arrive safely in planetary # orbit, or alongside other ships or spaceborne objects. interface org.example.ftl # The current state of the FTL drive and the amount of # fuel available to jump. type DriveCondition ( state: (idle, spooling, busy), tylium_level: int ) # Speed, trajectory and jump duration is calculated prior # to activating the FTL drive. type DriveConfiguration ( speed: int, trajectory: int, duration: int ) # The galactic coordinates use the Sun as the origin. # Galactic longitude is measured with primary direction # from the Sun to the center of the galaxy in the galactic # plane, while the galactic latitude measures the angle # of the object above the galactic plane. type Coordinate ( longitude: float, latitude: float, distance: int ) # Monitor the drive. The method will reply with an update # whenever the drive's state changes method Monitor() -> (condition: DriveCondition) # Calculate the drive's jump parameters from the current # position to the target position in the galaxy method CalculateConfiguration( current: Coordinate, target: Coordinate ) -> (configuration: DriveConfiguration) # Jump to the calculated point in space method Jump(configuration: DriveConfiguration) -> () # There is not enough tylium to jump with the given # parameters error NotEnoughEnergy () # The supplied parameters are outside the supported range error ParameterOutOfRange (field: string) Protocol All messages are encoded as JSON objects and terminated with a single NUL byte. A service responds to requests in the same order that they are receivedmessages are never multiplexed. However, multiple requests can be queued on a connection to enable pipelining. This simplifies and minimizes the amount of state clients need to track. The common case is a simple method call with a single reply. To support monitoring calls, subscriptions, chunked data, streaming, calls may carry instructions for the server to not reply, or to reply multiple times to a single method call. See the Method Call page for a detailed description. In common programming languages, varlink clients do not require complex modules or libraries, already existing JSON and socket communication facilities are used to integrate natively into the programming languages object model. Requests specify the fully-qualified method that should be called, along with its input parameters: { \"method\": \"org.example.ftl.CalculateConfiguration\", \"parameters\": { \"current\": { \"longitude\": \"27.13\", \"latitude\": \"-12.4\", \"distance\": \"48732498234\" }, \"target\": { \"longitude\": \"-48.7\", \"latitude\": \"12.9\", \"distance\": \"354667658787\" } } } A service replies with an object that contains the output parameters: { \"parameters\": { \"configuration\": { \"speed\": \"32434234\", \"trajectory\": \"686787\", \"duration\": \"13256445\" } } } Errors contain the fully-qualified error and optional parameters as specified and documented in the varlink interface file: { \"error\": \"org.example.ftl.ParameterOutOfRange\", \"parameters\": { \"field\": \"current.distance\" } } Service Every varlink service offers the org.varlink.service interface, which describes all interfaces the service provides and provides information about the service implementation itself. See the Service page for details. Address Varlink services are expressed in URI notation. All properties after a ; character should be ignored to allow future extensions. Type Example Comment TCP tcp:127.0.0.1:12345 hostname/IP address and port UNIX socket unix:/run/org.example.ftl UNIX abstract namespace socket unix:@org.example.ftl device node device:/dev/org.kernel.example See the transport screenshot for examples. Activation A listen socket a.k.a FD3 might be passed to a varlink service at startup. Environment Example Comment LISTEN_FDS LISTEN_FDS=2 Number of file descriptors passed to the service. It always starts at 3. If more than one file descriptor is passed, LISTEN_FDNAMES identifies the varlink file descriptor. LISTEN_PID LISTEN_PID=4711 The process id of the started service. This should match the PID of the started service, otherwise everything should be ignored. LISTEN_FDNAMES LISTEN_FDNAMES=other:varlink A colon separated list of names of the passed file descriptors. The varlink file descriptor should be named varlink. The service activator should pass the command line option varlink=ADDRESS to the service, to make the listening address and possible parameters known to the service. It also shows the listening address in ps, which helps to debug a running service. See an example implementation of a service activator. Resolver Public varlink interfaces are registered system-wide by their well-known address, by default /run/org.varlink.resolver. The resolver translates a given varlink interface to the service address which provides this interface. Multiple services can implement and offer the same interface, but only one of the services is registered with the resolver. The set of registered interfaces becomes the globally visible system interface, its actual configuration is usually defined by the operating system and not managed by the services themselves. See the org.varlink.resolver interface for details. Bridge The varlink command line tool supports a bridge mode to bridge a single connection to the resolver and its registered services. It intercepts the calls to the org.varlink.service interface and replies with the information the resolver supplies. If the bridge is used over SSH, all the interfaces of the locally running services appear to the remote ssh client as if they were implemented by the bridge. See a screenshot as an example. ",
        "So... it looks like this is an RPC protocol, similar to JSON-RPC, but the word \"RPC\" doesn't appear anywhere on the homepage or the FAQ. And it has an interface description language, similar to Protobuf.<p>It seems like it might be a nice middle ground between Protobuf (strongly-typed, specified) and JSON-RPC (human-readable, doesn't need a special library). But requiring clients to use a new RPC format is still a tough sell. What are the main advantages of Varlink over more mainstream RPC formats?",
        "This article gives some nice background.<p><a href=\"https://lwn.net/Articles/742675/\" rel=\"nofollow\">https://lwn.net/Articles/742675/</a><p>It would be interesting for the shell to open a varlink connection to commands to allow progress updates. I could also see shells command completion interface being greatly simplified by using varlink."
      ],
      "relevant": "true"
    },
    {
      "id": 21240641,
      "title": "Show HN: Hyperfine – a command-line benchmarking tool",
      "search": [
        "Show HN: Hyperfine – a command-line benchmarking tool",
        "https://github.com/sharkdp/hyperfine",
        " A command-line benchmarking tool. Demo: Benchmarking fd and find: Features Statistical analysis across multiple runs. Support for arbitrary shell commands. Constant feedback about the benchmark progress and current estimates. Warmup runs can be executed before the actual benchmark. Cache-clearing commands can be set up before each timing run. Statistical outlier detection to detect interference from other programs and caching effects. Export results to various formats: CSV, JSON, Markdown, AsciiDoc. Parameterized benchmarks (e.g. vary the number of threads). Cross-platform Usage Basic benchmark To run a benchmark, you can simply call hyperfine <command>.... The argument(s) can be any shell command. For example: Hyperfine will automatically determine the number of runs to perform for each command. By default, it will perform at least 10 benchmarking runs. To change this, you can use the -m/--min-runs option: hyperfine --min-runs 5 'sleep 0.2' 'sleep 3.2' Warmup runs and preparation commands If the program execution time is limited by disk I/O, the benchmarking results can be heavily influenced by disk caches and whether they are cold or warm. If you want to run the benchmark on a warm cache, you can use the -w/--warmup option to perform a certain number of program executions before the actual benchmark: hyperfine --warmup 3 'grep -R TODO *' Conversely, if you want to run the benchmark for a cold cache, you can use the -p/--prepare option to run a special command before each timing run. For example, to clear harddisk caches on Linux, you can run sync; echo 3 | sudo tee /proc/sys/vm/drop_caches To use this specific command with Hyperfine, call sudo -v to temporarily gain sudo permissions and then call: hyperfine --prepare 'sync; echo 3 | sudo tee /proc/sys/vm/drop_caches' 'grep -R TODO *' Parameterized benchmarks If you want to run a benchmark where only a single parameter is varied (say, the number of threads), you can use the -P/--parameter-scan option and call: hyperfine --prepare 'make clean' --parameter-scan num_threads 1 12 'make -j {num_threads}' This also works with decimal numbers. The -D/--parameter-step-size option can be used to control the step size: hyperfine --parameter-scan delay 0.3 0.7 -D 0.2 'sleep {delay}' This runs sleep 0.3, sleep 0.5 and sleep 0.7. Shell functions and aliases If you are using bash, you can export shell functions to directly benchmark them with hyperfine: $ my_function() { sleep 1; } $ export -f my_function $ hyperfine my_function If you are using a different shell, or if you want to benchmark shell aliases, you may try to put them in a separate file: echo 'my_function() { sleep 1 }' > /tmp/my_function.sh echo 'alias my_alias=\"sleep 1\"' > /tmp/my_alias.sh hyperfine 'source /tmp/my_function.sh; eval my_function' hyperfine 'source /tmp/my_alias.sh; eval my_alias' Export results Hyperfine has multiple options for exporting benchmark results: CSV, JSON, Markdown (see --help text for details). To export results to Markdown, for example, you can use the --export-markdown option that will create tables like this: Command Mean [s] Min [s] Max [s] Relative find . -iregex '.*[0-9]\\.jpg$' 2.275 0.046 2.243 2.397 9.79 0.22 find . -iname '*[0-9].jpg' 1.427 0.026 1.405 1.468 6.14 0.13 fd -HI '.*[0-9]\\.jpg$' 0.232 0.002 0.230 0.236 1.00 The JSON output is useful if you want to analyze the benchmark results in more detail. See the scripts/ folder for some examples. Installation On Ubuntu Download the appropriate .deb package from the Release page and install it via dpkg: wget https://github.com/sharkdp/hyperfine/releases/download/v1.12.0/hyperfine_1.12.0_amd64.deb sudo dpkg -i hyperfine_1.12.0_amd64.deb On Fedora On Fedora, hyperfine can be installed from the official repositories: On Alpine Linux On Alpine Linux, hyperfine can be installed from the official repositories: On Arch Linux On Arch Linux, hyperfine can be installed from the official repositories: On Funtoo Linux On Funtoo Linux, hyperfine can be installed from core-kit: emerge app-benchmarks/hyperfine On NixOS On NixOS, hyperfine can be installed from the official repositories: On Void Linux Hyperfine can be installed via xbps xbps-install -S hyperfine On macOS Hyperfine can be installed via Homebrew: Or you can install using MacPorts: sudo port selfupdate sudo port install hyperfine On FreeBSD Hyperfine can be installed via pkg: On OpenBSD With conda Hyperfine can be installed via conda from the conda-forge channel: conda install -c conda-forge hyperfine With cargo (Linux, macOS, Windows) Hyperfine can be installed via cargo: Make sure that you use Rust 1.46 or higher. From binaries (Linux, macOS, Windows) Download the corresponding archive from the Release page. Alternative tools Hyperfine is inspired by bench. Integration with other tools Chronologer is a tool that uses hyperfine to visualize changes in benchmark timings across your Git history. Make sure to check out the scripts folder in this repository for a set of tools to work with hyperfine benchmark results. Origin of the name The name hyperfine was chosen in reference to the hyperfine levels of caesium 133 which play a crucial role in the definition of our base unit of time the second. License hyperfine is dual-licensed under the terms of the MIT License and the Apache License 2.0. See the LICENSE-APACHE and LICENSE-MIT files for details. ",
        "I have submitted \"hyperfine\" 1.5 years ago when it just came out. Since then, the program has gained functionality (statistical outlier detection, result export, parametrized benchmarks) and maturity.<p>Old discussion: <a href=\"https://news.ycombinator.com/item?id=16193225\" rel=\"nofollow\">https://news.ycombinator.com/item?id=16193225</a><p>Looking forward to your feedback!",
        "Most -- nearly all -- benchmarking tools like this work from a normality assumption, i.e. assume that results follow the normal distribution, or is close to it. Some do this on blind faith, others argue from the CLT that \"with infinite samples, the mean is normally distributed, so surely it must be also with finite number of samples, at least a little?\"<p>In fact, performance numbers (latencies) often follow a heavy-tailed distribution. For these, you need a literal shitload of samples to get even a slightly normal mean. For these, the sample mean, the sample variance, the sample centiles -- they all severely underestimate the true values.<p>What's worse is when these tools start to remove \"outliers\". With a heavy-tailed distribution, the majority of samples don't contribute very much at all to the expectation. The strongest signal is found in the extreme values. The strongest signal is found in the stuff that is thrown out. The junk that's left is the noise, the stuff that doesn't tell you very much about what you're dealing with.<p>I stand firm in my belief that unless you can prove how CLT applies to your input distributions, you should not assume normality.<p>And if you don't know what you are doing, stop reporting means. Stop reporting centiles. Report the maximum value. That's a really boring thing to hear, but it is nearly always statistically  and analytically meaningful, so it is a good default."
      ],
      "relevant": "true"
    },
    {
      "id": 20449610,
      "title": "Show HN: OctoSQL – Query and join multiple databases and files, written in Go",
      "search": [
        "Show HN: OctoSQL – Query and join multiple databases and files, written in Go",
        "https://github.com/cube2222/octosql",
        "OctoSQL is a query tool that allows you to join, analyse and transform data from multiple databases, streaming sources and file formats using SQL. OctoSQL is currently being rewritten on the redesign branch. Problems OctoSQL Solves You need to join / analyze data from multiple datasources. Think of enriching an Excel file by joining it with a PostgreSQL database. You need stream aggregates over time, with live output updates. Think of a live-updated leaderboard with cat images based on a \"like\" event stream. You need aggregate streams per time window, with live output updates. Think of a unique user count per hour, per country live summary. Table of Contents What is OctoSQL? Installation Quickstart Temporal SQL Features Watermarks Triggers Retractions Example Durability Configuration JSON CSV Excel Parquet PostgreSQL MySQL Redis Kafka Documentation Architecture Datasource Pushdown Operations Roadmap What is OctoSQL? OctoSQL is a SQL query engine which allows you to write standard SQL queries on data stored in multiple SQL databases, NoSQL databases, streaming sources and files in various formats trying to push down as much of the work as possible to the source databases, not transferring unnecessary data. OctoSQL does that by creating an internal representation of your query and later translating parts of it into the query languages or APIs of the source databases. Whenever a datasource doesn't support a given operation, OctoSQL will execute it in memory, so you don't have to worry about the specifics of the underlying datasources. OctoSQL also includes temporal SQL extensions, to operate ergonomically on streams and respect their event-time (not the current system-time when the records are being processed). With OctoSQL you don't need O(n) client tools or a large data analysis system deployment. Everything's contained in a single binary. Why the name? OctoSQL stems from Octopus SQL. Octopus, because octopi have many arms, so they can grasp and manipulate multiple objects, like OctoSQL is able to handle multiple datasources simultaneously. Installation Either download the binary for your operating system (Linux, OS X and Windows are supported) from the Releases page, or install using the go command line tool: GO111MODULE=on go get -u github.com/cube2222/octosql/cmd/octosql Quickstart Let's say we have a csv file with cats, and a redis database with people (potential cat owners). Now we want to get a list of cities with the number of distinct cat names in them and the cumulative number of cat lives (as each cat has up to 9 lives left). First, create a configuration file (Configuration Syntax) For example: dataSources: - name: cats type: csv config: path: \"~/Documents/cats.csv\" - name: people type: redis config: address: \"localhost:6379\" password: \"\" databaseIndex: 0 databaseKeyName: \"id\" Then, set the OCTOSQL_CONFIG environment variable to point to the configuration file. export OCTOSQL_CONFIG=~/octosql.yaml You can also use the --config command line argument. Finally, query to your hearts desire: octosql \"SELECT p.city, FIRST(c.name), COUNT(DISTINCT c.name) cats, SUM(c.livesleft) catlives FROM cats c JOIN people p ON c.ownerid = p.id GROUP BY p.city ORDER BY catlives DESC LIMIT 9\" Example output: +---------+--------------+------+----------+ | p.city | c.name_first | cats | catlives | +---------+--------------+------+----------+ | Warren | Zoey | 68 | 570 | | Gadsden | Snickers | 52 | 388 | | Staples | Harley | 54 | 383 | | Buxton | Lucky | 45 | 373 | | Bethany | Princess | 46 | 366 | | Noxen | Sheba | 49 | 361 | | Yorklyn | Scooter | 45 | 359 | | Tuttle | Toby | 57 | 356 | | Ada | Jasmine | 49 | 351 | +---------+--------------+------+----------+ You can choose between live-table batch-table live-csv batch-csv stream-json output formats. (The live-* types will update the terminal view repeatedly every second, the batch-* ones will write the output once before exiting, the stream-* ones will print records whenever they are available) Temporal SQL Features OctoSQL features temporal SQL extensions inspired by the paper One SQL to Rule Them All. Introduction Often when you're working with streams of events, you'd like to use the time dimension somehow: Calculate average values for a day sliced by hours. Get unique user counts per day. and others All those examples have one thing in common: The time value of an event is crucial for correctness. A naive system could just use the current clock time whenever it receives an event. The correctness of this approach however, degrades quickly in the face of network problems, delivery delays, clock skew. This can be solved by using a value from the event as its time value. A new problem arises though: how do I know that I've received all events up to time X and can publish results for a given hour. You never know if there isn't somewhere a delayed event which should be factored in. This is where watermarks come into play. Watermarks Watermarks are a heuristic which try to approximate the \"current time\" when processing events. Said differently: When I receive a watermark for 12:00 I can be sure enough I've received all events of interest up to 12:00. To achieve this, they are generated at streaming sources and propagate downstream through the whole processing pipeline. The generation of watermarks usually relies on heuristics which provide satisfactory results for our given use case. OctoSQL currently contains the following watermark generators: Maximum difference watermark generator (with an offset argument) With an offset of 10 seconds, this generator says: When I've received an event for 12:00:00, then I'm sure I won't receive any event older than 11:59:50. Percentile watermark generator (with a percentile argument) With a percentile of 99.5, it will look at a specified number of recent events, and generate a watermark so that 99.5% of those events are after the watermark (not yet triggered), and the remaining 0.5% are before it. This way we set the watermark so that only a fraction of the recently seen events is potentially ignored as being late. Watermark generators are specified using table valued functions and are documented in the wiki. Triggers Another matter is triggering of keys in aggregations. Sometimes you'd like to only see the value for a given key (hour) when you know it's done, but othertimes you'd like to see partial results (how's the unique user count going this hour). That's where you can use triggers. Triggers allow you to specify when a given aggregate (or join window for that matter) is emitted or updated. OctoSQL contains multiple triggers: Watermark Trigger This is the most straightforward trigger. It emits a value whenever the watermark for a given key (or the end of the stream) is reached. So basically the \"show me when it's done\". Counting Trigger (with a count argument) This trigger will emit a value for a key every time it receives count records with this key. The count is reset whenever the key is triggered. Delay Trigger (with a delay argument) This trigger will emit a value for a key whenever the key has been inactive for the delay period. You can use multiple triggers simultaneously. (Show me the current sum every 10 received events, but also the final value after having received the watermark.) Retractions A key can be triggered multiple times with partial results. How do we know a given record is a retriggering of some key, and not a new unrelated record? OctoSQL solves this problem using a dataflow-like architecture. This means whenever a new value is sent for a key, a retraction is send for the old value. In practice this means every update is accompanied by the old record with an undo flag set. This can be visible when using a stream-* output format with partial results. Example Now we can see how it all fits together. In this example we have an events file, which contains records about points being scored in a game by multiple teams. WITH with_watermark AS (SELECT * FROM max_diff_watermark(source=>TABLE(events), offset=>INTERVAL 5 SECONDS, time_field=>DESCRIPTOR(time)) e), with_tumble AS (SELECT * FROM tumble(source=>TABLE(with_watermark), time_field=>DESCRIPTOR(e.time), window_length=> INTERVAL 1 MINUTE, offset => INTERVAL 0 SECONDS) e), counts_per_team AS (SELECT e.window_end, e.team, COUNT(*) as goals FROM with_tumble e GROUP BY e.window_end, e.team TRIGGER COUNTING 100, ON WATERMARK) SELECT * FROM counts_per_team cpt ORDER BY cpt.window_end DESC, cpt.goals ASC, cpt.team DESC We use common table expressions to break the query up into multiple stages. First we create the with_watermark intermediate table/stream. Here we use the table valued function max_diff_watermark to add watermarks to the events table - with an offset of 5 seconds based on the time record field. Then we use this intermediate table to create the with_tumble table, where we use the tumble table valued function to add a window_start and window_end field to each record, based on the record's time field. This assigns the records to 1 minute long windows. Next we create the counts_per_team table, which groups the records by their window end and team. Finally, we order those results by window end, goal count and team. Durability OctoSQL in its current design is based on on-disk transactional storage. All state is saved this way. All interactions with datasources are designed so that no records get duplicated in the face of errors or application restarts. You can also kill the OctoSQL process and start it again with the same query and storage-directory (command line argument), it will start where it left off. By default, OctoSQL will create a temporary directory for the state and delete it after termination. Configuration The configuration file has the following form dataSources: - name: <table_name_in_octosql> type: <datasource_type> config: <datasource_specific_key>: <datasource_specific_value> <datasource_specific_key>: <datasource_specific_value> ... - name: <table_name_in_octosql> type: <datasource_type> config: <datasource_specific_key>: <datasource_specific_value> <datasource_specific_key>: <datasource_specific_value> ... ... physical: physical_plan_option: <value> Available OctoSQL-wide configuration options are: physical groupByParallelism: The parallelism of group by's and distinct queries. Will default to the CPU core count of your machine. streamJoinParallelism: The parallelism of streaming joins. Will default to the CPU core count of your machine. execution lookupJoinPrefetchCount: The count of simultaneously processed records in a lookup join. Supported Datasources JSON JSON file in one of the following forms: one record per line, no commas JSON list of records options: path - path to file containing the data, required arrayFormat - if the JSON list of records format should be used, optional: defaults to false batchSize - number of records extracted from json file in one storage transaction, optional: defaults to 1000 CSV CSV file separated using commas. The file may or may not have column names as it's first row. options: path - path to file containing the data, required headerRow - whether the first row of the CSV file contains column names or not, optional: defaults to true separator - columns separator, optional: defaults to \",\" batchSize - number of records extracted from csv file in one storage transaction, optional: defaults to 1000 Excel A single table in an Excel spreadsheet. The table may or may not have column names as it's first row. The table can be in any sheet, and start at any point, but it cannot contain spaces between columns nor spaces between rows. options: path - path to file, required headerRow - does the first row contain column names, optional: defaults to true sheet - name of the sheet in which data is stored, optional: defaults to \"Sheet1\" rootCell - name of cell (i.e \"A3\", \"BA14\") which is the leftmost cell of the first, optional: defaults to \"A1\" timeColumns - a list of columns to parse as datetime values with second precision row, optional: defaults to [] batchSize - number of records extracted from excel file in one storage transaction, optional: defaults to 1000 Parquet A single Parquet file. Nested repeated elements are not supported. Otherwise repeated xor nested elements are supported. Currently unsupported logical types, they will get parsed as the underlying primitive type: - ENUM - TIME with NANOS precision - TIMESTAMP with NANOS precision (both UTC and non-UTC) - INTERVAL - MAP options path - path to file, required batchSize - number of records extracted from parquet file in one storage transaction, optional: defaults to 1000 PostgreSQL Single PostgreSQL database table. options: address - address including port number, optional: defaults to localhost:5432 user - required password - required databaseName - required tableName - required batchSize - number of records extracted from PostgreSQL database in one storage transaction, optional: defaults to 1000 MySQL Single MySQL database table. options: address - address including port number, optional: defaults to localhost:3306 user - required password - required databaseName - required tableName - required batchSize - number of records extracted from MySQL database in one storage transaction, optional: defaults to 1000 Redis Redis database with the given index. Currently only hashes are supported. options: address - address including port number, optional: defaults to localhost:6379 password - optional: defaults to \"\" databaseIndex - index number of Redis database, optional: defaults to 0 databaseKeyName - column name of Redis key in OctoSQL records, optional: defaults to \"key\" batchSize - number of records extracted from Redis database in one storage transaction, optional: defaults to 1000 Kafka Multi-partition kafka topic. optional brokers - list of broker addresses (separately hosts and ports) used to connect to the kafka cluster, optional: defaults to [\"localhost:9092\"] topic - name of topic to read messages from, required partitions - topic partition count, optional: defaults to 1 startOffset - offset from which the first batch of messages will be read, optional: defaults to -1 batchSize - number of records extracted from Kafka in one storage transaction, optional: defaults to 1 json - should the messages be decoded as JSON, optional: defaults to false Documentation Documentation for the available functions: https://github.com/cube2222/octosql/wiki/Function-Documentation Documentation for the available aggregates: https://github.com/cube2222/octosql/wiki/Aggregate-Documentation Documentation for the available triggers: https://github.com/cube2222/octosql/wiki/Trigger-Documentation Documentation for the available table valued functions: https://github.com/cube2222/octosql/wiki/Table-Valued-Functions-Documentation The SQL dialect documentation: TODO ;) in short though: Available SQL constructs: Select, Where, Order By, Group By, Offset, Limit, Left Join, Right Join, Inner Join, Distinct, Union, Union All, Subqueries, Operators, Table Valued Functions, Trigger, Common Table Expressions. Available SQL types: Int, Float, String, Bool, Time, Duration, Tuple (array), Object (e.g. JSON) Describe You can describe the current plan in graphviz format using the -describe flag, like this: octosql \"...\" --describe | dot -Tpng > output.png Architecture An OctoSQL invocation gets processed in multiple phases. SQL AST First, the SQL query gets parsed into an abstract syntax tree. This phase only rules out syntax errors. Logical Plan The SQL AST gets converted into a logical query plan. This plan is still mostly a syntactic validation. It's the most naive possible translation of the SQL query. However, this plan already has more of a map-filter-reduce form. If you wanted to add a new query language to OctoSQL, the only problem you'd have to solve is translating it to this logical plan. Physical Plan The logical plan gets converted into a physical plan. This conversion finds any semantic errors in the query. If this phase is reached, then the input is correct and OctoSQL will be able execute it. This phase already understands the specifics of the underlying datasources. So it's here where the optimizer will iteratively transform the plan, pushing computation nodes down to the datasources, and deduplicating unnecessary parts. The optimizer uses a pattern matching approach, where it has rules for matching parts of the physical plan tree and how those patterns can be restructured into a more efficient version. The rules are meant to be as simple as possible and make the smallest possible changes. For example, pushing filters under maps, if they don't use any mapped variables. This way, the optimizer just keeps on iterating on the whole tree, until it can't change anything anymore. (each iteration tries to apply each rule in each possible place in the tree) This ensures that the plan reaches a local performance minimum, and the rules should be structured so that this local minimum is equal - or close to - the global minimum. (i.e. one optimization, shouldn't make another - much more useful one - impossible) Here is an example diagram of an optimized physical plan: Execution Plan The physical plan gets materialized into an execution plan. This phase has to be able to connect to the actual datasources. It may initialize connections, open files, etc. Stream Starting the execution plan creates a stream, which underneath may hold more streams, or parts of the execution plan to create streams in the future. This stream works in a pull based model. Datasource Pushdown Operations Datasource Equality In > < <= >= MySQL supported supported supported PostgreSQL supported supported supported Redis supported supported scan Kafka scan scan scan Parquet scan scan scan JSON scan scan scan CSV scan scan scan Where scan means that the whole table needs to be scanned for each access. Telemetry OctoSQL sends application telemetry on each run to help us gauge user interest and feature use. This way we know somebody uses our software, feel our work is actually useful and can prioritize features based on actual usefulness. You can turn it off (though please don't) by setting the OCTOSQL_TELEMETRY environment variable to 0. Telemetry is also fully printed in the output log of OctoSQL, if you want to see what precisely is being sent. Roadmap Additional Datasources. SQL Constructs: JSON Query HAVING, ALL, ANY Push down functions, aggregates to databases that support them. An in-memory index to save values of subqueries and save on rescanning tables which don't support a given operation, so as not to recalculate them each time. Runtime statistics Server mode Querying a json or csv table from standard input. Integration test suite Tuple splitter, returning the row for each tuple element, with the given element instead of the tuple. ",
        "Hey, one of the authors here.<p>The motivation behind this project is that I always wanted a simple commandline tool allowing me to join data from different places, without needing to set up stuff like presto or spark. On another hand, I never encountered any tool which allows me to easily query csv and json data using SQL (which at least in my opinion is fairly ergonomic to use).<p>This started as an university project, but we're now continuing it as an open source one, as it's been a great success so far.<p>Anyways, feedback greatly requested and appreciated!",
        "Really cool, thanks for sharing. You all might want to look at Apache Calcite (<a href=\"http://calcite.apache.org/\" rel=\"nofollow\">http://calcite.apache.org/</a>) as well for inspiration, which has similar functionality as a subset of its features!"
      ],
      "relevant": "true"
    },
    {
      "id": 21145755,
      "title": "The Rosie Pattern Language",
      "search": [
        "The Rosie Pattern Language",
        "http://rosie-lang.org/about/",
        "Mission Statement The Rosie Pattern Language (RPL) is intended to replace regular expressions (regex) in situations where: many regex are used in an application, project, or organization; or some regex are used by many people, or by a few people over a long period of time; or regex are used in production systems, where the cost of run-time errors is high. The advantages conferred by RPL in cases 1 and 2 derive from the RPL syntax and expressive power. The syntax resembles a programming language, making expressions easier to read and understand, and compatible with tools like diff. RPL is based on Parsing Expression Grammars, which are more powerful than regex, obviating the need for recursive regex or regex grammars, both of which are ad hoc extensions and are not commonly supported in regex libraries. All three situations listed above involve maintainability. RPL is easier to maintain than regex (which are considered write only by most developers). The Rosie project provides both a command line and REPL for development and debugging. Also, RPL supports executable unit tests, making it possible to: have a suite of regression tests test expressions independent of the code that uses them compile and test expressions at build time, avoiding run-time errors in production Rosie is like regex, but better Rosie is a supercharged alternative to Regular Expressions (regex), matching patterns against any input text. Rosie ships with a standard library of patterns for matching timestamps, network addresses, email addresses, CSV files, JSON, and many more common syntactic forms. Rosie/RPL scales in ways that regex do not Scale to many patterns RPL is readable and maintainable. It is structured like a programming language, so you can: Build complex patterns out of simple ones Write patterns that others can read and understand, using whitespace, comments, and built-in test expressions Create libraries of reusable patterns Import pattern libraries built by other people Scale to big data The Rosie Pattern Engine is small and fast. The entire run-time takes less than 400KB of disk space, and around 20MB of resident memory Basic patterns take linear time to match, whereas most modern regex engines can exponentially backtrack Recursive patterns are available when needed, to match recursive data like html or json Current speed (v1.1.0 release) is approximately 5x faster than the regex-based grok Scale for productivity Rosie is flexible and extensible. Unlike most regex tools, Rosie can generate structured (JSON) output, making its output easy to store or to consume by downstream processes An alternate compressed output format can be selected to reduce data transfer volume The CLI uses different colors for dates, times, network addresses, etc., so that you dont have to read JSON when working interactively Plain text (not JSON) output can be selected when using rosie to replace grep Rosie is extensible with new patterns, libraries, color assignments, output formats, and macros Rosie has an interactive pattern development mode to help write and debug patterns Rosie supports UTF-8 natively, but input text can be in any encoding; Rosie can even handle invalid codepoints gracefully Rosie is released under the MIT license You can download Rosie from gitlab ",
        "If it is so much better and easier to read than regex, why not show any examples? Even the examples page is void of any examples.",
        "I say this so often: \"I can't believe it's $year and we're still using regex!\"  So when I see \"new pattern matching language\" my eyes light up and I run over there -- but there's no example, no docs, even where it says \"examples\", and \"docs\", and it's hard to find anything I can use.  I'm sorry, am I a vulgar entitled twit for not finding the spoon right away?<p>Regex is terrible to read.  Give me an example, things we frequently match for, IP, credit card number, dates ... I read the example for date parsing, I'm ... not sure what the equiv is?  I suggest the authors put up a Rosetta stone of sort, eg. in regex-speak: [0-1][0-9]-[0-3][0-9]-201[0-9], in Rosie-speak: xyz.  What about capture group, that's what makes regex powerful, not just matching, that and the look-ahead look-behind.<p>Meta-comment: regex is buried in everything significant that I work with, it's buried in grep, the language libraries, Splunk.  It's going to be hard to dislodge, there's a deep moat because the tools and common use cases are ugly but well-understood.  Why <i>are</i> regex still being used?  Why has nothing better come along? How would I even regex-match extended Unicode?"
      ],
      "relevant": "false"
    },
    {
      "id": 21674729,
      "title": "HTTPie – A user-friendly CLI HTTP client",
      "search": [
        "HTTPie – A user-friendly CLI HTTP client",
        "https://github.com/jakubroztocil/httpie",
        "HTTPie (pronounced aitch-tee-tee-pie) is a command-line HTTP client. Its goal is to make CLI interaction with web services as human-friendly as possible. HTTPie is designed for testing, debugging, and generally interacting with APIs & HTTP servers. The http & https commands allow for creating and sending arbitrary HTTP requests. They use simple and natural syntax and provide formatted and colorized output. Getting started Installation instructions Full documentation Features Expressive and intuitive syntax Formatted and colorized terminal output Built-in JSON support Forms and file uploads HTTPS, proxies, and authentication Arbitrary request data Custom headers Persistent sessions wget-like downloads See all features Examples Hello World: Custom HTTP method, HTTP headers and JSON data: $ http PUT pie.dev/put X-API-Token:123 name=John Build and print a request without sending it using offline mode: $ http --offline pie.dev/post hello=offline Use GitHub API to post a comment on an Issue with authentication: $ http -a USERNAME POST https://api.github.com/repos/httpie/httpie/issues/83/comments body='HTTPie is awesome! :heart:' See more examples Community & support Visit the HTTPie website for full documentation and useful links. Join our Discord server is to ask questions, discuss features, and for general API chat. Tweet at @httpie on Twitter. Use StackOverflow to ask questions and include a httpie tag. Create GitHub Issues for bug reports and feature requests. Subscribe to the HTTPie newsletter for occasional updates. Contributing Have a look through existing Issues and Pull Requests that you could help with. If you'd like to request a feature or report a bug, please create a GitHub Issue using one of the templates provided. See contribution guide ",
        "Truly one of the best cli tools out there. The api is concise and flexible, the --help flag is easy to read. But most importantly, there's <i>sane</i> defaults.<p><i>defaults to GET request</i><p><pre><code>   http google.com\n</code></pre>\n<i>when json body exists it becomes a POST request</i><p><pre><code>   http google.com user=65</code></pre>",
        "HTTPie is amazing, but I grew tired of it being \"slow\", slower than some of my services response times at least.<p>Migrated to Curlie [1], `alias http=curlie`, and been happy with it since. Same API, better performance and access to full `curl` flags.<p>[1] <a href=\"https://curlie.io\" rel=\"nofollow\">https://curlie.io</a>"
      ],
      "relevant": "false"
    },
    {
      "id": 21454153,
      "title": "Stripe CLI",
      "search": [
        "Stripe CLI",
        "https://stripe.com/blog/stripe-cli",
        "Building and testing a Stripe integration can require frequent switching between the terminal, your code editor, and the Dashboard. Today, were excited to launch the Stripe command-line interface (CLI). It lets you interact with Stripe right from the terminal and makes it easier to build, test, and manage your integration.To start, the CLI will let you test webhooks, tail real-time API logs, and create or update API objects. Heres a preview of some of the features: Simplify webhook setup and testingStripe sends a variety ofwebhooks, which let you listen and respond to specific events programmatically. Runstripe listenwith the CLI to forward webhooks to your local web server during developmentno third-party tunneling tools required. You can also trigger and test specific webhook events withstripe trigger.Debug faster with real-time logsWhile integrating, it can be useful to look at logs to fix any issues. You can now usestripe logs tailto stream API request logs in real time in the terminal in addition to viewing these logs from the Dashboard. Quickly inspect parameters or JSON responses and debug errors as they happen.Speed up common tasks and workflowsYou can now create, retrieve, update, or delete any Stripe object directly from the CLI in both test and live mode. For example, you can usestripe customers createand specify parameters for properties inline.Since you can pipe results into other commands, this can be a simple and powerful way to automate tasks. Heres an example:The above command uses the CLI to list live subscriptions that are past due, pipes the JSON response tojqto extract the customer name and email, and exports the data in CSV format.To see a full list of supported commands, runstripe helpor visitthe docsto learn more.Getting startedThe Stripe CLI natively supportsmacOS, Windows, and Linux. You can also pull ourDocker imageto use in automated testing or a continuous integration setup.Were just getting started and well be adding a lot more features to the CLI. If you have feedback or feature requests, join the conversation onGitHub. You can also runstripe feedbackto share your ideas for which use cases we should tackle next. ",
        "Stripe is by far the best developer experience I’ve had in my career working with third party APIs/services. The attention to detail is just second to none, and documentation is a big part of this.",
        "In the Twilio world, I can do something like this, via the command line and their API:<p>- activate new phone number<p>- send SMS from that number<p>- deactivate number<p>I have not ever done <i>that particular workflow</i>, but I could.<p>Who is the Twilio of payments/fintech wherein I could perform a workflow like this:<p>- generate new CC number in my name<p>- set transaction limit(s) and expiry 10 days from now<p>- (go use that CC in real life)<p>- deactivate the CC<p>or maybe:<p>- disable existing card<p>- reenable existing card<p>I know I could do this if I wrote the API myself and had a big, complicated, sticky relationship with a bank ... but does someone let me do things like this if I am just an end-user (like Twilio does) ?"
      ],
      "relevant": "false"
    },
    {
      "id": 21676256,
      "title": "Open Source Webhook Server",
      "search": [
        "Open Source Webhook Server",
        "https://github.com/adnanh/webhook",
        "What is webhook? webhook is a lightweight configurable tool written in Go, that allows you to easily create HTTP endpoints (hooks) on your server, which you can use to execute configured commands. You can also pass data from the HTTP request (such as headers, payload or query variables) to your commands. webhook also allows you to specify rules which have to be satisfied in order for the hook to be triggered. For example, if you're using Github or Bitbucket, you can use webhook to set up a hook that runs a redeploy script for your project on your staging server, whenever you push changes to the master branch of your project. If you use Mattermost or Slack, you can set up an \"Outgoing webhook integration\" or \"Slash command\" to run various commands on your server, which can then report back directly to you or your channels using the \"Incoming webhook integrations\", or the appropriate response body. webhook aims to do nothing more than it should do, and that is: receive the request, parse the headers, payload and query variables, check if the specified rules for the hook are satisfied, and finally, pass the specified arguments to the specified command via command line arguments or via environment variables. Everything else is the responsibility of the command's author. Hookdoo If you don't have time to waste configuring, hosting, debugging and maintaining your webhook instance, we offer a SaaS solution that has all of the capabilities webhook provides, plus a lot more, and all that packaged in a nice friendly web interface. If you are interested, find out more at hookdoo website. If you have any questions, you can contact us at info@hookdoo.com If you need a way of inspecting, monitoring and replaying webhooks without the back and forth troubleshooting, give Hookdeck a try! Getting started Installation Building from source To get started, first make sure you've properly set up your Go 1.14 or newer environment and then run $ go build github.com/adnanh/webhook to build the latest version of the webhook. Using package manager Snap store Ubuntu If you are using Ubuntu linux (17.04 or later), you can install webhook using sudo apt-get install webhook which will install community packaged version. Debian If you are using Debian linux (\"stretch\" or later), you can install webhook using sudo apt-get install webhook which will install community packaged version (thanks @freeekanayaka) from https://packages.debian.org/sid/webhook Download prebuilt binaries Prebuilt binaries for different architectures are available at GitHub Releases. Configuration Next step is to define some hooks you want webhook to serve. webhook supports JSON or YAML configuration files, but we'll focus primarily on JSON in the following example. Begin by creating an empty file named hooks.json. This file will contain an array of hooks the webhook will serve. Check Hook definition page to see the detailed description of what properties a hook can contain, and how to use them. Let's define a simple hook named redeploy-webhook that will run a redeploy script located in /var/scripts/redeploy.sh. Make sure that your bash script has #!/bin/sh shebang on top. Our hooks.json file will now look like this: [ { \"id\": \"redeploy-webhook\", \"execute-command\": \"/var/scripts/redeploy.sh\", \"command-working-directory\": \"/var/webhook\" } ] NOTE: If you prefer YAML, the equivalent hooks.yaml file would be: - id: redeploy-webhook execute-command: \"/var/scripts/redeploy.sh\" command-working-directory: \"/var/webhook\" You can now run webhook using $ /path/to/webhook -hooks hooks.json -verbose It will start up on default port 9000 and will provide you with one HTTP endpoint http://yourserver:9000/hooks/redeploy-webhook Check webhook parameters page to see how to override the ip, port and other settings such as hook hotreload, verbose output, etc, when starting the webhook. By performing a simple HTTP GET or POST request to that endpoint, your specified redeploy script would be executed. Neat! However, hook defined like that could pose a security threat to your system, because anyone who knows your endpoint, can send a request and execute your command. To prevent that, you can use the \"trigger-rule\" property for your hook, to specify the exact circumstances under which the hook would be triggered. For example, you can use them to add a secret that you must supply as a parameter in order to successfully trigger the hook. Please check out the Hook rules page for detailed list of available rules and their usage. Multipart Form Data webhook provides limited support the parsing of multipart form data. Multipart form data can contain two types of parts: values and files. All form values are automatically added to the payload scope. Use the parse-parameters-as-json settings to parse a given value as JSON. All files are ignored unless they match one of the following criteria: The Content-Type header is application/json. The part is named in the parse-parameters-as-json setting. In either case, the given file part will be parsed as JSON and added to the payload map. Templates webhook can parse the hooks configuration file as a Go template when given the -template CLI parameter. See the Templates page for more details on template usage. Using HTTPS webhook by default serves hooks using http. If you want webhook to serve secure content using https, you can use the -secure flag while starting webhook. Files containing a certificate and matching private key for the server must be provided using the -cert /path/to/cert.pem and -key /path/to/key.pem flags. If the certificate is signed by a certificate authority, the cert file should be the concatenation of the server's certificate followed by the CA's certificate. TLS version and cipher suite selection flags are available from the command line. To list available cipher suites, use the -list-cipher-suites flag. The -tls-min-version flag can be used with -list-cipher-suites. CORS Headers If you want to set CORS headers, you can use the -header name=value flag while starting webhook to set the appropriate CORS headers that will be returned with each response. Interested in running webhook inside of a Docker container? You can use one of the following Docker images, or create your own (please read this discussion): almir/webhook roxedus/webhook thecatlady/webhook Examples Check out Hook examples page for more complex examples of hooks. Guides featuring webhook Plex 2 Telegram by @psyhomb Webhook & JIRA by @perfecto25 Trigger Ansible AWX job runs on SCM (e.g. git) commit by @jpmens Deploy using GitHub webhooks by @awea Setting up Automatic Deployment and Builds Using Webhooks by Will Browning Auto deploy your Node.js app on push to GitHub in 3 simple steps by Karolis Rusenas Automate Static Site Deployments with Salt, Git, and Webhooks by Linode Using Prometheus to Automatically Scale WebLogic Clusters on Kubernetes by Marina Kogan Github Pages and Jekyll - A New Platform for LACNIC Labs by Carlos Martnez Cagnazzo How to Deploy React Apps Using Webhooks and Integrating Slack on Ubuntu by Arslan Ud Din Shafiq Private webhooks by Thomas Adventures in webhooks by Drake GitHub pro tips by Spencer Lyon XiaoMi Vacuum + Amazon Button = Dash Cleaning by c0mmensal Set up Automated Deployments From Github With Webhook by Maxim Orlov VIDEO: Gitlab CI/CD configuration using Docker and adnanh/webhook to deploy on VPS - Tutorial #1 by Yes! Let's Learn Software Engineering Integrate automatic deployment in 20 minutes using webhooks + Nginx setup by Anksus ... Want to add your own? Open an Issue or create a PR :-) Community Contributions See the webhook-contrib repository for a collections of tools and helpers related to webhook that have been contributed by the webhook community. Need help? Check out existing issues to see if someone else also had the same problem, or open a new one. Support active development Sponsors DigitalOcean is a simple and robust cloud computing platform, designed for developers. BrowserStack is a cloud-based cross-browser testing tool that enables developers to test their websites across various browsers on different operating systems and mobile devices, without requiring users to install virtual machines, devices or emulators. Support this project by becoming a sponsor. Your logo will show up here with a link to your website. By contributing This project exists thanks to all the people who contribute. Contribute!. By giving money OpenCollective Backer OpenCollective Sponsor PayPal Patreon Faircode Flattr Thank you to all our backers! License The MIT License (MIT) Copyright (c) 2015 Adnan Hajdarevic adnanh@gmail.com Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. ",
        "Interesting, could be useful to interface some of my stuff with IFTTT or Integromat.",
        "I have been using this webhook server in prod for a few years and its been easy to setup/maintain ... you just define  github.com repo to publish `git push` or whatever then this webhook server listens to every git push my team makes to launch a code recompile/redeploy ...  foolproof and solid ... I highly recommend"
      ],
      "relevant": "false"
    }
  ]
}
