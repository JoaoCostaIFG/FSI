{
  "docs": [
    {
      "id": 20559240,
      "title": "Learn a little jq, Awk and sed",
      "search": [
        "Learn a little jq, Awk and sed",
        "https://letterstoanewdeveloper.com/2019/07/29/learn-a-little-jq-awk-and-sed/",
        "Dear new developer, You are probably going to be dealing with text files sometime during your development career. These could be plain text, csv, or json. They may have data you want to get out, or log files you want to examine. You may be transforming from one format to another. Now, if this is a regular occurrence, you may want to build a script or a program around this problem (or use a third party service which aggregates everything together). But sometimes these files are one offs. Or you use them once in a blue moon. And it can take a little while to write a script, look at the libraries, and put it all together. Another alternative is to learn some of the unix tools available on the command line. Here are three that I consider table stakes. awk This is a multi purpose line processing utility. I often want to grab lines of a log file and figure out what is going on. Heres a few lines of a log file: 54.147.20.92 - - [26/Jul/2019:20:21:04 -0600] \"GET /wordpress HTTP/1.1\" 301 241 \"-\" \"Slackbot 1.0 (+https://api.slack.com/robots)\" 185.24.234.106 - - [26/Jul/2019:20:20:50 -0600] \"GET /wordpress/archives/date/2004/02 HTTP/1.1\" 200 87872 \"http://www.mooreds.com\" \"DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)\" 185.24.234.106 - - [26/Jul/2019:20:20:50 -0600] \"GET /wordpress/archives/date/2004/08 HTTP/1.1\" 200 81183 \"http://www.mooreds.com\" \"DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)\" If I want to see only the ip addresses (assuming these are all in a file called logs.txt), Id run something like: $ awk '{print $1}' logs.txt 54.147.20.92 185.24.234.106 185.24.234.106 Theres lots more, but you can see that youd be able to slice and dice delimited data pretty easily. Heres a great article which dives in further. sed This is another line utility. You can use it for all kinds of things, but I primarily use it to do search and replace on a file. Suppose you had the same log file, but you wanted to anonymize the the ip address and the user agent. Perhaps youre going to ship them off for long term storage or something. You can easily remove this with a couple of sed commands. $ sed 's/^[^ ]*//' logs.txt |sed 's/\"[^\"]*\"$//' - - [26/Jul/2019:20:21:04 -0600] \"GET /wordpress HTTP/1.1\" 301 241 \"-\" - - [26/Jul/2019:20:20:50 -0600] \"GET /wordpress/archives/date/2004/02 HTTP/1.1\" 200 87872 \"http://www.mooreds.com\" - - [26/Jul/2019:20:20:50 -0600] \"GET /wordpress/archives/date/2004/08 HTTP/1.1\" 200 81183 \"http://www.mooreds.com\" Yes, it looks like line noise, but this is the power of regular expressions. Theyre in every language (though with slight variations) and worth learning. sed gives you the power of regular expressions at the command line for processing files. I dont have a great sed tutorial Ive found, but googling shows a number. jq If you work on the command line with modern software at all, you have encountered json. Its used for configuration files and data transmission. Sometimes you get an array of json and you just want to pick out certain attributes of it. Tools like sed and awk fail at this, because they are used to newlines separating records, not curly braces and commas. Sure, you could use regular expressions to parse simple json, and there are times when Ive done this. But a far better tool is jq. Im not as savvy with this as with the others, but have used it whenever Im dealing with an API that delivers json (which is most modern ones). I can pull the API down with curl (another great tool) and parse it out with jq. I can put these all in a script and have the exploration be repeatable. I did this a few months ago when I was doing some exploration of an elastic search system. I crafted the queries with curl and then used jq to parse out the results so that I could make some sense of this. Yes, I could have done this with a real programming language, but it would have taken longer. I could also have used a gui tool like postman, but then it would not have been replicable. sed and awk should be on every system you run across; jq is non standard, but easy to install. Its worth spending some time getting to know these tools. So next time you are processing a text file and need to extract just a bit of it, reach for sed and awk. Next time you get a hairy json file and you are peering at it, look at jq. I think youll be happy with the result. Sincerely, Dan Published July 29, 2019October 17, 2020 ",
        "When I first saw someone using zsh (omz), I was awe-struck.<p>Same thing happens to the person sitting next to me when I pipe an output to jq.",
        "however, I find jq not so friendly piping its output to other programs."
      ],
      "relevant": "true"
    },
    {
      "id": 18937195,
      "title": "Gunk: Modern front end and syntax for Protocol Buffers",
      "search": [
        "Gunk: Modern front end and syntax for Protocol Buffers",
        "https://github.com/gunk/gunk",
        "Gunk is a modern frontend and syntax for Protocol Buffers. Quickstart | Installing | Syntax | Configuring | About | Releases Overview Gunk provides a modern project-based workflow along with a Go-derived syntax for defining types and services for use with Protocol Buffers. Gunk is designed to integrate cleanly with existing protoc based build pipelines, while standardizing workflows in a way that is familiar/accessible to Go developers. Quickstart Create a working directory for a project: $ mkdir -p ~/src/example && cd ~/src/example Install gunk and place the following Gunk definitions in example/util.gunk: package util // Util is a utility service. type Util interface { // Echo returns the passed message. Echo(Message) Message } // Message contains an echo message. type Message struct { // Msg is a message from a client. Msg string `pb:\"1\"` } Create the corresponding project configuration in example/.gunkconfig: [generate go] [generate js] import_style=commonjs binary Then, generate protocol buffer definitions/code: $ ls -A .gunkconfig util.gunk $ gunk generate $ ls -A all.pb.go all_pb.js .gunkconfig util.gunk As seen above, gunk generated the corresponding Go and JavaScript protobuf code using the options defined in the .gunkconfig. End-to-end Example A end-to-end example gRPC server implementation, using Gunk definitions is available for review. Debugging protoc commands Underlying commands executed by gunk can be viewed with the following: $ gunk generate -x protoc-gen-go protoc --js_out=import_style=commonjs,binary:/home/user/example --descriptor_set_in=/dev/stdin all.proto Installing The gunk command-line tool can be installed via Release, via Homebrew, via Scoop or via Go: Installing via Release Download a release for your platform Extract the gunk or gunk.exe file from the .tar.bz2 or .zip file Move the extracted executable to somewhere on your $PATH (Linux/macOS) or %PATH% (Windows) Installing via Homebrew (macOS) gunk is available in the gunk/gunk tap, and can be installed in the usual way with the brew command: # add tap $ brew tap gunk/gunk # install gunk $ brew install gunk Installing via Scoop (Windows) gunk can be installed using Scoop: # install scoop if not already installed iex (new-object net.webclient).downloadstring('https://get.scoop.sh') scoop install gunk Installing via Go gunk can be installed in the usual Go fashion: # install gunk $ go get -u github.com/gunk/gunk Protobuf Dependency and Caching The gunk command-line tool uses the protoc command-line tool. gunk can be configured to use protoc at a specified path. If it isn't available, gunk will download the latest protobuf release to the user's cache, for use. It's also possible to pin a specific version, see the section on protoc configuration. Protocol Types and Messages Gunk provides an alternate, Go-derived syntax for defining protocol buffers. As such, Gunk definitions are a subset of the Go programming language. Additionally, a special +gunk annotation is recognized by gunk, to allow the declaration of protocol buffer options: package message import \"github.com/gunk/opt/http\" // Message is a Echo message. type Message struct { // Msg holds a message. Msg string `pb:\"1\" json:\"msg\"` Code int `pb:\"2\" json:\"code\"` } // Util is a utility service. type Util interface { // Echo echoes a message. // // +gunk http.Match{ // Method: \"POST\", // Path: \"/v1/echo\", // Body: \"*\", // } Echo(Message) Message } Technically speaking, gunk is not actually strict subset of go, as gunk allows unused imports; it actually requires them for some features. See the example above; in pure go, this would not be a valid go code, as http is not used outside of the comment. Scalars Gunk's Go-derived syntax uses the canonical Go scalar types of the proto3 syntax, defined by the protocol buffer project: Proto3 Type Gunk Type double float64 float float32 int32 int int32 int32 int64 int64 uint32 uint uint32 uint32 uint64 uint64 bool bool string string bytes []byte Note: Variable-length scalars will be enabled in the future using a tag parameter. Messages Gunk's Go-derived syntax uses Go's struct type declarations for declaring messages, and require a pb:\"<field_number>\" tag to indicate the field number: type Message struct { FieldA string `pb:\"1\"` } type Envelope struct { Message Message `pb:\"1\" json:\"msg\"` } There are additional tags (for example, the json: tag above), that will be recognized by gunk format, and passed on to generators, where possible. Note: When using gunk format, a valid pb:\"<field_number>\" tag will be automatically inserted if not declared. Services Gunk's Go-derived syntax uses Go's interface syntax for declaring services: type SearchService interface { Search(SearchRequest) SearchResponse } The above is equivalent to the following protobuf syntax: service SearchService { rpc Search (SearchRequest) returns (SearchResponse); } Enums Gunk's Go-derived syntax uses Go const's for declaring enums: type MyEnum int const ( MYENUM MyEnum = iota MYENUM2 ) Note: values can also be fixed numeric values or a calculated value (using iota). Maps Gunk's Go-derived syntax uses Go map's for declaring map fields: type Project struct { ProjectID string `pb:\"1\" json:\"project_id\"` } type GetProjectResponse struct { Projects map[string]Project `pb:\"1\"` } Repeated Values Gunk's Go-derived syntax uses Go's slice syntax ([]) for declaring a repeated field: type MyMessage struct { FieldA []string `pb:\"1\"` } Message Streams Gunk's Go-derived syntax uses Go chan syntax for declaring streams: type MessageService interface { List(chan Message) chan Message } The above is equivalent to the following protobuf syntax: service MessageService { rpc List(stream Message) returns (stream Message); } Protocol Options Protocol buffer options are standard messages (ie, a struct), and can be attached to any service, message, enum, or other other type declaration in a Gunk file via the doccomment preceding the type, field, or service: // MyOption is an option. type MyOption struct { Name string `pb:\"1\"` } // +gunk MyOption { // Name: \"test\", // } type MyMessage struct { /* ... */ } Project Configuration Files Gunk uses a top-level .gunkconfig configuration file for managing the Gunk protocol definitons for a project: # Example .gunkconfig for Go, grpc-gateway, Python and JS [generate go] out=v1/go plugins=grpc [generate] out=v1/go command=protoc-gen-grpc-gateway logtostderr=true [generate python] out=v1/python [generate js] out=v1/js import_style=commonjs binary Project Search Path When gunk is invoked from the command-line, it searches the passed package spec (or current working directory) for a .gunkconfig file, and walks up the directory hierarchy until a .gunkconfig is found, or the project's root is encountered. The project root is defined as the top-most directory containing a .git subdirectory, or where a go.mod file is located. Format The .gunkconfig file format is compatible with Git config syntax, and in turn is compatible with the INI file format: [generate] command=protoc-gen-go [generate] out=v1/js protoc=js Global section import_path - see \"Converting Existing Protobuf Files\" strip_enum_type_names - with this option on, enums with their type prefixed will be renamed to the version without prefix. Note that this might produce invalid protobuf that stops compiling in 1.4.* protoc-gen-go, if the enum names clash. Section [protoc] The path where to check for (or where to download) the protoc binary can be configured. The version can also be pinned. Parameters version - the version of protoc to use. If unspecified, defaults to the latest release available. Otherwise, gunk will either download the specified version, or check that the version of protoc at the specified path matches what was configured. path - the path to check for the protoc binary. If unspecified, defaults appropriate user cache directory for the user's OS. If no file exists at the path, gunk will attempt to download protoc. Section [generate[ <type>]] Each [generate] or [generate <type>] section in a .gunkconfig corresponds to a invocation of the protoc-gen-<type> tool. Parameters Each name[=value] parameter defined within a [generate] section will be passed as a parameter to the protoc-gen-<type> tool, with the exception of the following special parameters that override the behavior of the gunk generate tool: command - overrides the protoc-gen-* command executable used by gunk generate. The executable must be findable on $PATH (Linux/macOS) or %PATH% (Windows), or may be the full path to the executable. If not defined, then command will be protoc-gen-<type>, when <type> is the value in [generate <type>]. protoc - overrides the <type> value, causing gunk generate to use the protoc value in place of <type>. out - overrides the output path of protoc. If not defined, output will be the same directory as the location of the .gunk files. plugin_version - specify version of plugin. The plugin is downloaded from github/maven, built in cache and used. It is not installed in $PATH. This currently works with the following plugins: protoc-gen-go protoc-gen-grpc-java protoc-gen-grpc-gateway protoc-gen-openapiv2 (protoc-gen-swagger support is deprecated) protoc-gen-swift (installing swift itself first is necessary) protoc-gen-grpc-swift (installing swift itself first is necessary) protoc-gen-ts (installing node and npm first is necessary) protoc-gen-grpc-python (cmake, gcc is necessary; takes ~10 minutes to clone build) It is recommended to use this function everywhere, for reproducible builds, together with version for protoc. json_tag_postproc - uses json tags defined in gunk file also for go-generated file fix_paths_postproc - for js and ts - by default, gunk generates wrong paths for other imported gunk packages, because of the way gunk moves files around. Works only if js also has import_style=commonjs option. All other name[=value] pairs specified within the generate section will be passed as plugin parameters to protoc and the protoc-gen-<type> generators. Short Form The following .gunkconfig: [generate go] [generate js] out=v1/js is equivalent to: [generate] command=protoc-gen-go [generate] out=v1/js protoc=js Different forms of invocation There are three different forms of gunkconfig sections that have three different semantics. [generate] command=protoc-gen-go [generate] protoc=go [generate go] The first one uses protoc-gen-go plugin directly, without using protoc. It also attempts to move files to the same directory as the gunk file. The second one uses protoc and does not attempt to move any files. Protoc attempts to load plugin from $PATH, if it is not one of the built-in protoc plugins; this will not work together with pinned version and other gunk features and is not recommended outside of built-in protoc generators. The third version is reccomended. It will try to detect whether language is one of built-in protoc generators, in that case behaves like the second way, otherwise behaves like the first. The built-in protoc generators are: cpp java python php ruby csharp objc js Third-Party Protobuf Options Gunk provides the +gunk annotation syntax for declaring protobuf options, and specially recognizes some third-party API annotations, such as Google HTTP options, including all builtin/standard protoc options for code generation: // +gunk java.Package(\"com.example.message\") // +gunk java.MultipleFiles(true) package message import ( \"github.com/gunk/opt/http\" \"github.com/gunk/opt/file/java\" ) type Util interface { // +gunk http.Match{ // Method: \"POST\", // Path: \"/v1/echo\", // Body: \"*\", // } Echo() } Further documentation on available options can be found at the Gunk options project. Formatting Gunk Files Gunk provides the gunk format command to format .gunk files (akin to gofmt): $ gunk format /path/to/file.gunk $ gunk format <pathspec> Converting Existing Protobuf Files Gunk provides the gunk convert command that will converting existing .proto files (or a directory) to the Go-derived Gunk syntax: $ gunk convert /path/to/file.proto $ gunk convert /path/to/protobuf/directory If your .proto is referencing another .proto from another directory, you can add import_path in the global section of your .gunkconfig. If you don't provide import_path it will only search in the root directory. import_path=relative/path/to/protobuf/directory The path to provide is relative from the .gunkconfig location. Furthermore, the referenced files must contain: option go_package=\"path/of/go/package\"; The resulting .gunk file will contain the import path as defined in go_package: import ( name \"path/of/go/package\" ) About Gunk is developed by the team at Brankas, and was designed to streamline API design and development. History From the beginning of the company, the Brankas team defined API types and services in .proto files, leveraging ad-hoc Makefile's, shell scripts, and other non-standardized mechanisms for generating Protocol Buffer code. As development exploded in 2017 (and beyond) with continued addition of backend microservices/APIs, more code repositories and projects, and team members, it became necessary to standardize tooling for the organization as well as reduce the cognitive load of developers (who for the most part were working almost exclusively with Go) when declaring gRPC and REST services. Naming The Gunk name has a cheeky, backronym \"Gunk Unified N-terface Kompiler\", however the name was chosen because it was possible to secure the GitHub gunk project name, was short, concise, and not used by other projects. Additionally, \"gunk\" is an apt description for the \"gunk\" surrounding protocol definition, generation, compilation, and delivery. Contributing Issues, Pull Requests, and other contributions are greatly welcomed and appreciated! Get started with building and running gunk: # clone source repository $ git clone https://github.com/gunk/gunk.git && cd gunk # force GO111MODULES $ export GO111MODULE=on # build and run $ go build && ./gunk Dependency Management Gunk uses Go modules for dependency management, and as such requires Go 1.11+. Please run go mod tidy before submitting any PRs: $ export GO111MODULE=on $ cd gunk && go mod tidy ",
        "I'm not seeing what makes this \"modern\". proto3 is only a few years old and nothing about it strikes me as unusually archaic. Protobuf in general isn't that much older than Go. I can see why Go-compatible syntax would be attractive to Go developers, so maybe that should be in the description rather than \"modern\"?",
        "Not sure why I’d want to define a language independent interchange format in a language specific way and remove all of the tooling help at the same time. Why is this better? A why section/motivations would help greatly."
      ],
      "relevant": "false"
    },
    {
      "id": 18895856,
      "title": "Writing custom tools with Swift",
      "search": [
        "Writing custom tools with Swift",
        "https://paul-samuels.com/blog/2019/01/12/writing-custom-tools-with-swift/",
        "12 Jan 2019I write a lot of custom command line tools that live alongside my projects. The tools vary in complexity and implementation. From simplest to most involved heres my high level implementation strategy: A single file containing a shebang #!/usr/bin/swift. A Swift Package Manager project of type executable. A Swift Package Manager project of type executable that builds using sources from the main project (Ive written about this here). The same as above but special care has been taken to ensure that the tool can be dockerized and run on Linux. The hardest part with writing custom tools is knowing how to get started, this post will run through creating a single file tool. Problem Outline Lets imagine that we want to grab our most recent app store reviews, get a high level overview of star distribution of the recent reviews and look at any comments that have a rating of 3 stars or below. Skeleton Lets start by making sure we can get an executable Swift file. In your terminal you can do the following: echo '#!/usr/bin/swift\\nprint(\"It works!!\")' > reviews chmod u+x reviews ./reviews The result will be It works!! The first line is equivalent to just creating a file called reviews with the following contents #!/usr/bin/swift print(\"It works!!\") Its not the most exciting file but its good enough to get us rolling. The next command chmod u+x reviews makes the file executable and finally we execute it with ./reviews. Now that we have an executable file lets figure out what our data looks like. Source data Before we progress with writing the rest of the script we need to figure out how to get the data, Im going to do this using curl and jq. This is a useful step because it helps me figure out what the structure of the data is and allows me to experiment with the transformations that I need to apply in my tool. First lets checkout the URL that I grabbed from Stack Overflow (for this example Im just using the Apple Support apps id for reviews): curl \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\" To see how this looks I can pretty print it by piping it through jq: curl \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\" \\ | jq . Response structure ``` { \"feed\": { \"author\": { \"name\": { \"label\": \"...\" }, \"uri\": { \"label\": \"...\" } }, \"entry\": [ { \"author\": { \"uri\": { \"label\": \"...\" }, \"name\": { \"label\": \"...\" }, \"label\": \"\" }, \"im:version\": { \"label\": \"...\" }, \"im:rating\": { \"label\": \"...\" }, \"id\": { \"label\": \"...\" }, \"title\": { \"label\": \"...\" }, \"content\": { \"label\": \"...\", \"attributes\": { \"type\": \"text\" } }, \"link\": { \"attributes\": { \"rel\": \"related\", \"href\": \"...\" } }, \"im:voteSum\": { \"label\": \"...\" }, \"im:contentType\": { \"attributes\": { \"term\": \"Application\", \"label\": \"Application\" } }, \"im:voteCount\": { \"label\": \"...\" } } ], \"updated\": { \"label\": \"...\" }, \"rights\": { \"label\": \"...\" }, \"title\": { \"label\": \"...\" }, \"icon\": { \"label\": \"...\" }, \"link\": [ { \"attributes\": { \"rel\": \"...\", \"type\": \"text/html\", \"href\": \"...\" } }, { \"attributes\": { \"rel\": \"self\", \"href\": \"...\" } }, { \"attributes\": { \"rel\": \"first\", \"href\": \"...\" } }, { \"attributes\": { \"rel\": \"last\", \"href\": \"...\" } }, { \"attributes\": { \"rel\": \"previous\", \"href\": \"...\" } }, { \"attributes\": { \"rel\": \"next\", \"href\": \"...\" } } ], \"id\": { \"label\": \"...\" } } } ``` Looking at the structure I can see that the data I really care about is under feed.entry so I update my jq filter to scope the data a little better: curl \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\" \\ | jq '.feed.entry' Response structure ``` [ { \"author\": { \"uri\": { \"label\": \"...\" }, \"name\": { \"label\": \"...\" }, \"label\": \"\" }, \"im:version\": { \"label\": \"...\" }, \"im:rating\": { \"label\": \"...\" }, \"id\": { \"label\": \"...\" }, \"title\": { \"label\": \"...\" }, \"content\": { \"label\": \"...\", \"attributes\": { \"type\": \"text\" } }, \"link\": { \"attributes\": { \"rel\": \"related\", \"href\": \"...\" } }, \"im:voteSum\": { \"label\": \"...\" }, \"im:contentType\": { \"attributes\": { \"term\": \"Application\", \"label\": \"Application\" } }, \"im:voteCount\": { \"label\": \"...\" } } ] ``` Finally I pull out the fields that I feel will be important for the tool we are writing: curl \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\" \\ | jq '.feed.entry[] | {title: .title.label, rating: .\"im:rating\".label, comment: .content.label}' Response structure ``` [ { \"title\" : \"...\", \"rating\" : \"...\", \"comment\" : \"...\" } ] ``` This is a really fast way of experimenting with data and as well see later its helpful when we come to write the Swift code. The result of the jq filter above is that the large feed will be reduced down to an array of objects with just the title, rating and comment. At this point Im feeling pretty confident that I know what my data will look like so I can go ahead and write this in Swift. Network Request in swift Well use URLSession to make our request - a first attempt might look like: 1 2 3 4 5 6 7 8 #!/usr/bin/swift import Foundation let url = URL(string: \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\")! URLSession.shared.dataTask(with: url, completionHandler: { data, response, error in print(response as Any) }).resume() 2 we need to import Foundation in order to use URLSession and URL. 6 well use the default session as we dont need anything custom. 7 to start well just print anything to check this works. 8 lets not forget to resume the task or nothing will happen. Taking the above we can return to terminal and run ./reviews. nothing happened. The issue here is that dataTask is an asynchronous operation and our script will exit immediately without waiting for the completion to be called. Modifying the code to call dispatchMain() at the end resolves this: #!/usr/bin/swift import Foundation let url = URL(string: \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\")! URLSession.shared.dataTask(with: url, completionHandler: { data, response, error in print(response as Any) }).resume() dispatchMain() Heading back to terminal and running ./reviews we should get some output like Optional(41678 bytes) but weve also introduced a new problem - the programme didnt terminate. Lets fix this and then we can crack on with the rest of our tasks: 1 2 3 4 5 6 7 8 9 10 11 #!/usr/bin/swift import Foundation let url = URL(string: \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\")! URLSession.shared.dataTask(with: url, completionHandler: { data, response, error in print(response as Any) exit(EXIT_SUCCESS) }).resume() dispatchMain() On line 8 Ive added an exit, well provide different exit codes later on depending on whether the tool succeeded or not. To prepare for the next steps well just add some error handling: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 #!/usr/bin/swift import Foundation let url = URL(string: \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\")! URLSession.shared.dataTask(with: url, completionHandler: { data, response, error in if let error = error { print(error.localizedDescription) exit(EXIT_FAILURE) } guard let httpResponse = response as? HTTPURLResponse, 200..<300 ~= httpResponse.statusCode else { print(\"Invalid response \\(String(describing: response))\") exit(EXIT_FAILURE) } if let data = data, data.count > 0 { print(data as Any) exit(EXIT_SUCCESS) } else { print(\"No data!!\") exit(EXIT_FAILURE) } }).resume() dispatchMain() Lines 7-10 are covering cases where there is a failure at the task level. Lines 12-15 are covering errors at the http level. Lines 20-23 are covering cases where there is no data returned. The happy path is hidden in lines 17-20. Side note: Depending on the usage of your scripts you may choose to tailor the level of error reporting and decide if things like force unwraps are acceptable. I tend to find its worth putting error handling in as Ill rarely look at this code, so when it goes wrong it will be a pain to debug without some guidance. Parsing the data We can look at the jq filter we created earlier to guide us on what we need to build. jq .feed.entry[] | {title: .title.label, rating: .\"im:rating\".label, comment: .content.label} We need to dive into the JSON through feed and entry - we can do this by mirroring this structure and using Swifts Decodable: struct Response: Decodable { let feed: Feed struct Feed: Decodable { let entry: [Entry] struct Entry: Decodable { } } } In order to decode an Entry well provide a custom implementation of init(from:) - this will allow us to flatten the data e.g. instead of having entry.title.label we end up with just entry.title. We can do this with the following: struct Entry: Decodable { let comment: String let rating: Int let title: String init(from decoder: Decoder) throws { let container = try decoder.container(keyedBy: CodingKeys.self) comment = try container.nestedContainer(keyedBy: CodingKeys.self, forKey: .comment).decode(String.self, forKey: .label) rating = Int(try container.nestedContainer(keyedBy: CodingKeys.self, forKey: .rating).decode(String.self, forKey: .label))! title = try container.nestedContainer(keyedBy: CodingKeys.self, forKey: .title).decode(String.self, forKey: .label) } private enum CodingKeys: String, CodingKey { case comment = \"content\" case rating = \"im:rating\" case title case label } } With this done we can wire it all up - well go back to the happy path and add: do { print(try JSONDecoder().decode(Response.self, from: data)) exit(EXIT_SUCCESS) } catch { print(\"Failed to decode - \\(error.localizedDescription)\") exit(EXIT_FAILURE) } Thats the complicated stuff out of the way - the next part is the data manipulation that makes the tool actually useful. Processing the data Lets start by printing a summary of the different star ratings. The high level approach will be to loop over all the reviews and keep a track of how many times each star rating was used. Well then return a string that shows the rating number and then an asterisk to represent the number of ratings. func ratings(entries: [Response.Feed.Entry]) -> String { let countedSet = NSCountedSet() entries.forEach { countedSet.add($0.rating) } return (countedSet.allObjects as! [Int]) .sorted(by: >) .reduce(into: \"\") { result, key in result.append(\"\\(key): \\(String(repeating: \"*\", count: countedSet.count(for: key)))\\n\") } } This will yield output like: 5: ***************** 4: ** 3: * 2: **** 1: ************************** The other task we wanted to do was print all the comments that had a rating of 3 or less. This is the simpler of the two tasks as we just need to filter the entries and then format for printing: func reviews(entries: [Response.Feed.Entry]) -> String { return entries .filter { $0.rating <= 3 } .map({ \"\"\" (\\($0.rating)) - \\($0.title) > \\($0.comment) \"\"\" }).joined(separator: \"\\n\\n-\\n\\n\") } This will yield output like: (3) - Love it > This is my favourite app. Putting it all together we end up with: #!/usr/bin/swift import Foundation struct Response: Decodable { let feed: Feed struct Feed: Decodable { let entry: [Entry] struct Entry: Decodable { let comment: String let rating: Int let title: String init(from decoder: Decoder) throws { let container = try decoder.container(keyedBy: CodingKeys.self) comment = try container.nestedContainer(keyedBy: CodingKeys.self, forKey: .comment).decode(String.self, forKey: .label) rating = Int(try container.nestedContainer(keyedBy: CodingKeys.self, forKey: .rating).decode(String.self, forKey: .label))! title = try container.nestedContainer(keyedBy: CodingKeys.self, forKey: .title).decode(String.self, forKey: .label) } private enum CodingKeys: String, CodingKey { case comment = \"content\" case rating = \"im:rating\" case title case label } } } } func ratings(entries: [Response.Feed.Entry]) -> String { let countedSet = NSCountedSet() entries.forEach { countedSet.add($0.rating) } return (countedSet.allObjects as! [Int]) .sorted(by: >) .reduce(into: \"\") { result, key in result.append(\"\\(key): \\(String(repeating: \"*\", count: countedSet.count(for: key)))\\n\") } } func reviews(entries: [Response.Feed.Entry]) -> String { return entries .filter { $0.rating <= 3 } .map({ \"\"\" (\\($0.rating)) - \\($0.title) > \\($0.comment) \"\"\" }).joined(separator: \"\\n\\n-\\n\\n\") } let url = URL(string: \"https://itunes.apple.com/gb/rss/customerreviews/id=1130498044/sortBy=mostRecent/json\")! URLSession.shared.dataTask(with: url, completionHandler: { data, response, error in if let error = error { print(error.localizedDescription) exit(EXIT_FAILURE) } guard let httpResponse = response as? HTTPURLResponse, 200..<300 ~= httpResponse.statusCode else { print(\"Invalid response \\(String(describing: response))\") exit(EXIT_FAILURE) } if let data = data, data.count > 0 { do { let entries = try JSONDecoder().decode(Response.self, from: data).feed.entry print(ratings(entries: entries)) print() print(reviews(entries: entries)) exit(EXIT_SUCCESS) } catch { print(\"Failed to decode - \\(error.localizedDescription)\") exit(EXIT_FAILURE) } } else { print(\"No data!!\") exit(EXIT_FAILURE) } }).resume() dispatchMain() Conclusion Creating tools is a lot of fun and isnt as scary as it might seem at first. Weve done networking, data parsing and some data munging all in one file with not too much effort, which is very rewarding. The single file approach is probably best for shorter tasks. In the example above its already becoming unwieldy and it would be worth considering moving to a Swift Package Manager tool (maybe thats a future post). ",
        "Swift tries to be a jack of all trades, but I think as a scripting language (as in this example) it falls short. Having to start a run loop, write asynchronous callbacks (completion handlers), and implement custom JSON decoders just to make a web request is introducing a ton of complexity that might make sense in an event-driven interactive GUI application, but not so much in a quick shell script.<p>Swift tools might be a good choice for use cases where you need to integrate with an existing Swift project, or if you need lower level APIs.<p>In this example, a Bash script would have been almost done by the time you worked out the `curl | jq` command. But in other similar cases I would suggest Python Requests, which will take perhaps 10% as much code and avoid issues of Linux compatibility and mistakes like forgetting to call `resume()` on your download task or `exit()` in some branch (there are five calls just to keep the program from looping forever).<p>That said, I think this blog post is very informative and well-made for a beginner interested in talking to a web-based JSON API from Swift.",
        "I like it, being able to start with a shell script and then evolve the tool to a static binary is pretty neat, I do that all the time with ActionScript<p>I don't think it's trying to be a \"jack of all trades\", sure Bash can do plenty but when you reach few hundred lines of Bash and start to fight against the syntax to do simple things well...<p>The shebang line is there to be used, you can use sh, bash, etc. but there are other citizen like perl, php, python, etc. so why not swift or anything else if it help you build quickly command-line tools?"
      ],
      "relevant": "false"
    },
    {
      "id": 20811829,
      "title": "Curl exercises",
      "search": [
        "Curl exercises",
        "https://jvns.ca/blog/2019/08/27/curl-exercises/",
        "Recently Ive been interested in how people learn things. I was reading Kathy Sierras great book Badass: Making Users Awesome. It talks about the idea of deliberate practice. The idea is that you find a small micro-skill that can be learned in maybe 3 sessions of 45 minutes, and focus on learning that micro-skill. So, as an exercise, I was trying to think of a computer skill that I thought could be learned in 3 45-minute sessions. I thought that making HTTP requests with curl might be a skill like that, so here are some curl exercises as an experiment! whats curl? curl is a command line tool for making HTTP requests. I like it because its an easy way to test that servers or APIs are doing what I think, but its a little confusing at first! Heres a drawing explaining curls most important command line arguments (which is page 6 of my Bite Size Networking zine). You can click to make it bigger. fluency is valuable With any command line tool, I think having fluency is really helpful. Its really nice to be able to just type in the thing you need. For example recently I was testing out the Gumroad API and I was able to just type in: curl https://api.gumroad.com/v2/sales \\ -d \"access_token=<SECRET>\" \\ -X GET -d \"before=2016-09-03\" and get things working from the command line. 21 curl exercises These exercises are about understanding how to make different kinds of HTTP requests with curl. Theyre a little repetitive on purpose. They exercise basically everything I do with curl. To keep it simple, were going to make a lot of our requests to the same website: https://httpbin.org. httpbin is a service that accepts HTTP requests and then tells you what request you made. Request https://httpbin.org Request https://httpbin.org/anything. httpbin.org/anything will look at the request you made, parse it, and echo back to you what you requested. curls default is to make a GET request. Make a POST request to https://httpbin.org/anything Make a GET request to https://httpbin.org/anything, but this time add some query parameters (set value=panda). Request googles robots.txt file (www.google.com/robots.txt) Make a GET request to https://httpbin.org/anything and set the header User-Agent: elephant. Make a DELETE request to https://httpbin.org/anything Request https://httpbin.org/anything and also get the response headers Make a POST request to https://httpbin.org/anything with the JSON body {\"value\": \"panda\"} Make the same POST request as the previous exercise, but set the Content-Type header to application/json (because POST requests need to have a content type that matches their body). Look at the json field in the response to see the difference from the previous one. Make a GET request to https://httpbin.org/anything and set the header Accept-Encoding: gzip (what happens? why?) Put a bunch of a JSON in a file and then make a POST request to https://httpbin.org/anything with the JSON in that file as the body Make a request to https://httpbin.org/image and set the header Accept: image/png. Save the output to a PNG file and open the file in an image viewer. Try the same thing with different Accept: headers. Make a PUT request to https://httpbin.org/anything Request https://httpbin.org/image/jpeg, save it to a file, and open that file in your image editor. Request https://www.twitter.com. Youll get an empty response. Get curl to show you the response headers too, and try to figure out why the response was empty. Make any request to https://httpbin.org/anything and just set some nonsense headers (like panda: elephant) Request https://httpbin.org/status/404 and https://httpbin.org/status/200. Request them again and get curl to show the response headers. Request https://httpbin.org/anything and set a username and password (with -u username:password) Download the Twitter homepage (https://twitter.com) in Spanish by setting the Accept-Language: es-ES header. Make a request to the Stripe API with curl. (see https://stripe.com/docs/development for how, they give you a test API key). Try making exactly the same request to https://httpbin.org/anything. ",
        "I really like using HTTPie (<a href=\"https://httpie.org\" rel=\"nofollow\">https://httpie.org</a>). Much friendlier syntax then curl, formats the output...<p>Also works great with fx (<a href=\"https://github.com/antonmedv/fx\" rel=\"nofollow\">https://github.com/antonmedv/fx</a>). Just pipe the output from HTTPie and you get easy json processing.",
        "This is more SysAdmin related, but one power-curl function I use atleast 30 times a day is this alias I have in my .bash_aliases<p>This will output the HTTP status code for a given URL.<p><pre><code>     alias hstat=\"curl -o /dev/null --silent --head --write-out '%{http_code}\\n'\" $1  \n</code></pre>\nExample:<p><pre><code>  $ hstat google.com\n  301\n</code></pre>\nI also use curl as an 'uptime monitor' by adding onto that code section.  (a file with a list of URLs, and \"if http status code !=200\" then email me.)<p>There are variations on this all over the place but I really depend on it and I like it."
      ],
      "relevant": "false"
    },
    {
      "id": 19427332,
      "title": "Show HN: Drogon – A C++14/17 based high performance HTTP application framework",
      "search": [
        "Show HN: Drogon – A C++14/17 based high performance HTTP application framework",
        "https://github.com/an-tao/drogon",
        "English | | Overview Drogon is a C++14/17-based HTTP application framework. Drogon can be used to easily build various types of web application server programs using C++. Drogon is the name of a dragon in the American TV series \"Game of Thrones\" that I really like. Drogon is a cross-platform framework, It supports Linux, macOS, FreeBSD, OpenBSD, HaikuOS, and Windows. Its main features are as follows: Use a non-blocking I/O network lib based on epoll (kqueue under macOS/FreeBSD) to provide high-concurrency, high-performance network IO, please visit the TFB Tests Results for more details; Provide a completely asynchronous programming mode; Support Http1.0/1.1 (server side and client side); Based on template, a simple reflection mechanism is implemented to completely decouple the main program framework, controllers and views. Support cookies and built-in sessions; Support back-end rendering, the controller generates the data to the view to generate the Html page. Views are described by CSP template files, C++ codes are embedded into Html pages through CSP tags. And the drogon command-line tool automatically generates the C++ code files for compilation; Support view page dynamic loading (dynamic compilation and loading at runtime); Provide a convenient and flexible routing solution from the path to the controller handler; Support filter chains to facilitate the execution of unified logic (such as login verification, Http Method constraint verification, etc.) before handling HTTP requests; Support https (based on OpenSSL); Support WebSocket (server side and client side); Support JSON format request and response, very friendly to the Restful API application development; Support file download and upload; Support gzip, brotli compression transmission; Support pipelining; Provide a lightweight command line tool, drogon_ctl, to simplify the creation of various classes in Drogon and the generation of view code; Support non-blocking I/O based asynchronously reading and writing database (PostgreSQL and MySQL(MariaDB) database); Support asynchronously reading and writing sqlite3 database based on thread pool; Support Redis with asynchronous reading and writing; Support ARM Architecture; Provide a convenient lightweight ORM implementation that supports for regular object-to-database bidirectional mapping; Support plugins which can be installed by the configuration file at load time; Support AOP with build-in joinpoints. Support C++ coroutines A very simple example Unlike most C++ frameworks, the main program of the drogon application can be kept clean and simple. Drogon uses a few tricks to decouple controllers from the main program. The routing settings of controllers can be done through macros or configuration file. Below is the main program of a typical drogon application: #include <drogon/drogon.h> using namespace drogon; int main() { app().setLogPath(\"./\") .setLogLevel(trantor::Logger::kWarn) .addListener(\"0.0.0.0\", 80) .setThreadNum(16) .enableRunAsDaemon() .run(); } It can be further simplified by using configuration file as follows: #include <drogon/drogon.h> using namespace drogon; int main() { app().loadConfigFile(\"./config.json\").run(); } Drogon provides some interfaces for adding controller logic directly in the main() function, for example, user can register a handler like this in Drogon: app().registerHandler(\"/test?username={name}\", [](const HttpRequestPtr& req, std::function<void (const HttpResponsePtr &)> &&callback, const std::string &name) { Json::Value json; json[\"result\"]=\"ok\"; json[\"message\"]=std::string(\"hello,\")+name; auto resp=HttpResponse::newHttpJsonResponse(json); callback(resp); }, {Get,\"LoginFilter\"}); While such interfaces look intuitive, they are not suitable for complex business logic scenarios. Assuming there are tens or even hundreds of handlers that need to be registered in the framework, isn't it a better practice to implement them separately in their respective classes? So unless your logic is very simple, we don't recommend using above interfaces. Instead, we can create an HttpSimpleController as follows: /// The TestCtrl.h file #pragma once #include <drogon/HttpSimpleController.h> using namespace drogon; class TestCtrl:public drogon::HttpSimpleController<TestCtrl> { public: virtual void asyncHandleHttpRequest(const HttpRequestPtr& req, std::function<void (const HttpResponsePtr &)> &&callback) override; PATH_LIST_BEGIN PATH_ADD(\"/test\",Get); PATH_LIST_END }; /// The TestCtrl.cc file #include \"TestCtrl.h\" void TestCtrl::asyncHandleHttpRequest(const HttpRequestPtr& req, std::function<void (const HttpResponsePtr &)> &&callback) { //write your application logic here auto resp = HttpResponse::newHttpResponse(); resp->setBody(\"<p>Hello, world!</p>\"); resp->setExpiredTime(0); callback(resp); } Most of the above programs can be automatically generated by the command line tool drogon_ctl provided by drogon (The command is drogon_ctl create controller TestCtrl). All the user needs to do is add their own business logic. In the example, the controller returns a Hello, world! string when the client accesses the http://ip/test URL. For JSON format response, we create the controller as follows: /// The header file #pragma once #include <drogon/HttpSimpleController.h> using namespace drogon; class JsonCtrl : public drogon::HttpSimpleController<JsonCtrl> { public: virtual void asyncHandleHttpRequest(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback) override; PATH_LIST_BEGIN //list path definitions here; PATH_ADD(\"/json\", Get); PATH_LIST_END }; /// The source file #include \"JsonCtrl.h\" void JsonCtrl::asyncHandleHttpRequest(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback) { Json::Value ret; ret[\"message\"] = \"Hello, World!\"; auto resp = HttpResponse::newHttpJsonResponse(ret); callback(resp); } Let's go a step further and create a demo RESTful API with the HttpController class, as shown below (Omit the source file): /// The header file #pragma once #include <drogon/HttpController.h> using namespace drogon; namespace api { namespace v1 { class User : public drogon::HttpController<User> { public: METHOD_LIST_BEGIN //use METHOD_ADD to add your custom processing function here; METHOD_ADD(User::getInfo, \"/{id}\", Get); //path is /api/v1/User/{arg1} METHOD_ADD(User::getDetailInfo, \"/{id}/detailinfo\", Get); //path is /api/v1/User/{arg1}/detailinfo METHOD_ADD(User::newUser, \"/{name}\", Post); //path is /api/v1/User/{arg1} METHOD_LIST_END //your declaration of processing function maybe like this: void getInfo(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback, int userId) const; void getDetailInfo(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback, int userId) const; void newUser(const HttpRequestPtr &req, std::function<void(const HttpResponsePtr &)> &&callback, std::string &&userName); public: User() { LOG_DEBUG << \"User constructor!\"; } }; } // namespace v1 } // namespace api As you can see, users can use the HttpController to map paths and parameters at the same time. This is a very convenient way to create a RESTful API application. In addition, you can also find that all handler interfaces are in asynchronous mode, where the response is returned by a callback object. This design is for performance reasons because in asynchronous mode the drogon application can handle a large number of concurrent requests with a small number of threads. After compiling all of the above source files, we get a very simple web application. This is a good start. For more information, please visit the wiki or DocsForge Contributions Every contribution is welcome. Please refer to the contribution guidelines for more information. ",
        "2 small things.<p>a) Show me the code to start with, don't send me digging for it. A SimpleController example as the very first thing would help give a feel for the project, and makes me more likely to consider the project.<p>b) If there's an easier way (like the drogon_ctl utility at the start of Quickstart [0]), show that first, and the more detailed way second.<p>Other than that, it looks great. I've used libmicrohttpd a few times, so a bit less of an overhead always looks great.<p>[0] <a href=\"https://github.com/an-tao/drogon/wiki/quick-start\" rel=\"nofollow\">https://github.com/an-tao/drogon/wiki/quick-start</a>",
        "This is cool, and I like it. Very Haskell like, which is a compliment in my book.<p>But one thing that surprises me is that folks are essentially sleeping on HTTP/2. HTTP/2 is just a hell of a lot better in most every dimension. It's better for handshake latency, it's better for bandwidth in most cases, it's better for eliminating excess SSL overhead and also, it's kinda easier to write client libraries for, because it's so much simpler (although the parallel and concurrent nature of connections will challenge a lot of programmers).<p>It's not bad to see a new contender in this space, but it's surprising that it isn't http/2 first. Is there a good reason for this? It's busted through 90% support on caniuse, so it's hard to make an argument that adoption holds it back."
      ],
      "relevant": "false"
    },
    {
      "id": 20062064,
      "title": "Semantic: Parsing, analyzing, and comparing source code across many languages",
      "search": [
        "Semantic: Parsing, analyzing, and comparing source code across many languages",
        "https://github.com/github/semantic",
        "semantic is a Haskell library and command line tool for parsing, analyzing, and comparing source code. In a hurry? Check out our documentation of example uses for the semantic command line tool. Table of Contents Usage Language support Development Technology and architecture Licensing Usage Run semantic --help for complete list of up-to-date options. Parse Usage: semantic parse [--sexpression | (--json-symbols|--symbols) | --proto-symbols | --show | --quiet] [FILES...] Generate parse trees for path(s) Available options: --sexpression Output s-expression parse trees (default) --json-symbols,--symbols Output JSON symbol list --proto-symbols Output protobufs symbol list --show Output using the Show instance (debug only, format subject to change without notice) --quiet Don't produce output, but show timing stats -h,--help Show this help text Language support Language Parse AST Symbols Stack graphs Ruby JavaScript TypeScript Python Go PHP Java JSON JSX TSX CodeQL Haskell Used for code navigation on github.com. Supported Partial support Under development - N/A Development semantic requires at least GHC 8.10.1 and Cabal 3.0. We strongly recommend using ghcup to sandbox GHC versions, as GHC packages installed through your OS's package manager may not install statically-linked versions of the GHC boot libraries. semantic currently builds only on Unix systems; users of other operating systems may wish to use the Docker images. We use cabal's Nix-style local builds for development. To get started quickly: git clone git@github.com:github/semantic.git cd semantic script/bootstrap cabal v2-build all cabal v2-run semantic:test cabal v2-run semantic:semantic -- --help You can also use the Bazel build system for development. To learn more about Bazel and why it might give you a better development experience, check the build documentation. git clone git@github.com:github/semantic.git cd semantic script/bootstrap-bazel bazel build //... stack as a build tool is not officially supported; there is unofficial stack.yaml support available, though we cannot make guarantees as to its stability. Technology and architecture Architecturally, semantic: Generates per-language Haskell syntax types based on tree-sitter grammar definitions. Reads blobs from a filesystem or provided via a protocol buffer request. Returns blobs or performs analysis. Renders output in one of many supported formats. Throughout its lifestyle, semantic has leveraged a number of interesting algorithms and techniques, including: Myers' algorithm (SES) as described in the paper An O(ND) Difference Algorithm and Its Variations RWS as described in the paper RWS-Diff: Flexible and Efficient Change Detection in Hierarchical Data. Open unions and data types la carte. An implementation of Abstracting Definitional Interpreters extended to work with an la carte representation of syntax terms. Contributions Contributions are welcome! Please see our contribution guidelines and our code of conduct for details on how to participate in our community. Licensing Semantic is licensed under the MIT license. ",
        "Looks very interesting - would benefit from showing some examples and/or use cases",
        "The late 20th century, early 00's version:<p><a href=\"http://www.program-transformation.org/Transform/CodeCrawler\" rel=\"nofollow\">http://www.program-transformation.org/Transform/CodeCrawler</a><p>(And MOOSE)"
      ],
      "relevant": "true"
    },
    {
      "id": 21676256,
      "title": "Open Source Webhook Server",
      "search": [
        "Open Source Webhook Server",
        "https://github.com/adnanh/webhook",
        "What is webhook? webhook is a lightweight configurable tool written in Go, that allows you to easily create HTTP endpoints (hooks) on your server, which you can use to execute configured commands. You can also pass data from the HTTP request (such as headers, payload or query variables) to your commands. webhook also allows you to specify rules which have to be satisfied in order for the hook to be triggered. For example, if you're using Github or Bitbucket, you can use webhook to set up a hook that runs a redeploy script for your project on your staging server, whenever you push changes to the master branch of your project. If you use Mattermost or Slack, you can set up an \"Outgoing webhook integration\" or \"Slash command\" to run various commands on your server, which can then report back directly to you or your channels using the \"Incoming webhook integrations\", or the appropriate response body. webhook aims to do nothing more than it should do, and that is: receive the request, parse the headers, payload and query variables, check if the specified rules for the hook are satisfied, and finally, pass the specified arguments to the specified command via command line arguments or via environment variables. Everything else is the responsibility of the command's author. Hookdoo If you don't have time to waste configuring, hosting, debugging and maintaining your webhook instance, we offer a SaaS solution that has all of the capabilities webhook provides, plus a lot more, and all that packaged in a nice friendly web interface. If you are interested, find out more at hookdoo website. If you have any questions, you can contact us at info@hookdoo.com If you need a way of inspecting, monitoring and replaying webhooks without the back and forth troubleshooting, give Hookdeck a try! Getting started Installation Building from source To get started, first make sure you've properly set up your Go 1.14 or newer environment and then run $ go build github.com/adnanh/webhook to build the latest version of the webhook. Using package manager Snap store Ubuntu If you are using Ubuntu linux (17.04 or later), you can install webhook using sudo apt-get install webhook which will install community packaged version. Debian If you are using Debian linux (\"stretch\" or later), you can install webhook using sudo apt-get install webhook which will install community packaged version (thanks @freeekanayaka) from https://packages.debian.org/sid/webhook Download prebuilt binaries Prebuilt binaries for different architectures are available at GitHub Releases. Configuration Next step is to define some hooks you want webhook to serve. webhook supports JSON or YAML configuration files, but we'll focus primarily on JSON in the following example. Begin by creating an empty file named hooks.json. This file will contain an array of hooks the webhook will serve. Check Hook definition page to see the detailed description of what properties a hook can contain, and how to use them. Let's define a simple hook named redeploy-webhook that will run a redeploy script located in /var/scripts/redeploy.sh. Make sure that your bash script has #!/bin/sh shebang on top. Our hooks.json file will now look like this: [ { \"id\": \"redeploy-webhook\", \"execute-command\": \"/var/scripts/redeploy.sh\", \"command-working-directory\": \"/var/webhook\" } ] NOTE: If you prefer YAML, the equivalent hooks.yaml file would be: - id: redeploy-webhook execute-command: \"/var/scripts/redeploy.sh\" command-working-directory: \"/var/webhook\" You can now run webhook using $ /path/to/webhook -hooks hooks.json -verbose It will start up on default port 9000 and will provide you with one HTTP endpoint http://yourserver:9000/hooks/redeploy-webhook Check webhook parameters page to see how to override the ip, port and other settings such as hook hotreload, verbose output, etc, when starting the webhook. By performing a simple HTTP GET or POST request to that endpoint, your specified redeploy script would be executed. Neat! However, hook defined like that could pose a security threat to your system, because anyone who knows your endpoint, can send a request and execute your command. To prevent that, you can use the \"trigger-rule\" property for your hook, to specify the exact circumstances under which the hook would be triggered. For example, you can use them to add a secret that you must supply as a parameter in order to successfully trigger the hook. Please check out the Hook rules page for detailed list of available rules and their usage. Multipart Form Data webhook provides limited support the parsing of multipart form data. Multipart form data can contain two types of parts: values and files. All form values are automatically added to the payload scope. Use the parse-parameters-as-json settings to parse a given value as JSON. All files are ignored unless they match one of the following criteria: The Content-Type header is application/json. The part is named in the parse-parameters-as-json setting. In either case, the given file part will be parsed as JSON and added to the payload map. Templates webhook can parse the hooks configuration file as a Go template when given the -template CLI parameter. See the Templates page for more details on template usage. Using HTTPS webhook by default serves hooks using http. If you want webhook to serve secure content using https, you can use the -secure flag while starting webhook. Files containing a certificate and matching private key for the server must be provided using the -cert /path/to/cert.pem and -key /path/to/key.pem flags. If the certificate is signed by a certificate authority, the cert file should be the concatenation of the server's certificate followed by the CA's certificate. TLS version and cipher suite selection flags are available from the command line. To list available cipher suites, use the -list-cipher-suites flag. The -tls-min-version flag can be used with -list-cipher-suites. CORS Headers If you want to set CORS headers, you can use the -header name=value flag while starting webhook to set the appropriate CORS headers that will be returned with each response. Interested in running webhook inside of a Docker container? You can use one of the following Docker images, or create your own (please read this discussion): almir/webhook roxedus/webhook thecatlady/webhook Examples Check out Hook examples page for more complex examples of hooks. Guides featuring webhook Plex 2 Telegram by @psyhomb Webhook & JIRA by @perfecto25 Trigger Ansible AWX job runs on SCM (e.g. git) commit by @jpmens Deploy using GitHub webhooks by @awea Setting up Automatic Deployment and Builds Using Webhooks by Will Browning Auto deploy your Node.js app on push to GitHub in 3 simple steps by Karolis Rusenas Automate Static Site Deployments with Salt, Git, and Webhooks by Linode Using Prometheus to Automatically Scale WebLogic Clusters on Kubernetes by Marina Kogan Github Pages and Jekyll - A New Platform for LACNIC Labs by Carlos Martnez Cagnazzo How to Deploy React Apps Using Webhooks and Integrating Slack on Ubuntu by Arslan Ud Din Shafiq Private webhooks by Thomas Adventures in webhooks by Drake GitHub pro tips by Spencer Lyon XiaoMi Vacuum + Amazon Button = Dash Cleaning by c0mmensal Set up Automated Deployments From Github With Webhook by Maxim Orlov VIDEO: Gitlab CI/CD configuration using Docker and adnanh/webhook to deploy on VPS - Tutorial #1 by Yes! Let's Learn Software Engineering Integrate automatic deployment in 20 minutes using webhooks + Nginx setup by Anksus ... Want to add your own? Open an Issue or create a PR :-) Community Contributions See the webhook-contrib repository for a collections of tools and helpers related to webhook that have been contributed by the webhook community. Need help? Check out existing issues to see if someone else also had the same problem, or open a new one. Support active development Sponsors DigitalOcean is a simple and robust cloud computing platform, designed for developers. BrowserStack is a cloud-based cross-browser testing tool that enables developers to test their websites across various browsers on different operating systems and mobile devices, without requiring users to install virtual machines, devices or emulators. Support this project by becoming a sponsor. Your logo will show up here with a link to your website. By contributing This project exists thanks to all the people who contribute. Contribute!. By giving money OpenCollective Backer OpenCollective Sponsor PayPal Patreon Faircode Flattr Thank you to all our backers! License The MIT License (MIT) Copyright (c) 2015 Adnan Hajdarevic adnanh@gmail.com Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. ",
        "Interesting, could be useful to interface some of my stuff with IFTTT or Integromat.",
        "I have been using this webhook server in prod for a few years and its been easy to setup/maintain ... you just define  github.com repo to publish `git push` or whatever then this webhook server listens to every git push my team makes to launch a code recompile/redeploy ...  foolproof and solid ... I highly recommend"
      ],
      "relevant": "false"
    },
    {
      "id": 19074170,
      "title": "Show HN: Gita – a CLI tool to manage multiple Git repos",
      "search": [
        "Show HN: Gita – a CLI tool to manage multiple Git repos",
        "https://github.com/nosarthur/gita",
        "_______________________________ ( ____ \\__ __|__ __( ___ ) | ( \\/ ) ( ) ( | ( ) | | | | | | | | (___) | | | ____ | | | | | ___ | | | \\_ ) | | | | | ( ) | | (___) |__) (___ | | | ) ( | (_______)_______/ )_( |/ \\| v0.15 Gita: a command-line tool to manage multiple git repos This tool does two things display the status of multiple git repos such as branch, modification, commit message side by side (batch) delegate git commands/aliases from any working directory If several repos are related, it helps to see their status together. I also hate to change directories to execute git commands. In this screenshot, the gita ll command displays the status of all repos. The gita remote dotfiles command translates to git remote -v for the dotfiles repo, even though we are not in the repo. The gita fetch command fetches from all repos and two of them have updates. To see the pre-defined commands, run gita -h or take a look at cmds.json. To add your own commands, see the customization section. To run arbitrary git command, see the superman mode section. To run arbitrary shell command, see the shell mode section. The branch color distinguishes 5 situations between local and remote branches: color meaning white local has no remote green local is the same as remote red local has diverged from remote purple local is ahead of remote (good for push) yellow local is behind remote (good for merge) The choice of purple for ahead and yellow for behind is motivated by blueshift and redshift, using green as baseline. You can change the color scheme using the gita color command. See the customization section. The additional status symbols denote symbol meaning + staged changes * unstaged changes _ untracked files/folders The bookkeeping sub-commands are gita add <repo-path(s)> [-g <groupname>]: add repo(s) to gita, optionally into an existing group gita add -a <repo-parent-path(s)>: add repo(s) in <repo-parent-path(s)> recursively and automatically generate hierarchical groups. See the customization section for more details. gita add -b <bare-repo-path(s)>: add bare repo(s) to gita. See the customization section for more details on setting custom worktree. gita add -r <repo-parent-path(s)>: add repo(s) in <repo-parent-path(s)> recursively gita clone <config-file>: clone repos in config-file (generated by gita freeze) to current directory. gita clone -p <config-file>: clone repos in config-file to prescribed paths. gita context: context sub-command gita context: show current context gita context <group-name>: set context to group-name, all operations then only apply to repos in this group gita context auto: set context automatically according to the current working directory gita context none: remove context gita color: color sub-command gita color [ll]: Show available colors and the current coloring scheme gita color reset: Reset to the default coloring scheme gita color set <situation> <color>: Use the specified color for the local-remote situation gita flags: flags sub-command gita flags set <repo-name> <flags>: add custom flags to repo gita flags [ll]: display repos with custom flags gita freeze: print information of all repos such as URL, name, and path. Use with gita clone. gita group: group sub-command gita group add <repo-name(s)> -n <group-name>: add repo(s) to a new or existing group gita group [ll]: display existing groups with repos gita group ls: display existing group names gita group rename <group-name> <new-name>: change group name gita group rm <group-name(s)>: delete group(s) gita group rmrepo <repo-name(s)> -n <group-name>: remove repo(s) from existing group gita info: info sub-command gita info [ll]: display the used and unused information items gita info add <info-item>: enable information item gita info rm <info-item>: disable information item gita ll: display the status of all repos gita ll <group-name>: display the status of repos in a group gita ll -g: display the repo summaries by groups gita ls: display the names of all repos gita ls <repo-name>: display the absolute path of one repo gita rename <repo-name> <new-name>: rename a repo gita rm <repo-name(s)>: remove repo(s) from gita (won't remove files on disk) gita -v: display gita version The git delegating sub-commands are of two formats gita <sub-command> [repo-name(s) or group-name(s)]: optional repo or group input, and no input means all repos. gita <sub-command> <repo-name(s) or groups-name(s)>: required repo name(s) or group name(s) input They translate to git <sub-command> for the corresponding repos. By default, only fetch and pull take optional input. In other words, gita fetch and gita pull apply to all repos. To see the pre-defined sub-commands, run gita -h or take a look at cmds.json. To add your own sub-commands or override the default behaviors, see the customization section. To run arbitrary git command, see the superman mode section. If more than one repos are specified, the git command runs asynchronously, with the exception of log, difftool and mergetool, which require non-trivial user input. Repo configuration is saved in $XDG_CONFIG_HOME/gita/repos.csv (most likely ~/.config/gita/repos.csv). Installation To install the latest version, run If you prefer development mode, download the source code and run pip3 install -e <gita-source-folder> In either case, calling gita in terminal may not work, then put the following line in the .bashrc file. alias gita=\"python3 -m gita\" Windows users may need to enable the ANSI escape sequence in terminal for the branch color to work. See this stackoverflow post for details. Auto-completion Download .gita-completion.bash or .gita-completion.zsh and source it in shell. Superman mode The superman mode delegates any git command or alias. Usage: gita super [repo-name(s) or group-name(s)] <any-git-command-with-or-without-options> Here repo-name(s) or group-name(s) are optional, and their absence means all repos. For example, gita super checkout master puts all repos on the master branch gita super frontend-repo backend-repo commit -am 'implement a new feature' executes git commit -am 'implement a new feature' for frontend-repo and backend-repo Shell mode The shell mode delegates any shell command. Usage: gita shell [repo-name(s) or group-name(s)] <any-shell-command> Here repo-name(s) or group-name(s) are optional, and their absence means all repos. For example, gita shell ll lists contents for all repos gita shell repo1 repo2 mkdir docs create a new directory docs in repo1 and repo2 gita shell \"git describe --abbrev=0 --tags | xargs git checkout\": check out the latest tag for all repos Customization define repo group and context When the project contains several independent but related repos, we can define a group and execute gita command on this group. For example, gita group add repo1 repo2 -n my-group gita ll my-group gita pull my-group To save more typing, one can set a group as context, then any gita command is scoped to the group gita context my-group gita ll gita pull The most useful context maybe auto. In this mode, the context is automatically determined from the current working directory (CWD): the context is the group whose member repo's path contains CWD. To set it, run To remove the context, run It is also possible to recursively add repos within a directory and generate hierarchical groups automatically. For example, running on the following folder structure src project1 repo1 repo2 repo3 project2 repo4 repo5 repo6 gives rise to 3 groups: src:repo1,repo2,repo3,repo4,repo5,repo6 src-project1:repo1,repo2 src-project2:repo4,repo5 add user-defined sub-command using json file Custom delegating sub-commands can be defined in $XDG_CONFIG_HOME/gita/cmds.json (most likely ~/.config/gita/cmds.json) And they shadow the default ones if name collisions exist. Default delegating sub-commands are defined in cmds.json. For example, gita stat <repo-name(s)> is registered as \"stat\":{ \"cmd\": \"git diff --stat\", \"help\": \"show edit statistics\" } which executes git diff --stat for the specified repo(s). To disable asynchronous execution, set disable_async to be true. See the difftool example: \"difftool\":{ \"cmd\": \"git difftool\", \"disable_async\": true, \"help\": \"show differences using a tool\" } If you want a custom command to behave like gita fetch, i.e., to apply to all repos when no repo is specified, set allow_all to be true. For example, the following snippet creates a new command gita comaster [repo-name(s)] with optional repo name input. \"comaster\":{ \"cmd\": \"checkout master\", \"allow_all\": true, \"help\": \"checkout the master branch\" } Any command that runs in the superman mode mode or the shell mode can be defined in this json format. For example, the following command runs in shell mode and fetches only the current branch from upstream. \"fetchcrt\":{ \"cmd\": \"git rev-parse --abbrev-ref HEAD | xargs git fetch --prune upstream\", \"allow_all\": true, \"shell\": true, \"help\": \"fetch current branch only\" } customize the local/remote relationship coloring displayed by the gita ll command You can see the default color scheme and the available colors via gita color. To change the color coding, use gita color set <situation> <color>. The configuration is saved in $XDG_CONFIG_HOME/gita/color.csv. customize information displayed by the gita ll command You can customize the information displayed by gita ll. The used and unused information items are shown with gita info, and the configuration is saved in $XDG_CONFIG_HOME/gita/info.csv. For example, the default setting corresponds to branch,commit_msg,commit_time customize git command flags One can set custom flags to run git commands. For example, with gita flags set my-repo --git-dir=`gita ls dotfiles` --work-tree=$HOME any git command/alias triggered from gita on dotfiles will use these flags. Note that the flags are applied immediately after git. For example, gita st dotfiles translates to git --git-dir=$HOME/somefolder --work-tree=$HOME status running from the dotfiles directory. This feature was originally added to deal with bare repo dotfiles. Requirements Gita requires Python 3.6 or higher, due to the use of f-string and asyncio module. Under the hood, gita uses subprocess to run git commands/aliases. Thus the installed git version may matter. I have git 1.8.3.1, 2.17.2, and 2.20.1 on my machines, and their results agree. Tips effect shell command enter <repo> directory cd `gita ls <repo>` delete repos in <group> gita group ll <group> | xargs gita rm Contributing To contribute, you can report/fix bugs request/implement features star/recommend this project Read this article if you have never contribute code to open source project before. Chat room is available on To run tests locally, simply pytest in the source code folder. Note that context should be set as none. More implementation details are in design.md. A step-by-step guide to reproduce this project is here. You can also sponsor me on GitHub. Any amount is appreciated! Other multi-repo tools I haven't tried them but I heard good things about them. myrepos repo ",
        "One thing this project does really well is to start the readme with a screenshot. I open the link, scroll down to the readme, and I immediately see what sort of user interface/experience I will get. Some commenters have noted that Gita is similar to other multi-repo tools, but both Repo and wstool are more effort to evaluate, because their readmes don't have pictures.",
        "Nice, but is there a way to just run any command? I.e. just `gita <optional repo names/paths> <pass entire command line git -C repodir>`. This has advantages in that you don't have to go round via a command file, don't have to keep it synced across machines, don't have to remember what you put in the file etc, and can just use the git syntax which already took long enough to learn by heart :P<p>I've used multiple multiple repository tools and in the end all I happen to use is one (usually versioned) file to store a list of repositories and then a command which just loops over all repos and applies anything to it. If I need custom commands I use git aliases so that works both for normal git and whatever tool used."
      ],
      "relevant": "false"
    },
    {
      "id": 21671843,
      "title": "Sherlock: Find usernames across social networks",
      "search": [
        "Sherlock: Find usernames across social networks",
        "https://github.com/sherlock-project/sherlock",
        "Hunt down social media accounts by username across social networks Installation | Usage | Docker Notes | Contributing Installation # clone the repo $ git clone https://github.com/sherlock-project/sherlock.git # change the working directory to sherlock $ cd sherlock # install the requirements $ python3 -m pip install -r requirements.txt Usage $ python3 sherlock --help usage: sherlock [-h] [--version] [--verbose] [--folderoutput FOLDEROUTPUT] [--output OUTPUT] [--tor] [--unique-tor] [--csv] [--site SITE_NAME] [--proxy PROXY_URL] [--json JSON_FILE] [--timeout TIMEOUT] [--print-all] [--print-found] [--no-color] [--browse] [--local] USERNAMES [USERNAMES ...] Sherlock: Find Usernames Across Social Networks (Version 0.14.0) positional arguments: USERNAMES One or more usernames to check with social networks. optional arguments: -h, --help show this help message and exit --version Display version information and dependencies. --verbose, -v, -d, --debug Display extra debugging information and metrics. --folderoutput FOLDEROUTPUT, -fo FOLDEROUTPUT If using multiple usernames, the output of the results will be saved to this folder. --output OUTPUT, -o OUTPUT If using single username, the output of the result will be saved to this file. --tor, -t Make requests over Tor; increases runtime; requires Tor to be installed and in system path. --unique-tor, -u Make requests over Tor with new Tor circuit after each request; increases runtime; requires Tor to be installed and in system path. --csv Create Comma-Separated Values (CSV) File. --site SITE_NAME Limit analysis to just the listed sites. Add multiple options to specify more than one site. --proxy PROXY_URL, -p PROXY_URL Make requests over a proxy. e.g. socks5://127.0.0.1:1080 --json JSON_FILE, -j JSON_FILE Load data from a JSON file or an online, valid, JSON file. --timeout TIMEOUT Time (in seconds) to wait for response to requests. Default timeout is infinity. A longer timeout will be more likely to get results from slow sites. On the other hand, this may cause a long delay to gather all results. --print-all Output sites where the username was not found. --print-found Output sites where the username was found. --no-color Don't color terminal output --browse, -b Browse to all results on default browser. --local, -l Force the use of the local data.json file. To search for only one user: To search for more than one user: python3 sherlock user1 user2 user3 Accounts found will be stored in an individual text file with the corresponding username (e.g user123.txt). Anaconda (Windows) Notes If you are using Anaconda in Windows, using 'python3' might not work. Use 'python' instead. Docker Notes If docker is installed you can build an image and run this as a container. docker build -t mysherlock-image . Once the image is built, sherlock can be invoked by running the following: docker run --rm -t mysherlock-image user123 The optional --rm flag removes the container filesystem on completion to prevent cruft build-up. See: https://docs.docker.com/engine/reference/run/#clean-up---rm The optional -t flag allocates a pseudo-TTY which allows colored output. See: https://docs.docker.com/engine/reference/run/#foreground Use the following command to access the saved results: docker run --rm -t -v \"$PWD/results:/opt/sherlock/results\" mysherlock-image -o /opt/sherlock/results/text.txt user123 The -v \"$PWD/results:/opt/sherlock/results\" options tell docker to create (or use) the folder results in the present working directory and to mount it at /opt/sherlock/results on the docker container. The -o /opt/sherlock/results/text.txt option tells sherlock to output the result. Or you can use \"Docker Hub\" to run sherlock: docker run theyahya/sherlock user123 Using docker-compose You can use the docker-compose.yml file from the repository and use this command: docker-compose run sherlock -o /opt/sherlock/results/text.txt user123 Contributing We would love to have you help us with the development of Sherlock. Each and every contribution is greatly valued! Here are some things we would appreciate your help on: Addition of new site support Bringing back site support of sites that have been removed in the past due to false positives [1] Please look at the Wiki entry on adding new sites to understand the issues. Tests Thank you for contributing to Sherlock! Before creating a pull request with new development, please run the tests to ensure that everything is working great. It would also be a good idea to run the tests before starting development to distinguish problems between your environment and the Sherlock software. The following is an example of the command line to run all the tests for Sherlock. This invocation hides the progress text that Sherlock normally outputs, and instead shows the verbose output of the tests. $ cd sherlock/sherlock $ python3 -m unittest tests.all --verbose Note that we do currently have 100% test coverage. Unfortunately, some of the sites that Sherlock checks are not always reliable, so it is common to get response problems. Any problems in connection will show up as warnings in the tests instead of true errors. If some sites are failing due to connection problems (site is down, in maintenance, etc) you can exclude them from tests by creating a tests/.excluded_sites file with a list of sites to ignore (one site name per line). Stargazers over time License MIT Sherlock Project Original Creator - Siddharth Dushantha ",
        "<i>CODE_OF_CONDUCT.md</i><p><i>Examples of unacceptable behavior by participants include:</i><p><i>Publishing others' private information, such as a physical or electronic address, without explicit permission</i><p>How is this tool not a violation of its own CoC?",
        "I don't get this? It checks against a limited list of websites if a username is taken. So what? This is hardly doxxing or \"smart\". Simply a faster method than the manual way, except it doesn't exhaust all avenues of search."
      ],
      "relevant": "true"
    },
    {
      "id": 21572308,
      "title": "Run an Internet Speed Test from the Command Line",
      "search": [
        "Run an Internet Speed Test from the Command Line",
        "https://www.putorius.net/speed-test-command-line.html",
        "We have all used tools like speedtest.net to test upload and download speeds. Whether it was to test the WiFi in that coffee shop (I use my own tether, never unknown hot spots), preparing for a LAN party (do people still do that?), or just a step in troubleshooting, we have all been there. For one reason or another you simply think you are being cheated of bandwidth, so you want independent verification of your speeds. This typically means opening a browser and going to a website to test your connection. But what if you want to run a speed test on a remote server? In this article we will discuss running an internet speed test from the Linux command line, and skipping the browser. There is something about the raw efficiency of the command line that I am really attracted to. As I discussed in the article 5 Command Line Tool to Break Your Dependence on the GUI, I try my best to stay away from the browser. It usually creates an unnecessary distraction. The internet is designed to grab your attention like a laser pointer does to a cat. So lets get started, and figure out one more way to stay away from the GUI. Different Speed Test Packages There are a few different tools you can use to run a speed test from the command line. To make things even more confusing the two most popular share the same exact name, but both use the speedtest.net service. Unofficial Speedtest-CLI Python Script The first one is an independently written Python script that is simple to install and use. It is available in the default repositories for some popular Linux distributions. Pros: Easy to installWide AvailabilityFull list of serversCan specify upload test, download test, or both Cons: Minimal output format optionsNo verbose output option Jump to Installing speedtest-cli Python Script or How to Use speedtest-cli Python Script. Official Ookla Speedtest CLI The second tool is built by Ookla, the people who bring you the speedtest.net website and service. Installing it requires you to add a repo for your package manager. But the maintainers offer simple instructions for installation. Pros: Official release from OoklaMore robust formatting optionsOutput easier to read, better layoutVerbose output availableHas repo making it easy to get updates Cons: Use limited to nearby serversCannot specify download or upload only Jump to Official Ookla Speedtest CLI The Speedtest-cli Python Script This is an easy way to get started running a speed test on the Linux command line. Installing the Speedtest-cli Python Script Simply use your package manager to install the package. Install on Fedora using DNF sudo dnf install speedtest-cli Ubuntu or Debian using APT sudo apt-get install speedtest-cli CentOS/Red Hat 7 / 8 Unfortunately, CentOS does not offer the rpm in their repos. It can still be easily installed. Change to /usr/bin directory to make command available to all users: cd /usr/bin Install dependencies: sudo yum install -y python wget Fetch script from github: sudo wget https://raw.githubusercontent.com/sivel/speedtest-cli/master/speedtest.py -O speedtest-cli Make script executable: sudo chmod +x speedtest-cli Or just copy and paste the whole thing below as a single line: cd /usr/bin; sudo yum install -y python wget && wget https://raw.githubusercontent.com/sivel/speedtest-cli/master/speedtest.py -O speedtest-cli && sudo chmod +x speedtest-cli How to Use the Python Script to Run a Speed Test The most basic usage is to simply run the command. It will automatically select the best server based on ping responses. Speedtest-cli Python Script Options There are several options available to change the default behavior. Here we will outline the most popular options. List Available Speed Test Servers You can use the list option to find a list of available servers to run your test against. At the time of writing this list is pretty extensive with 8829 possible servers. NOTE: The servers are sorted by distance, closest first. [[emailprotected] ~]$ speedtest-cli --list Retrieving speedtest.net configuration 4847) Hotwire Fision (Philadelphia, PA, United States) [10.92 km] 10979) School District of Philadelphia (Philadelphia, PA, United States) [10.92 km] ...OUTPUT TRUNCATED... Specify Specific Server to Test Against Once you have found the server you want to test against, you can use the server <SERVER ID> to select it. The server ID is the first column in the output of the list option above. [[emailprotected] ~]$ speedtest-cli --server 4847 Retrieving speedtest.net configuration Testing from Verizon Fios (x.x.x.x) Retrieving speedtest.net server list Selecting best server based on ping Hosted by School District of Philadelphia (Philadelphia, PA) [10.92 km]: 25.033 ms Testing download speed.. Download: 384.07 Mbit/s Testing upload speed Upload: 417.93 Mbit/s Only Test Upload or Download Speeds The option is actually designed to exclude a test. But since there are only two options it is effectively the same as selecting only one. To run only the download test, you exclude the upload, and vice versa. [[emailprotected] ~]$ speedtest-cli --no-upload Retrieving speedtest.net configuration Testing from Verizon Fios (x.x.x.x) Retrieving speedtest.net server list Selecting best server based on ping Hosted by KamaTera INC (New Jersey, NJ) [62.36 km]: 19.785 ms Testing download speed.. Download: 600.89 Mbit/s Skipping upload test Format Output in JSON or CSV You can specify the output format in JSON or CSV. You also have the opton to use CSV with a custom delimiter. This is handy if you are going to use the output in some other script or application. [[emailprotected] ~]$ speedtest-cli --json {\"download\": 597726146.0529929, \"upload\": 562476134.8046777, \"ping\": 17.004, \"server\": {\"url\": \"http://speedtest.us-ny2.kamatera.com:8080/speedtest/upload.php\", \"lat\": \"40.0583\", \"lon\": \"-74.4057\", \"name\": \"New Jersey, NJ\", \"country\": \"United States\", \"cc\": \"US\", \"sponsor\": \"KamaTera INC\", \"id\": \"11612\" ...OUTPUT TRUNCATED... Using CSV with a custom delimiter. The default delimiter is a comma, which is implied by the name CSV. Here we use the csv-delimiter option to change the delimiter to a pipe character. [[emailprotected] ~]$ speedtest-cli --csv --csv-delimiter \"|\" 11612|KamaTera INC|New Jersey, NJ|2019-11-17T14:51:53.636981Z|62.35865439150934|8.546|588013638.8767571|512001168.48230773||x.x.x.x The Official Ookla Speedtest CLI The official Speedtest CLI (Command Line Interface) from Ookla is a little more robust. It has all of the options of the python script and more. There are also several output formats not available with the unofficial python script. Ooklas speedtest is also a little easier on the eyes. It spreads the information out which makes it easier to read and displays a neat little progress bar. A URL you can use to share the results is also displayed by default. Installing the Official Speedtest CLI Install Speedtest CLI on Ubuntu / Debian: The Speedtest CLI from Ookla is supported on Ubuntu (xenial & bionic) and Debian (jessie, stretch, buster). $ sudo apt-get install gnupg1 apt-transport-https dirmngr $ export INSTALL_KEY=379CE192D401AB61 $ export DEB_DISTRO=$(lsb_release -sc) $ sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys $INSTALL_KEY $ echo \"deb https://ookla.bintray.com/debian ${DEB_DISTRO} main\" | sudo tee /etc/apt/sources.list.d/speedtest.list $ sudo apt-get update $ sudo apt-get install speedtest Install Speedtest CLI on Fedora / Redhat / CentOS: Fedora has moved on to DNF for package management, but is still compatible with YUM. These instructions were tested on Fedora 31, CentOS 7 and Red Hat 8. $ sudo yum install wget $ wget https://bintray.com/ookla/rhel/rpm -O bintray-ookla-rhel.repo $ sudo mv bintray-ookla-rhel.repo /etc/yum.repos.d/ $ sudo yum install speedtest How to Use the Official Speedtest CLI Once installed you can simply call the utility by typing speedtest at the command line. This will give you all the default information that you would see on the web version of speedtest.net. Official Speedtest CLI Options The options available in the official release are more robust. Here we will outline the popular options and how to use them. List Available Speed Test Servers Using the -L (servers) option will give you a list of servers available to run a test against. This option will only show you servers that are nearby. What exactly determines nearby is undefined. But for me it looks like they are staying in the tri-state area (PA, NJ, DE). [[emailprotected] ~]$ speedtest -L Closest servers: ID Name Location Country 4847 Hotwire Fision Philadelphia, PA United States 10979 School District of Philadelphia Philadelphia, PA United States 9840 Comcast New Castle, DE United States 11612 KamaTera INC New Jersey, NJ United States ...OUTPUT TRUNCATED... Optionally, you can use the -o (host) option and specify the FQDN of the server instead of the ID. But oddly, I dont see a way to get the FQDN of the servers on the list. I am guessing this option is available for using a custom server. I havent found a way to list all servers. If you are looking to test against a server on the other side of the country, you will have to find it another way. Select Specific Server to Run Speed Test Against You can use the -s (server-id) option to select a server to use from the list. You must supply the server ID with this option. The server ID is the number in the first column of the list output above. [[emailprotected] ~]$ speedtest -s 4847 Speedtest by Ookla Server: Hotwire Fision - Philadelphia, PA (id = 4847) Change Unit Used for Speed Output The -u (unit) option can display the speed output in many different formats. Decimal prefix, bits per second: bps, kbps, Mbps, Gbps Decimal prefix, bytes per second: B/s, kB/s, MB/s, GB/s Binary prefix, bits per second: kibps, Mibps, Gibps Binary prefix, bytes per second: kiB/s, MiB/s, GiB/s [[emailprotected] ~]$ speedtest -u MiB/s Speedtest by Ookla Server: Hotwire Fision - Philadelphia, PA (id = 4847) ISP: Verizon Fios Latency: 10.67 ms ( 0.95 ms jitter) Download: 63.45 MiB/s (data used: 700.2 MiB) ...OUTPUT TRUNCATED... Output Formatting Options The Ookla Speedtest CLI offers decent options for output formats. Human Readable DefaultCSV Comma Separated ValueTSV Tab Separated ValueJSON JavaScript Object NotationJSONL JSON LinesJSON-PRETTY JSON Pretty Printed Here is an example using json-pretty. [[emailprotected] ~]$ speedtest -f json-pretty { \"type\": \"result\", \"timestamp\": \"2019-11-17T16:42:06Z\", \"ping\": { \"jitter\": 0.29899999999999999, \"latency\": 17.474 }, \"download\": { \"bandwidth\": 92184614, \"bytes\": 491967724, \"elapsed\": 5303 }, \"upload\": { \"bandwidth\": 45010100, \"bytes\": 313859035, \"elapsed\": 6714 }, ...OUTPUT TRUNCATED... Conclusion Running a speed test from the command line may not be something that is needed on a daily basis for most people. However, it may prove useful in some troubleshooting situations. In this article we cover how to run a speed test from the command line using two similar tools. The unofficial python script and the official Ookla Speedtest CLI. We discussed installing, using and setting options for each one. This should be enough to get you started. For more information on these tools, visit their respective home pages found in the resources section below. Resources and Links: Ookla Speedtest CLISpeedtest-cli (Python Version) on GitHub ",
        "Note that some ISPs prioritise traffic to known speedtest targets, turning off traffic shaping rules that might otherwise slow bulk transfers. When this happens it means you are testing the likely maximum throughput of your connection not necessarily the throughput you will see more generally.<p>This is why Netflix started fast.com - because it draws data from the same distribution points as their video streaming apps it means you can't prioritise the speedtest without also doing so for the video traffic or (more likely) you can't de-prioritise the video traffic without also getting bad scores in that particular speedtest. From Netflix's point of view it is an answer to people contacting support with \"my speedtest results are fine, the problem must be your servers\" when they are experiencing video lag/drops and other such problems and the issue is due to ISP traffic shaping or the ISP simply not having enough backhaul bandwidth.<p>A more reliable test might be taking part in a busy public torrent: that way you are testing against arbitrary locations so your ISP can't be setting different shaping rules for them. Just remember to throttle upstream when testing downstream and vice-versa or saturation in the other direction will slow control packets that will in turn give you lower results for the one you are testing. This may fall into another trap though: unless you limit the number of active streams it may be an unrealistic test as more generally most processes use a small number of streams (or just a single one), and if you limit the number of streams too much you might get a lower result because each swarm member you connect to may be fairly saturated and sharing its bandwidth amongst many connections.",
        "speedtest-cli is <i>garbage</i> if you have >100Mbps Speeds. The dev refuses to acknowledge this: <a href=\"https://github.com/sivel/speedtest-cli/issues/226\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/226</a><p>Also not just me:\n<a href=\"https://github.com/sivel/speedtest-cli/issues/649\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/649</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/648\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/648</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/641\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/641</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/616\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/616</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/601\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/601</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/588\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/588</a><p><a href=\"https://github.com/sivel/speedtest-cli/issues/546\" rel=\"nofollow\">https://github.com/sivel/speedtest-cli/issues/546</a>"
      ],
      "relevant": "true"
    },
    {
      "id": 20449610,
      "title": "Show HN: OctoSQL – Query and join multiple databases and files, written in Go",
      "search": [
        "Show HN: OctoSQL – Query and join multiple databases and files, written in Go",
        "https://github.com/cube2222/octosql",
        "OctoSQL is a query tool that allows you to join, analyse and transform data from multiple databases, streaming sources and file formats using SQL. OctoSQL is currently being rewritten on the redesign branch. Problems OctoSQL Solves You need to join / analyze data from multiple datasources. Think of enriching an Excel file by joining it with a PostgreSQL database. You need stream aggregates over time, with live output updates. Think of a live-updated leaderboard with cat images based on a \"like\" event stream. You need aggregate streams per time window, with live output updates. Think of a unique user count per hour, per country live summary. Table of Contents What is OctoSQL? Installation Quickstart Temporal SQL Features Watermarks Triggers Retractions Example Durability Configuration JSON CSV Excel Parquet PostgreSQL MySQL Redis Kafka Documentation Architecture Datasource Pushdown Operations Roadmap What is OctoSQL? OctoSQL is a SQL query engine which allows you to write standard SQL queries on data stored in multiple SQL databases, NoSQL databases, streaming sources and files in various formats trying to push down as much of the work as possible to the source databases, not transferring unnecessary data. OctoSQL does that by creating an internal representation of your query and later translating parts of it into the query languages or APIs of the source databases. Whenever a datasource doesn't support a given operation, OctoSQL will execute it in memory, so you don't have to worry about the specifics of the underlying datasources. OctoSQL also includes temporal SQL extensions, to operate ergonomically on streams and respect their event-time (not the current system-time when the records are being processed). With OctoSQL you don't need O(n) client tools or a large data analysis system deployment. Everything's contained in a single binary. Why the name? OctoSQL stems from Octopus SQL. Octopus, because octopi have many arms, so they can grasp and manipulate multiple objects, like OctoSQL is able to handle multiple datasources simultaneously. Installation Either download the binary for your operating system (Linux, OS X and Windows are supported) from the Releases page, or install using the go command line tool: GO111MODULE=on go get -u github.com/cube2222/octosql/cmd/octosql Quickstart Let's say we have a csv file with cats, and a redis database with people (potential cat owners). Now we want to get a list of cities with the number of distinct cat names in them and the cumulative number of cat lives (as each cat has up to 9 lives left). First, create a configuration file (Configuration Syntax) For example: dataSources: - name: cats type: csv config: path: \"~/Documents/cats.csv\" - name: people type: redis config: address: \"localhost:6379\" password: \"\" databaseIndex: 0 databaseKeyName: \"id\" Then, set the OCTOSQL_CONFIG environment variable to point to the configuration file. export OCTOSQL_CONFIG=~/octosql.yaml You can also use the --config command line argument. Finally, query to your hearts desire: octosql \"SELECT p.city, FIRST(c.name), COUNT(DISTINCT c.name) cats, SUM(c.livesleft) catlives FROM cats c JOIN people p ON c.ownerid = p.id GROUP BY p.city ORDER BY catlives DESC LIMIT 9\" Example output: +---------+--------------+------+----------+ | p.city | c.name_first | cats | catlives | +---------+--------------+------+----------+ | Warren | Zoey | 68 | 570 | | Gadsden | Snickers | 52 | 388 | | Staples | Harley | 54 | 383 | | Buxton | Lucky | 45 | 373 | | Bethany | Princess | 46 | 366 | | Noxen | Sheba | 49 | 361 | | Yorklyn | Scooter | 45 | 359 | | Tuttle | Toby | 57 | 356 | | Ada | Jasmine | 49 | 351 | +---------+--------------+------+----------+ You can choose between live-table batch-table live-csv batch-csv stream-json output formats. (The live-* types will update the terminal view repeatedly every second, the batch-* ones will write the output once before exiting, the stream-* ones will print records whenever they are available) Temporal SQL Features OctoSQL features temporal SQL extensions inspired by the paper One SQL to Rule Them All. Introduction Often when you're working with streams of events, you'd like to use the time dimension somehow: Calculate average values for a day sliced by hours. Get unique user counts per day. and others All those examples have one thing in common: The time value of an event is crucial for correctness. A naive system could just use the current clock time whenever it receives an event. The correctness of this approach however, degrades quickly in the face of network problems, delivery delays, clock skew. This can be solved by using a value from the event as its time value. A new problem arises though: how do I know that I've received all events up to time X and can publish results for a given hour. You never know if there isn't somewhere a delayed event which should be factored in. This is where watermarks come into play. Watermarks Watermarks are a heuristic which try to approximate the \"current time\" when processing events. Said differently: When I receive a watermark for 12:00 I can be sure enough I've received all events of interest up to 12:00. To achieve this, they are generated at streaming sources and propagate downstream through the whole processing pipeline. The generation of watermarks usually relies on heuristics which provide satisfactory results for our given use case. OctoSQL currently contains the following watermark generators: Maximum difference watermark generator (with an offset argument) With an offset of 10 seconds, this generator says: When I've received an event for 12:00:00, then I'm sure I won't receive any event older than 11:59:50. Percentile watermark generator (with a percentile argument) With a percentile of 99.5, it will look at a specified number of recent events, and generate a watermark so that 99.5% of those events are after the watermark (not yet triggered), and the remaining 0.5% are before it. This way we set the watermark so that only a fraction of the recently seen events is potentially ignored as being late. Watermark generators are specified using table valued functions and are documented in the wiki. Triggers Another matter is triggering of keys in aggregations. Sometimes you'd like to only see the value for a given key (hour) when you know it's done, but othertimes you'd like to see partial results (how's the unique user count going this hour). That's where you can use triggers. Triggers allow you to specify when a given aggregate (or join window for that matter) is emitted or updated. OctoSQL contains multiple triggers: Watermark Trigger This is the most straightforward trigger. It emits a value whenever the watermark for a given key (or the end of the stream) is reached. So basically the \"show me when it's done\". Counting Trigger (with a count argument) This trigger will emit a value for a key every time it receives count records with this key. The count is reset whenever the key is triggered. Delay Trigger (with a delay argument) This trigger will emit a value for a key whenever the key has been inactive for the delay period. You can use multiple triggers simultaneously. (Show me the current sum every 10 received events, but also the final value after having received the watermark.) Retractions A key can be triggered multiple times with partial results. How do we know a given record is a retriggering of some key, and not a new unrelated record? OctoSQL solves this problem using a dataflow-like architecture. This means whenever a new value is sent for a key, a retraction is send for the old value. In practice this means every update is accompanied by the old record with an undo flag set. This can be visible when using a stream-* output format with partial results. Example Now we can see how it all fits together. In this example we have an events file, which contains records about points being scored in a game by multiple teams. WITH with_watermark AS (SELECT * FROM max_diff_watermark(source=>TABLE(events), offset=>INTERVAL 5 SECONDS, time_field=>DESCRIPTOR(time)) e), with_tumble AS (SELECT * FROM tumble(source=>TABLE(with_watermark), time_field=>DESCRIPTOR(e.time), window_length=> INTERVAL 1 MINUTE, offset => INTERVAL 0 SECONDS) e), counts_per_team AS (SELECT e.window_end, e.team, COUNT(*) as goals FROM with_tumble e GROUP BY e.window_end, e.team TRIGGER COUNTING 100, ON WATERMARK) SELECT * FROM counts_per_team cpt ORDER BY cpt.window_end DESC, cpt.goals ASC, cpt.team DESC We use common table expressions to break the query up into multiple stages. First we create the with_watermark intermediate table/stream. Here we use the table valued function max_diff_watermark to add watermarks to the events table - with an offset of 5 seconds based on the time record field. Then we use this intermediate table to create the with_tumble table, where we use the tumble table valued function to add a window_start and window_end field to each record, based on the record's time field. This assigns the records to 1 minute long windows. Next we create the counts_per_team table, which groups the records by their window end and team. Finally, we order those results by window end, goal count and team. Durability OctoSQL in its current design is based on on-disk transactional storage. All state is saved this way. All interactions with datasources are designed so that no records get duplicated in the face of errors or application restarts. You can also kill the OctoSQL process and start it again with the same query and storage-directory (command line argument), it will start where it left off. By default, OctoSQL will create a temporary directory for the state and delete it after termination. Configuration The configuration file has the following form dataSources: - name: <table_name_in_octosql> type: <datasource_type> config: <datasource_specific_key>: <datasource_specific_value> <datasource_specific_key>: <datasource_specific_value> ... - name: <table_name_in_octosql> type: <datasource_type> config: <datasource_specific_key>: <datasource_specific_value> <datasource_specific_key>: <datasource_specific_value> ... ... physical: physical_plan_option: <value> Available OctoSQL-wide configuration options are: physical groupByParallelism: The parallelism of group by's and distinct queries. Will default to the CPU core count of your machine. streamJoinParallelism: The parallelism of streaming joins. Will default to the CPU core count of your machine. execution lookupJoinPrefetchCount: The count of simultaneously processed records in a lookup join. Supported Datasources JSON JSON file in one of the following forms: one record per line, no commas JSON list of records options: path - path to file containing the data, required arrayFormat - if the JSON list of records format should be used, optional: defaults to false batchSize - number of records extracted from json file in one storage transaction, optional: defaults to 1000 CSV CSV file separated using commas. The file may or may not have column names as it's first row. options: path - path to file containing the data, required headerRow - whether the first row of the CSV file contains column names or not, optional: defaults to true separator - columns separator, optional: defaults to \",\" batchSize - number of records extracted from csv file in one storage transaction, optional: defaults to 1000 Excel A single table in an Excel spreadsheet. The table may or may not have column names as it's first row. The table can be in any sheet, and start at any point, but it cannot contain spaces between columns nor spaces between rows. options: path - path to file, required headerRow - does the first row contain column names, optional: defaults to true sheet - name of the sheet in which data is stored, optional: defaults to \"Sheet1\" rootCell - name of cell (i.e \"A3\", \"BA14\") which is the leftmost cell of the first, optional: defaults to \"A1\" timeColumns - a list of columns to parse as datetime values with second precision row, optional: defaults to [] batchSize - number of records extracted from excel file in one storage transaction, optional: defaults to 1000 Parquet A single Parquet file. Nested repeated elements are not supported. Otherwise repeated xor nested elements are supported. Currently unsupported logical types, they will get parsed as the underlying primitive type: - ENUM - TIME with NANOS precision - TIMESTAMP with NANOS precision (both UTC and non-UTC) - INTERVAL - MAP options path - path to file, required batchSize - number of records extracted from parquet file in one storage transaction, optional: defaults to 1000 PostgreSQL Single PostgreSQL database table. options: address - address including port number, optional: defaults to localhost:5432 user - required password - required databaseName - required tableName - required batchSize - number of records extracted from PostgreSQL database in one storage transaction, optional: defaults to 1000 MySQL Single MySQL database table. options: address - address including port number, optional: defaults to localhost:3306 user - required password - required databaseName - required tableName - required batchSize - number of records extracted from MySQL database in one storage transaction, optional: defaults to 1000 Redis Redis database with the given index. Currently only hashes are supported. options: address - address including port number, optional: defaults to localhost:6379 password - optional: defaults to \"\" databaseIndex - index number of Redis database, optional: defaults to 0 databaseKeyName - column name of Redis key in OctoSQL records, optional: defaults to \"key\" batchSize - number of records extracted from Redis database in one storage transaction, optional: defaults to 1000 Kafka Multi-partition kafka topic. optional brokers - list of broker addresses (separately hosts and ports) used to connect to the kafka cluster, optional: defaults to [\"localhost:9092\"] topic - name of topic to read messages from, required partitions - topic partition count, optional: defaults to 1 startOffset - offset from which the first batch of messages will be read, optional: defaults to -1 batchSize - number of records extracted from Kafka in one storage transaction, optional: defaults to 1 json - should the messages be decoded as JSON, optional: defaults to false Documentation Documentation for the available functions: https://github.com/cube2222/octosql/wiki/Function-Documentation Documentation for the available aggregates: https://github.com/cube2222/octosql/wiki/Aggregate-Documentation Documentation for the available triggers: https://github.com/cube2222/octosql/wiki/Trigger-Documentation Documentation for the available table valued functions: https://github.com/cube2222/octosql/wiki/Table-Valued-Functions-Documentation The SQL dialect documentation: TODO ;) in short though: Available SQL constructs: Select, Where, Order By, Group By, Offset, Limit, Left Join, Right Join, Inner Join, Distinct, Union, Union All, Subqueries, Operators, Table Valued Functions, Trigger, Common Table Expressions. Available SQL types: Int, Float, String, Bool, Time, Duration, Tuple (array), Object (e.g. JSON) Describe You can describe the current plan in graphviz format using the -describe flag, like this: octosql \"...\" --describe | dot -Tpng > output.png Architecture An OctoSQL invocation gets processed in multiple phases. SQL AST First, the SQL query gets parsed into an abstract syntax tree. This phase only rules out syntax errors. Logical Plan The SQL AST gets converted into a logical query plan. This plan is still mostly a syntactic validation. It's the most naive possible translation of the SQL query. However, this plan already has more of a map-filter-reduce form. If you wanted to add a new query language to OctoSQL, the only problem you'd have to solve is translating it to this logical plan. Physical Plan The logical plan gets converted into a physical plan. This conversion finds any semantic errors in the query. If this phase is reached, then the input is correct and OctoSQL will be able execute it. This phase already understands the specifics of the underlying datasources. So it's here where the optimizer will iteratively transform the plan, pushing computation nodes down to the datasources, and deduplicating unnecessary parts. The optimizer uses a pattern matching approach, where it has rules for matching parts of the physical plan tree and how those patterns can be restructured into a more efficient version. The rules are meant to be as simple as possible and make the smallest possible changes. For example, pushing filters under maps, if they don't use any mapped variables. This way, the optimizer just keeps on iterating on the whole tree, until it can't change anything anymore. (each iteration tries to apply each rule in each possible place in the tree) This ensures that the plan reaches a local performance minimum, and the rules should be structured so that this local minimum is equal - or close to - the global minimum. (i.e. one optimization, shouldn't make another - much more useful one - impossible) Here is an example diagram of an optimized physical plan: Execution Plan The physical plan gets materialized into an execution plan. This phase has to be able to connect to the actual datasources. It may initialize connections, open files, etc. Stream Starting the execution plan creates a stream, which underneath may hold more streams, or parts of the execution plan to create streams in the future. This stream works in a pull based model. Datasource Pushdown Operations Datasource Equality In > < <= >= MySQL supported supported supported PostgreSQL supported supported supported Redis supported supported scan Kafka scan scan scan Parquet scan scan scan JSON scan scan scan CSV scan scan scan Where scan means that the whole table needs to be scanned for each access. Telemetry OctoSQL sends application telemetry on each run to help us gauge user interest and feature use. This way we know somebody uses our software, feel our work is actually useful and can prioritize features based on actual usefulness. You can turn it off (though please don't) by setting the OCTOSQL_TELEMETRY environment variable to 0. Telemetry is also fully printed in the output log of OctoSQL, if you want to see what precisely is being sent. Roadmap Additional Datasources. SQL Constructs: JSON Query HAVING, ALL, ANY Push down functions, aggregates to databases that support them. An in-memory index to save values of subqueries and save on rescanning tables which don't support a given operation, so as not to recalculate them each time. Runtime statistics Server mode Querying a json or csv table from standard input. Integration test suite Tuple splitter, returning the row for each tuple element, with the given element instead of the tuple. ",
        "Hey, one of the authors here.<p>The motivation behind this project is that I always wanted a simple commandline tool allowing me to join data from different places, without needing to set up stuff like presto or spark. On another hand, I never encountered any tool which allows me to easily query csv and json data using SQL (which at least in my opinion is fairly ergonomic to use).<p>This started as an university project, but we're now continuing it as an open source one, as it's been a great success so far.<p>Anyways, feedback greatly requested and appreciated!",
        "Really cool, thanks for sharing. You all might want to look at Apache Calcite (<a href=\"http://calcite.apache.org/\" rel=\"nofollow\">http://calcite.apache.org/</a>) as well for inspiration, which has similar functionality as a subset of its features!"
      ],
      "relevant": "false"
    },
    {
      "id": 19141001,
      "title": "Ludwig, a code-free deep learning toolbox",
      "search": [
        "Ludwig, a code-free deep learning toolbox",
        "https://eng.uber.com/introducing-ludwig/",
        "Over the last decade, deep learning models have proven highly effective at performing a wide variety of machine learning tasks in vision, speech, and language. At Uber we are using these models for a variety of tasks, including customer support, object detection, improving maps, streamlining chat communications, forecasting, and preventing fraud. Many open source libraries, including TensorFlow, PyTorch, CNTK, MXNET, and Chainer, among others, have implemented the building blocks needed to build such models, allowing for faster and less error-prone development. This, in turn, has propelled the adoption of such models both by the machine learning research community and by industry practitioners, resulting in fast progress in both architecture design and industrial solutions. At Uber AI, we decided to avoid reinventing the wheel and to develop packages built on top of the strong foundations open source libraries provide. To this end, in 2017 we released Pyro, a deep probabilistic programming language built on PyTorch, and continued to improve it with the help of the open source community. Another major open source AI tool created by Uber is Horovod, a framework hosted by the LF Deep Learning Foundation that allows distributed training of deep learning models over multiple GPUs and several machines. Extending our commitment to making deep learning more accessible, we are releasing Ludwig, an open source, deep learning toolbox built on top of TensorFlow that allows users to train and test deep learning models without writing code. Ludwig is unique in its ability to help make deep learning easier to understand for non-experts and enable faster model improvement iteration cycles for experienced machine learning developers and researchers alike. By using Ludwig, experts and researchers can simplify the prototyping process and streamline data processing so that they can focus on developing deep learning architectures rather than data wrangling. Ludwig We have been developing Ludwig internally at Uber over the past two years to streamline and simplify the use of deep learning models in applied projects, as they usually require comparisons among different architectures and fast iteration. We have witnessed its value to several of Ubers own projects, including our Customer Obsession Ticket Assistant (COTA), information extraction from driver licenses, identification of points of interest during conversations between driver-partners and riders, food delivery time prediction, and much more. For this reason we decided to release it as open source, as we believe there is no other solution currently available with the same ease of use and flexibility. We originally designed Ludwig as a generic tool for simplifying the model development and comparison process when dealing with new applied machine learning problems. In order to do so, we drew inspiration from other machine learning software: from Wekaand MLlib, the idea of working directly with raw data and providing a certain number of pre-built models; from Caffe, the declarative nature of the definition file; and from scikit-learn, its simple programmatic API. This mix of influences makes it a pretty different tool from the usual deep learning libraries that provide tensor algebra primitives and few other utilities to code models, while at the same time making it more general than other specialized libraries like PyText, StanfordNLP, AllenNLP, and OpenCV. Ludwig provides a set of model architectures that can be combined together to create an end-to-end model for a given use case. As an analogy, if deep learning libraries provide the building blocks to make your building, Ludwig provides the buildings to make your city, and you can chose among the available buildings or add your own building to the set of available ones. The core design principles we baked into the toolbox are: No coding required: no coding skills are required to train a model and use it for obtaining predictions. Generality: a new data type-based approach to deep learning model design that makes the tool usable across many different use cases. Flexibility: experienced users have extensive control over model building and training, while newcomers will find it easy to use. Extensibility: easy to add new model architecture and new feature data types. Understandability: deep learning model internals are often considered black boxes, but we provide standard visualizations to understand their performance and compare their predictions. Ludwig allows its users to train a deep learning model by providing just a tabular file (like CSV) containing the data and a YAML configuration file that specifies which columns of the tabular file are input features and which are output target variables. The simplicity of the configuration file enables faster prototyping, potentially reducing hours of coding down to a few minutes. If more than one output target variable is specified, Ludwig will perform multi-task learning, learning to predict all the outputs simultaneously, a task that usually requires custom code. The model definition can contain additional information, in particular preprocessing information for each feature in the dataset, which encoder or decoder to use for each feature, architectural parameters for each encoder and decoder, and training parameters. Default values of preprocessing, training, and various model architecture parameters are chosen based on our experience or are adapted from the academic literature, allowing novices to easily train complex models. At the same time, the ability to set each of them individually in the model configuration file offers full flexibility to experts. Each model trained with Ludwig is saved and can be loaded at a later time to obtain predictions on new data. As an example, models can be loaded in a serving environment to provide predictions in software applications. Figure 1: Several input and output features may be specified in Ludwigs model description file, and their combination covers many machine learning tasks. The main new idea that Ludwig introduces is the notion of data type-specific encoders and decoders, which results in a highly modularized and extensible architecture: each type of data supported (text, images, categories, and so on) has a specific preprocessing function. In short, encoders map the raw data to tensors, and decoders map tensors to the raw data. With this design, the user has access to combiners (glue components of the architecture) that combine the tensors from all input encoders, process them, and return the tensors to be used for the output decoders. For instance, Ludwigs default concat combiner concatenates the outputs of different encoders, passes them through fully connected layers, and provides the final activation as input for output decoders. Other combiners are available for other use cases, and many more can be easily added by implementing a simple function interface. By composing these data type-specific components, users can make Ludwig train models on a wide variety of tasks. For example, by combining a text encoder and a category decoder, the user can obtain a text classifier, while combining an image encoder and a text decoder will enable the user to obtain an image captioning model. Each data type may have more than one encoder and decoder. For instance, text can be encoded with a convolutional neural network (CNN), a recurrent neural network (RNN), or other encoders. The user can then specify which one to use and its hyperparameters directly in the model definition file without having to write a single line of code. This versatile and flexible encoder-decoder architecture makes it easy for less experienced deep learning practitioners to train models for diverse machine learning tasks, such as text classification, object classification, image captioning, sequence tagging, regression, language modeling, machine translation, time series forecasting, and question answering. This opens up a variety of use cases that would typically be out of reach forinexperienced practitioners, and allows users experienced in one domain to approach new domains. At the moment, Ludwig contains encoders and decoders for binary values, float numbers, categories, discrete sequences, sets, bags, images, text, and time series, together with the capability to load some pre-trained models (for instance word embeddings), but we plan to expand the supported data types in future releases. In addition to its accessibility and flexible architecture, Ludwig also offers additional benefits for non-programmers. Ludwig incorporates a set of command line utilities for training, testing models, and obtaining predictions. Furthering its ease-of-use, the toolbox provides a programmatic API that allows users to train and use a model with just a couple lines of code. Additionally, it includes a suite of other tools for evaluating models, comparing their performance and predictions through visualizations and extracting both model weights and activations from them. Finally, the ability to train models on multiple GPUs locally and in a distributed fashion through the use of Horovod, an open source distributed training framework, makes it possible to iterate on models and obtain results quickly. Using Ludwig To better understand how to use Ludwig for real-world applications, lets build a simple model with the toolbox. In this example, we create a model that predicts a books genre and price given its title, author, description, and cover. Training the model Our book dataset looks like the following: title author description cover genre price Do Androids Dream of Electric Sheep? Philip K. Dick By 2021, the World War has killed millions, driving entire species into extinction and sending mankind off-planet. path-to-image/do-android-cover.jpg sci-fi 9.32 War and Peace Leo Tolstoy War and Peace broadly focuses on Napoleons invasion of Russia in 1812 and follows three of the most well-known characters in literature path-to-image/war-and-peace-cover.jpg historical 5.42 The Name of the Rose Umberto Eco In 1327, Brother William of Baskerville is sent to investigate a wealthy Italian abbey whose monks are suspected of heresy. .. path-to-image/name-of-the-rose-cover.jpg historical 16.99 In order to learn a model that uses the content of the title, author, description, and cover columns as inputs to predict the values in the genre and price columns, the model definition YAML would be: input_features: name: title type: text name: author type: category name: description type: text name: cover type: image output_features: name: genre type: category name: price type: numerical training: epochs: 10 We start the training by typing the following command in our console: ludwig train data_csv path/to/file.csv model_definition_file model_definition.yaml With this command, Ludwig performs a random split of the data in training, validation, and test sets, preprocess them, and builds four different encoders for the four inputs and one combiner and two decoders for the two output targets. Then, it trains the model on the training set until the accuracy on the validation set stops improving or the maximum number of ten epochs is reached. Training progress will be displayed in the console, but TensorBoard can also be used. Text features are encoded by default with a CNN encoder, but we could use, say, an RNN encoder that uses a a bidirectional LSTM with a state size of 200 for encoding the title instead. We would only need to change the title encoder definition to: name: title type: text encoder: rnn cell_type: lstm bidirectional: true If we wanted to change training parameters like number of epochs, learning rate, and batch size, we would change the model definition like this: input_features: output_features: training: epochs: 100 learning_rate: 0.001 batch_size: 64 All parameters on how to perform the split and data preprocessing, the parameters of each encoder combiner and decoder have default values, but they are configurable. Refer to the user guide to discover the wide variety of model definitions and training parameters available, and take a look at our examples to see how Ludwig can be used for several different tasks. Visualizing training results After training, Ludwig creates a result directory containing the trained model with its hyperparameters and summary statistics of the training process. We can visualize them using one of the several visualization options available with the visualize tool, for instance: ludwig visualize visualization learning_curves training_stats results/training_stats.json This will display a graph that looks like the following, showing the loss and accuracy as functions of train epoch number: Figure 2: These learning curves show loss and accuracy over training epochs. Several visualizations are available. The visualization section in the user guideoffers more details. Predicting results with trained models Users with new data who want their previously trained models to predict target output values can type the following command: ludwig predict data_csv path/to/data.csv model_path /path/to/model If a dataset contains ground truth information to compare with predictions, running this command returns model predictions and also some test performance statistics. These can be visualized via the visualize command (above), which can also be used to compare the performance and results prediction of different models. For instance: ludwig visualize visualization compare_performance test_stats path/to/test_stats_model_1.json path/to/test_stats_model_2.json will return a bar plot comparing the models on different measures: Figure 3: This bar chart compares the performance of two models. There is also a handi experiment command that performs first training and then prediction without the need to use two separate command. Using Ludwigs programmatic API Ludwig also provides a simple Python programmatic API that lets users train or load a model and use it to obtain predictions on new data: from ludwig import LudwigModel # train a model model_definition = {} model = LudwigModel(model_definition) train_stats = model.train(training_dataframe) # or load a model model = LudwigModel.load(model_path) # obtain predictions predictions = model.predict(test_dataframe) model.close() This API enables using models trained with Ludwig inside existing code to build applications on top of them. More details on using a programmatic API with Ludwig are provided in the user guide and in the API documentation. Conclusions We decided to open source Ludwig because we believe that it can be a useful tool for non-expert machine learning practitioners and experienced deep learning developers and researchers alike. The non-experts can quickly train and test deep learning models without having to write code. Experts can obtain strong baselines to compare their models against and have an experimentation setting that makes it easy to test new ideas and analyze models by performing standard data preprocessing and visualization. In future releases, we hope to add several new encoders for each data type, such as Transformer, ELMo, and BERT for text, and DenseNet and FractalNet for images. We also want to add additional data types like audio, point clouds, and graphs, while at the same time integrating more scalable solutions for managing big data sets, like Petastorm. Ludwig is built with extensibility principles in mind and, in order to facilitate contributions from the community, we provide a developer guide that showcases how simple it is to add additional data types as well as additional encoders and decoders for already existing ones. We hope you will enjoy using our tool as much as we enjoyed building it! If building the next generation of machine learning tools interests you, consider applying for a role with Uber AI! ",
        "Technically, it is not code-free - it is declarative programming in YML. You still have to specify your input_features, output_features, and training architecture/specification. This is not a drag-and-drop UI (although, you could probably layer one on top of this).<p>This should work well with a Feature Store, where features are already pre-processed and ready for input to model training. With a feature store, this could be like the Tableau/Qlik/PowerBI tool for Data Science.",
        "I have attempted to do something similar in 2017 [1]. A couple of issues I noticed:<p>* The field is evolving quite rapidly, and so most options will require a lot of configuration, which IMO is not suitable for a declarative approach.<p>* It's hard to debug, eventually you will need to dive into the code at some point.<p>* Extending the library would mean touching the code anyways.<p>One major difference here, is that Ludwig tries to expose one interface, that for instance users without python knowledge can use, and inside a company, some machine learning engineers can extend the tool and do support by debugging other users use-cases.<p>I think at some point such approach can be useful for a very limited mature use-cases.<p>[1]: <a href=\"https://github.com/polyaxon/polyaxon-lib/blob/master/examples/configs_examples/yaml_configs/alexnet_flower17.yml\" rel=\"nofollow\">https://github.com/polyaxon/polyaxon-lib/blob/master/example...</a>"
      ],
      "relevant": "false"
    },
    {
      "id": 20993747,
      "title": "Show HN: Statistical tool for analyzing a Git repository",
      "search": [
        "Show HN: Statistical tool for analyzing a Git repository",
        "https://github.com/arzzen/git-quick-stats/",
        "GIT quick statistics git-quick-stats is a simple and efficient way to access various statistics in a git repository. Any git repository may contain tons of information about commits, contributors, and files. Extracting this information is not always trivial, mostly because there are a gadzillion options to a gadzillion git commands I dont think there is a single person alive who knows them all. Probably not even Linus Torvalds himself :). Table of Contents Screenshots Usage Interactive Non-interactive Command-line arguments Git log since and until Git log limit Git log options Git pathspec Git merge view strategy Color themes Installation UNIX and Linux macOS Windows Docker System requirements Dependencies FAQ Contribution Code reviews Some tips for good pull requests Formatting Tests Licensing Contributors Backers Sponsors Screenshots Usage Interactive git-quick-stats has a built-in interactive menu that can be executed as such: Or Non-interactive For those who prefer to utilize command-line options, git-quick-stats also has a non-interactive mode supporting both short and long options: git-quick-stats <optional-command-to-execute-directly> Or git quick-stats <optional-command-to-execute-directly> Command-line arguments Possible arguments in short and long form: GENERATE OPTIONS -T, --detailed-git-stats give a detailed list of git stats -R, --git-stats-by-branch see detailed list of git stats by branch -c, --changelogs see changelogs -L, --changelogs-by-author see changelogs by author -S, --my-daily-stats see your current daily stats -V, --csv-output-by-branch output daily stats by branch in CSV format -j, --json-output save git log as a JSON formatted file to a specified area LIST OPTIONS -b, --branch-tree show an ASCII graph of the git repo branch history -D, --branches-by-date show branches by date -C, --contributors see a list of everyone who contributed to the repo -a, --commits-per-author displays a list of commits per author -d, --commits-per-day displays a list of commits per day -m, --commits-by-month displays a list of commits per month -w, --commits-by-weekday displays a list of commits per weekday -o, --commits-by-hour displays a list of commits per hour -A, --commits-by-author-by-hour displays a list of commits per hour by author -z, --commits-by-timezone displays a list of commits per timezone -Z, --commits-by-author-by-timezone displays a list of commits per timezone by author SUGGEST OPTIONS -r, --suggest-reviewers show the best people to contact to review code -h, -?, --help display this help text in the terminal Git log since and until You can set the variables _GIT_SINCE and/or _GIT_UNTIL before running git-quick-stats to limit the git log. These work similar to git's built-in --since and --until log options. export _GIT_SINCE=\"2017-01-20\" export _GIT_UNTIL=\"2017-01-22\" Once set, run git quick-stats as normal. Note that this affects all stats that parse the git log history until unset. Git log limit You can set variable _GIT_LIMIT for limited output. It will affect the \"changelogs\" and \"branch tree\" options. Git log options You can set _GIT_LOG_OPTIONS for git log options: export _GIT_LOG_OPTIONS=\"--ignore-all-space --ignore-blank-lines\" Git pathspec You can exclude a directory from the stats by using pathspec export _GIT_PATHSPEC=':!directory' You can also exclude files from the stats. Note that it works with any alphanumeric, glob, or regex that git respects. export _GIT_PATHSPEC=':!package-lock.json' Git merge view strategy You can set the variable _GIT_MERGE_VIEW to enable merge commits to be part of the stats by setting _GIT_MERGE_VIEW to enable. You can also choose to only show merge commits by setting _GIT_MERGE_VIEW to exclusive. Default is to not show merge commits. These work similar to git's built-in --merges and --no-merges log options. export _GIT_MERGE_VIEW=\"enable\" export _GIT_MERGE_VIEW=\"exclusive\" Git branch You can set the variable _GIT_BRANCH to set the branch of the stats. Works with commands --git-stats-by-branch and --csv-output-by-branch. export _GIT_BRANCH=\"master\" Color themes You can change to the legacy color scheme by toggling the variable _MENU_THEME between default and legacy export _MENU_THEME=\"legacy\" Installation Debian and Ubuntu If you are on at least Debian Bullseye or Ubuntu Focal you can use apt for installation: apt install git-quick-stats UNIX and Linux git clone https://github.com/arzzen/git-quick-stats.git && cd git-quick-stats sudo make install For uninstalling, open up the cloned directory and run For update/reinstall macOS (homebrew) brew install git-quick-stats Or you can follow the UNIX and Linux instructions if you wish. Windows If you are installing with Cygwin, use these scripts: installer uninstaller If you are wishing to use this with WSL, follow the UNIX and Linux instructions. Docker You can use the Docker image provided: Build: docker build -t arzzen/git-quick-stats . Run interactive menu: docker run --rm -it -v $(pwd):/git arzzen/git-quick-stats Docker pull command: docker pull arzzen/git-quick-stats docker repository System requirements An OS with a Bash shell Tools we use: awk basename cat column echo git grep head printf seq sort tput tr uniq wc Dependencies bsdmainutils apt install bsdmainutils FAQ Q: I get some errors after run git-quick-stats in cygwin like /usr/local/bin/git-quick-stats: line 2: $'\\r': command not found A: You can run the dos2unix app in cygwin as follows: /bin/dos2unix.exe /usr/local/bin/git-quick-stats. This will convert the script from the CR-LF convention that Microsoft uses to the LF convention that UNIX, OS X, and Linux use. You should then should be able to run it as normal. Q: How they could be used in a project with many git projects and statistics would show a summary of all git projects? A: If you want to include submodule logs, you can try using the following: export _GIT_LOG_OPTIONS=\"-p --submodule=log\" (more info about git log --submodule) Contribution Want to contribute? Great! First, read this page. Code reviews All submissions, including submissions by project members, require review. We use GitHub pull requests for this purpose. Some tips for good pull requests Use our code When in doubt, try to stay true to the existing code of the project. Write a descriptive commit message. What problem are you solving and what are the consequences? Where and what did you test? Some good tips: here and here. If your PR consists of multiple commits which are successive improvements / fixes to your first commit, consider squashing them into a single commit (git rebase -i) such that your PR is a single commit on top of the current HEAD. This make reviewing the code so much easier, and our history more readable. Formatting This documentation is written using standard markdown syntax. Please submit your changes using the same syntax. Tests Licensing MIT see LICENSE for the full license text. Contributors This project exists thanks to all the people who contribute. Backers Thank you to all our backers! [Become a backer] Sponsors Support this project by becoming a sponsor. Your logo will show up here with a link to your website. [Become a sponsor] ",
        "Pull Panda [0] is another tool we've been using that is offered as a SaaS and is now free since it was acquired by Github this year. It tells you average PR review time, average PR diff size, who is most requested for review, review-comment ratio, etc. I can't believe it's taken Github so long to make progress on dashboards like this for engineering managers, but looking forward to the time Pull Panda is fully integrated.<p>[0] <a href=\"https://pullreminders.com\" rel=\"nofollow\">https://pullreminders.com</a>",
        "I made a tool myself [0] with slightly different tools. It answers two questions: Who are the relevant coders and what parts of the code are the hotspots?<p>Here is an example run on sqlite:<p><pre><code>    Top Committers (of 28 authors):\n    D. Richard Hipp      13359 commits during 19 years until 2019-09-17\n    Dan Kennedy          5813 commits during 17 years until 2019-09-16\n     together these authors have 80+% of the commits (19172/20987)\n\n    Files with most commits:\n    1143 commits: src/sqlite.h.in      during 19 years until 2019-09-16\n    1331 commits: src/where.c          during 19 years until 2019-09-03\n    1360 commits: src/btree.c          during 18 years until 2019-08-24\n    1650 commits: src/vdbe.c           during 19 years until 2019-09-16\n    1893 commits: src/sqliteInt.h      during 19 years until 2019-09-14\n\n    Files with most authors:\n    11 authors: src/main.c          \n    11 authors: src/sqliteInt.h     \n    12 authors: configure.ac        \n    12 authors: src/shell.c         \n    15 authors: Makefile.in         \n\n    By file extension:\n    .test: 1333 files\n       .c: 379 files\n     together these make up 80+% of the files (1712/2138)\n</code></pre>\n[0] <a href=\"https://github.com/qznc/dot/blob/master/bin/git-overview\" rel=\"nofollow\">https://github.com/qznc/dot/blob/master/bin/git-overview</a>"
      ],
      "relevant": "true"
    },
    {
      "id": 19023196,
      "title": "Show HN: Apprise – A lightweight all-in-one notification solution",
      "search": [
        "Show HN: Apprise – A lightweight all-in-one notification solution",
        "https://github.com/caronc/apprise",
        "apprise / verb To inform or tell (someone). To make one aware of something. Apprise allows you to send a notification to almost all of the most popular notification services available to us today such as: Telegram, Discord, Slack, Amazon SNS, Gotify, etc. One notification library to rule them all. A common and intuitive notification syntax. Supports the handling of images and attachments (to the notification services that will accept them). It's incredibly lightweight. Amazing response times because all messages sent asynchronously. Developers who wish to provide a notification service no longer need to research each and every one out there. They no longer need to try to adapt to the new ones that comeout thereafter. They just need to include this one library and then they can immediately gain access to almost all of the notifications services available to us today. System Administrators and DevOps who wish to send a notification now no longer need to find the right tool for the job. Everything is already wrapped and supported within the apprise command line tool (CLI) that ships with this product. Supported Notifications The section identifies all of the services supported by this library. Check out the wiki for more information on the supported modules here. Popular Notification Services The table below identifies the services this tool supports and some example service urls you need to use in order to take advantage of it. Click on any of the services listed below to get more details on how you can configure Apprise to access them. Notification Service Service ID Default Port Example Syntax Apprise API apprise:// or apprises:// (TCP) 80 or 443 apprise://hostname/Token Boxcar boxcar:// (TCP) 443 boxcar://hostnameboxcar://hostname/@tagboxcar://hostname/device_tokenboxcar://hostname/device_token1/device_token2/device_tokenNboxcar://hostname/@tag/@tag2/device_token Discord discord:// (TCP) 443 discord://webhook_id/webhook_tokendiscord://avatar@webhook_id/webhook_token Emby emby:// or embys:// (TCP) 8096 emby://user@hostname/emby://user:password@hostname Enigma2 enigma2:// or enigma2s:// (TCP) 80 or 443 enigma2://hostname Faast faast:// (TCP) 443 faast://authorizationtoken FCM fcm:// (TCP) 443 fcm://project@apikey/DEVICE_IDfcm://project@apikey/#TOPICfcm://project@apikey/DEVICE_ID1/#topic1/#topic2/DEVICE_ID2/ Flock flock:// (TCP) 443 flock://tokenflock://botname@tokenflock://app_token/u:useridflock://app_token/g:channel_idflock://app_token/u:userid/g:channel_id Gitter gitter:// (TCP) 443 gitter://token/roomgitter://token/room1/room2/roomN Google Chat gchat:// (TCP) 443 gchat://workspace/key/token Gotify gotify:// or gotifys:// (TCP) 80 or 443 gotify://hostname/tokengotifys://hostname/token?priority=high Growl growl:// (UDP) 23053 growl://hostnamegrowl://hostname:portnogrowl://password@hostnamegrowl://password@hostname:portNote: you can also use the get parameter version which can allow the growl request to behave using the older v1.x protocol. An example would look like: growl://hostname?version=1 Home Assistant hassio:// or hassios:// (TCP) 8123 or 443 hassio://hostname/accesstokenhassio://user@hostname/accesstokenhassio://user:password@hostname:port/accesstokenhassio://hostname/optional/path/accesstoken IFTTT ifttt:// (TCP) 443 ifttt://webhooksID/Eventifttt://webhooksID/Event1/Event2/EventNifttt://webhooksID/Event1/?+Key=Valueifttt://webhooksID/Event1/?-Key=value1 Join join:// (TCP) 443 join://apikey/devicejoin://apikey/device1/device2/deviceN/join://apikey/groupjoin://apikey/groupA/groupB/groupNjoin://apikey/DeviceA/groupA/groupN/DeviceN/ KODI kodi:// or kodis:// (TCP) 8080 or 443 kodi://hostnamekodi://user@hostnamekodi://user:password@hostname:port Kumulos kumulos:// (TCP) 443 kumulos://apikey/serverkey LaMetric Time lametric:// (TCP) 443 lametric://apikey@device_ipaddrlametric://apikey@hostname:portlametric://client_id@client_secret Mailgun mailgun:// (TCP) 443 mailgun://user@hostname/apikeymailgun://user@hostname/apikey/emailmailgun://user@hostname/apikey/email1/email2/emailNmailgun://user@hostname/apikey/?name=\"From%20User\" Matrix matrix:// or matrixs:// (TCP) 80 or 443 matrix://hostnamematrix://user@hostnamematrixs://user:pass@hostname:port/#room_aliasmatrixs://user:pass@hostname:port/!room_idmatrixs://user:pass@hostname:port/#room_alias/!room_id/#room2matrixs://token@hostname:port/?webhook=matrixmatrix://user:token@hostname/?webhook=slack&format=markdown Mattermost mmost:// or mmosts:// (TCP) 8065 mmost://hostname/authkeymmost://hostname:80/authkeymmost://user@hostname:80/authkeymmost://hostname/authkey?channel=channelmmosts://hostname/authkeymmosts://user@hostname/authkey Microsoft Teams msteams:// (TCP) 443 msteams://TokenA/TokenB/TokenC/ MQTT mqtt:// or mqtts:// (TCP) 1883 or 8883 mqtt://hostname/topicmqtt://user@hostname/topicmqtts://user:pass@hostname:9883/topic Nextcloud ncloud:// or nclouds:// (TCP) 80 or 443 ncloud://adminuser:pass@host/Usernclouds://adminuser:pass@host/User1/User2/UserN Notica notica:// (TCP) 443 notica://Token/ Notifico notifico:// (TCP) 443 notifico://ProjectID/MessageHook/ Office 365 o365:// (TCP) 443 o365://TenantID:AccountEmail/ClientID/ClientSecreto365://TenantID:AccountEmail/ClientID/ClientSecret/TargetEmailo365://TenantID:AccountEmail/ClientID/ClientSecret/TargetEmail1/TargetEmail2/TargetEmailN OneSignal onesignal:// (TCP) 443 onesignal://AppID@APIKey/PlayerIDonesignal://TemplateID:AppID@APIKey/UserIDonesignal://AppID@APIKey/#IncludeSegmentonesignal://AppID@APIKey/Email Opsgenie opsgenie:// (TCP) 443 opsgenie://APIKeyopsgenie://APIKey/UserIDopsgenie://APIKey/#Teamopsgenie://APIKey/*Scheduleopsgenie://APIKey/^Escalation ParsePlatform parsep:// or parseps:// (TCP) 80 or 443 parsep://AppID:MasterKey@Hostnameparseps://AppID:MasterKey@Hostname PopcornNotify popcorn:// (TCP) 443 popcorn://ApiKey/ToPhoneNopopcorn://ApiKey/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/popcorn://ApiKey/ToEmailpopcorn://ApiKey/ToEmail1/ToEmail2/ToEmailN/popcorn://ApiKey/ToPhoneNo1/ToEmail1/ToPhoneNoN/ToEmailN Prowl prowl:// (TCP) 443 prowl://apikeyprowl://apikey/providerkey PushBullet pbul:// (TCP) 443 pbul://accesstokenpbul://accesstoken/#channelpbul://accesstoken/A_DEVICE_IDpbul://accesstoken/email@address.compbul://accesstoken/#channel/#channel2/email@address.net/DEVICE Pushjet pjet:// or pjets:// (TCP) 80 or 443 pjet://hostname/secretpjet://hostname:port/secretpjets://secret@hostname/secretpjets://hostname:port/secret Push (Techulus) push:// (TCP) 443 push://apikey/ Pushed pushed:// (TCP) 443 pushed://appkey/appsecret/pushed://appkey/appsecret/#ChannelAliaspushed://appkey/appsecret/#ChannelAlias1/#ChannelAlias2/#ChannelAliasNpushed://appkey/appsecret/@UserPushedIDpushed://appkey/appsecret/@UserPushedID1/@UserPushedID2/@UserPushedIDN Pushover pover:// (TCP) 443 pover://user@tokenpover://user@token/DEVICEpover://user@token/DEVICE1/DEVICE2/DEVICENNote: you must specify both your user_id and token PushSafer psafer:// or psafers:// (TCP) 80 or 443 psafer://privatekeypsafers://privatekey/DEVICEpsafer://privatekey/DEVICE1/DEVICE2/DEVICEN Reddit reddit:// (TCP) 443 reddit://user:password@app_id/app_secret/subredditreddit://user:password@app_id/app_secret/sub1/sub2/subN Rocket.Chat rocket:// or rockets:// (TCP) 80 or 443 rocket://user:password@hostname/RoomID/Channelrockets://user:password@hostname:443/#Channel1/#Channel1/RoomIDrocket://user:password@hostname/#Channelrocket://webhook@hostnamerockets://webhook@hostname/@User/#Channel Ryver ryver:// (TCP) 443 ryver://Organization/Tokenryver://botname@Organization/Token SendGrid sendgrid:// (TCP) 443 sendgrid://APIToken:FromEmail/sendgrid://APIToken:FromEmail/ToEmailsendgrid://APIToken:FromEmail/ToEmail1/ToEmail2/ToEmailN/ SimplePush spush:// (TCP) 443 spush://apikeyspush://salt:password@apikeyspush://apikey?event=Apprise Slack slack:// (TCP) 443 slack://TokenA/TokenB/TokenC/slack://TokenA/TokenB/TokenC/Channelslack://botname@TokenA/TokenB/TokenC/Channelslack://user@TokenA/TokenB/TokenC/Channel1/Channel2/ChannelN SMTP2Go smtp2go:// (TCP) 443 smtp2go://user@hostname/apikeysmtp2go://user@hostname/apikey/emailsmtp2go://user@hostname/apikey/email1/email2/emailNsmtp2go://user@hostname/apikey/?name=\"From%20User\" Streamlabs strmlabs:// (TCP) 443 strmlabs://AccessToken/strmlabs://AccessToken/?name=name&identifier=identifier&amount=0&currency=USD SparkPost sparkpost:// (TCP) 443 sparkpost://user@hostname/apikeysparkpost://user@hostname/apikey/emailsparkpost://user@hostname/apikey/email1/email2/emailNsparkpost://user@hostname/apikey/?name=\"From%20User\" Spontit spontit:// (TCP) 443 spontit://UserID@APIKey/spontit://UserID@APIKey/Channelspontit://UserID@APIKey/Channel1/Channel2/ChannelN Syslog syslog:// (UDP) 514 (if hostname specified) syslog://syslog://Facilitysyslog://hostnamesyslog://hostname/Facility Telegram tgram:// (TCP) 443 tgram://bottoken/ChatIDtgram://bottoken/ChatID1/ChatID2/ChatIDN Twitter twitter:// (TCP) 443 twitter://CKey/CSecret/AKey/ASecrettwitter://user@CKey/CSecret/AKey/ASecrettwitter://CKey/CSecret/AKey/ASecret/User1/User2/User2twitter://CKey/CSecret/AKey/ASecret?mode=tweet Twist twist:// (TCP) 443 twist://pasword:logintwist://password:login/#channeltwist://password:login/#team:channeltwist://password:login/#team:channel1/channel2/#team3:channel XBMC xbmc:// or xbmcs:// (TCP) 8080 or 443 xbmc://hostnamexbmc://user@hostnamexbmc://user:password@hostname:port XMPP xmpp:// or xmpps:// (TCP) 5222 or 5223 xmpp://password@hostnamexmpp://user:password@hostnamexmpps://user:password@hostname:port?jid=user@hostname/resourcexmpps://password@hostname/target@myhost, target2@myhost/resource Webex Teams (Cisco) wxteams:// (TCP) 443 wxteams://Token Zulip Chat zulip:// (TCP) 443 zulip://botname@Organization/Tokenzulip://botname@Organization/Token/Streamzulip://botname@Organization/Token/Email SMS Notification Support Notification Service Service ID Default Port Example Syntax AWS SNS sns:// (TCP) 443 sns://AccessKeyID/AccessSecretKey/RegionName/+PhoneNosns://AccessKeyID/AccessSecretKey/RegionName/+PhoneNo1/+PhoneNo2/+PhoneNoNsns://AccessKeyID/AccessSecretKey/RegionName/Topicsns://AccessKeyID/AccessSecretKey/RegionName/Topic1/Topic2/TopicN ClickSend clicksend:// (TCP) 443 clicksend://user:pass@PhoneNoclicksend://user:pass@ToPhoneNo1/ToPhoneNo2/ToPhoneNoN D7 Networks d7sms:// (TCP) 443 d7sms://user:pass@PhoneNod7sms://user:pass@ToPhoneNo1/ToPhoneNo2/ToPhoneNoN DingTalk dingtalk:// (TCP) 443 dingtalk://token/dingtalk://token/ToPhoneNodingtalk://token/ToPhoneNo1/ToPhoneNo2/ToPhoneNo1/ Kavenegar kavenegar:// (TCP) 443 kavenegar://ApiKey/ToPhoneNokavenegar://FromPhoneNo@ApiKey/ToPhoneNokavenegar://ApiKey/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN MessageBird msgbird:// (TCP) 443 msgbird://ApiKey/FromPhoneNomsgbird://ApiKey/FromPhoneNo/ToPhoneNomsgbird://ApiKey/FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ MSG91 msg91:// (TCP) 443 msg91://AuthKey/ToPhoneNomsg91://SenderID@AuthKey/ToPhoneNomsg91://AuthKey/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Nexmo nexmo:// (TCP) 443 nexmo://ApiKey:ApiSecret@FromPhoneNonexmo://ApiKey:ApiSecret@FromPhoneNo/ToPhoneNonexmo://ApiKey:ApiSecret@FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Sinch sinch:// (TCP) 443 sinch://ServicePlanId:ApiToken@FromPhoneNosinch://ServicePlanId:ApiToken@FromPhoneNo/ToPhoneNosinch://ServicePlanId:ApiToken@FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/sinch://ServicePlanId:ApiToken@ShortCode/ToPhoneNosinch://ServicePlanId:ApiToken@ShortCode/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Twilio twilio:// (TCP) 443 twilio://AccountSid:AuthToken@FromPhoneNotwilio://AccountSid:AuthToken@FromPhoneNo/ToPhoneNotwilio://AccountSid:AuthToken@FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/twilio://AccountSid:AuthToken@FromPhoneNo/ToPhoneNo?apikey=Keytwilio://AccountSid:AuthToken@ShortCode/ToPhoneNotwilio://AccountSid:AuthToken@ShortCode/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Desktop Notification Support Notification Service Service ID Default Port Example Syntax Linux DBus Notifications dbus://qt://glib://kde:// n/a dbus://qt://glib://kde:// Linux Gnome Notifications gnome:// n/a gnome:// MacOS X Notifications macosx:// n/a macosx:// Windows Notifications windows:// n/a windows:// Email Support Service ID Default Port Example Syntax mailto:// (TCP) 25 mailto://userid:pass@domain.commailto://domain.com?user=userid&pass=passwordmailto://domain.com:2525?user=userid&pass=passwordmailto://user@gmail.com&pass=passwordmailto://mySendingUsername:mySendingPassword@example.com?to=receivingAddress@example.commailto://userid:password@example.com?smtp=mail.example.com&from=noreply@example.com&name=no%20reply mailtos:// (TCP) 587 mailtos://userid:pass@domain.commailtos://domain.com?user=userid&pass=passwordmailtos://domain.com:465?user=userid&pass=passwordmailtos://user@hotmail.com&pass=passwordmailtos://mySendingUsername:mySendingPassword@example.com?to=receivingAddress@example.commailtos://userid:password@example.com?smtp=mail.example.com&from=noreply@example.com&name=no%20reply Apprise have some email services built right into it (such as yahoo, fastmail, hotmail, gmail, etc) that greatly simplify the mailto:// service. See more details here. Custom Notifications Post Method Service ID Default Port Example Syntax JSON json:// or jsons:// (TCP) 80 or 443 json://hostnamejson://user@hostnamejson://user:password@hostname:portjson://hostname/a/path/to/post/to XML xml:// or xmls:// (TCP) 80 or 443 xml://hostnamexml://user@hostnamexml://user:password@hostname:portxml://hostname/a/path/to/post/to Installation The easiest way is to install this package is from pypi: Command Line A small command line tool is also provided with this package called apprise. If you know the server url's you wish to notify, you can simply provide them all on the command line and send your notifications that way: # Send a notification to as many servers as you want # as you can easily chain one after another (the -vv provides some # additional verbosity to help let you know what is going on): apprise -vv -t 'my title' -b 'my notification body' \\ 'mailto://myemail:mypass@gmail.com' \\ 'pbul://o.gn5kj6nfhv736I7jC3cj3QLRiyhgl98b' # If you don't specify a --body (-b) then stdin is used allowing # you to use the tool as part of your every day administration: cat /proc/cpuinfo | apprise -vv -t 'cpu info' \\ 'mailto://myemail:mypass@gmail.com' # The title field is totally optional uptime | apprise -vv \\ 'discord:///4174216298/JHMHI8qBe7bk2ZwO5U711o3dV_js' Configuration Files No one wants to put their credentials out for everyone to see on the command line. No problem apprise also supports configuration files. It can handle both a specific YAML format or a very simple TEXT format. You can also pull these configuration files via an HTTP query too! You can read more about the expected structure of the configuration files here. # By default if no url or configuration is specified aprise will attempt to load # configuration files (if present): # ~/.apprise # ~/.apprise.yml # ~/.config/apprise # ~/.config/apprise.yml # Windows users can store their default configuration files here: # %APPDATA%/Apprise/apprise # %APPDATA%/Apprise/apprise.yml # %LOCALAPPDATA%/Apprise/apprise # %LOCALAPPDATA%/Apprise/apprise.yml # If you loaded one of those files, your command line gets really easy: apprise -vv -t 'my title' -b 'my notification body' # If you want to deviate from the default paths or specify more than one, # just specify them using the --config switch: apprise -vv -t 'my title' -b 'my notification body' \\ --config=/path/to/my/config.yml # Got lots of configuration locations? No problem, you can specify them all: # Apprise can even fetch the configuration from over a network! apprise -vv -t 'my title' -b 'my notification body' \\ --config=/path/to/my/config.yml \\ --config=https://localhost/my/apprise/config Attaching Files Apprise also supports file attachments too! Specify as many attachments to a notification as you want. # Send a funny image you found on the internet to a colleague: apprise -vv --title 'Agile Joke' \\ --body 'Did you see this one yet?' \\ --attach https://i.redd.it/my2t4d2fx0u31.jpg \\ 'mailto://myemail:mypass@gmail.com' # Easily send an update from a critical server to your dev team apprise -vv --title 'system crash' \\ --body 'I do not think Jim fixed the bug; see attached...' \\ --attach /var/log/myprogram.log \\ --attach /var/debug/core.2345 \\ --tag devteam Developers To send a notification from within your python application, just do the following: import apprise # Create an Apprise instance apobj = apprise.Apprise() # Add all of the notification services by their server url. # A sample email notification: apobj.add('mailto://myuserid:mypass@gmail.com') # A sample pushbullet notification apobj.add('pbul://o.gn5kj6nfhv736I7jC3cj3QLRiyhgl98b') # Then notify these services any time you desire. The below would # notify all of the services loaded into our Apprise object. apobj.notify( body='what a great notification service!', title='my notification title', ) Configuration Files Developers need access to configuration files too. The good news is their use just involves declaring another object (called AppriseConfig) that the Apprise object can ingest. You can also freely mix and match config and notification entries as often as you wish! You can read more about the expected structure of the configuration files here. import apprise # Create an Apprise instance apobj = apprise.Apprise() # Create an Config instance config = apprise.AppriseConfig() # Add a configuration source: config.add('/path/to/my/config.yml') # Add another... config.add('https://myserver:8080/path/to/config') # Make sure to add our config into our apprise object apobj.add(config) # You can mix and match; add an entry directly if you want too # In this entry we associate the 'admin' tag with our notification apobj.add('mailto://myuser:mypass@hotmail.com', tag='admin') # Then notify these services any time you desire. The below would # notify all of the services that have not been bound to any specific # tag. apobj.notify( body='what a great notification service!', title='my notification title', ) # Tagging allows you to specifically target only specific notification # services you've loaded: apobj.notify( body='send a notification to our admin group', title='Attention Admins', # notify any services tagged with the 'admin' tag tag='admin', ) # If you want to notify absolutely everything (reguardless of whether # it's been tagged or not), just use the reserved tag of 'all': apobj.notify( body='send a notification to our admin group', title='Attention Admins', # notify absolutely everything loaded, reguardless on wether # it has a tag associated with it or not: tag='all', ) Attaching Files Attachments are very easy to send using the Apprise API: import apprise # Create an Apprise instance apobj = apprise.Apprise() # Add at least one service you want to notify apobj.add('mailto://myuser:mypass@hotmail.com') # Then send your attachment. apobj.notify( title='A great photo of our family', body='The flash caused Jane to close her eyes! hah! :)', attach='/local/path/to/my/DSC_003.jpg', ) # Send a web based attachment too! In the below example, we connect to a home # security camera and send a live image to an email. By default remote web # content is cached but for a security camera, we might want to call notify # again later in our code so we want our last image retrieved to expire(in # this case after 3 seconds). apobj.notify( title='Latest security image', attach='http:/admin:password@hikvision-cam01/ISAPI/Streaming/channels/101/picture?cache=3' ) To send more than one attachment, just use a list, set, or tuple instead: import apprise # Create an Apprise instance apobj = apprise.Apprise() # Add at least one service you want to notify apobj.add('mailto://myuser:mypass@hotmail.com') # Now add all of the entries we're intrested in: attach = ( # ?name= allows us to rename the actual jpeg as found on the site # to be another name when sent to our receipient(s) 'https://i.redd.it/my2t4d2fx0u31.jpg?name=FlyingToMars.jpg', # Now add another: '/path/to/funny/joke.gif', ) # Send your multiple attachments with a single notify call: apobj.notify( title='Some good jokes.', body='Hey guys, check out these!', attach=attach, ) Want To Learn More? If you're interested in reading more about this and other methods on how to customize your own notifications, please check out the following links: Using the CLI Development API Troubleshooting Configuration File Help Apprise API/Web Interface Showcase Want to help make Apprise better? Contribute to the Apprise Code Base Sponsorship and Donations ",
        "Cool stuff! But what about AWS SNS (<a href=\"https://aws.amazon.com/sns/\" rel=\"nofollow\">https://aws.amazon.com/sns/</a>) or Firebase Cloud Messaging (<a href=\"https://firebase.google.com/docs/cloud-messaging/\" rel=\"nofollow\">https://firebase.google.com/docs/cloud-messaging/</a>)?",
        "This is pretty cool, I drop the following shell script on all my servers:<p><pre><code>    #!/bin/bash\n    \n    if [[ -z $1 && -z $2 ]]; then\n        echo \"No Message passed\"\n    else\n        if [[ -z $2 ]]; then\n            curl -s --form-string \"token=MYAPPTOKEN\" --form-string \"user=MYUSERTOKEN\" --form-string \"message=$1\" https://api.pushover.net/1/messages.json\n        else\n            curl -s --form-string \"token=MYAPPTOKEN\" --form-string \"user=MYUSERTOKEN\" --form-string \"title=$1\" --form-string \"message=$2\"  https://api.pushover.net/1/messages.json\n        fi\n    fi\n</code></pre>\nIt's SUPER basic and probably shitty but for me it's perfect. I can add \" && push 'Command is Done'\" to the end of any command and get a notification on my watch/phone/(and desktop? But I don't have pushover on my desktop installed). Great for things you threw into a screen/tmux session and want to know when they finished."
      ],
      "relevant": "false"
    },
    {
      "id": 20587541,
      "title": "Show HN: Apprise – A lightweight all-in-one notification solution (update)",
      "search": [
        "Show HN: Apprise – A lightweight all-in-one notification solution (update)",
        "https://github.com/caronc/apprise/#showhn-one-last-time",
        "apprise / verb To inform or tell (someone). To make one aware of something. Apprise allows you to send a notification to almost all of the most popular notification services available to us today such as: Telegram, Discord, Slack, Amazon SNS, Gotify, etc. One notification library to rule them all. A common and intuitive notification syntax. Supports the handling of images and attachments (to the notification services that will accept them). It's incredibly lightweight. Amazing response times because all messages sent asynchronously. Developers who wish to provide a notification service no longer need to research each and every one out there. They no longer need to try to adapt to the new ones that comeout thereafter. They just need to include this one library and then they can immediately gain access to almost all of the notifications services available to us today. System Administrators and DevOps who wish to send a notification now no longer need to find the right tool for the job. Everything is already wrapped and supported within the apprise command line tool (CLI) that ships with this product. Supported Notifications The section identifies all of the services supported by this library. Check out the wiki for more information on the supported modules here. Popular Notification Services The table below identifies the services this tool supports and some example service urls you need to use in order to take advantage of it. Click on any of the services listed below to get more details on how you can configure Apprise to access them. Notification Service Service ID Default Port Example Syntax Apprise API apprise:// or apprises:// (TCP) 80 or 443 apprise://hostname/Token Boxcar boxcar:// (TCP) 443 boxcar://hostnameboxcar://hostname/@tagboxcar://hostname/device_tokenboxcar://hostname/device_token1/device_token2/device_tokenNboxcar://hostname/@tag/@tag2/device_token Discord discord:// (TCP) 443 discord://webhook_id/webhook_tokendiscord://avatar@webhook_id/webhook_token Emby emby:// or embys:// (TCP) 8096 emby://user@hostname/emby://user:password@hostname Enigma2 enigma2:// or enigma2s:// (TCP) 80 or 443 enigma2://hostname Faast faast:// (TCP) 443 faast://authorizationtoken FCM fcm:// (TCP) 443 fcm://project@apikey/DEVICE_IDfcm://project@apikey/#TOPICfcm://project@apikey/DEVICE_ID1/#topic1/#topic2/DEVICE_ID2/ Flock flock:// (TCP) 443 flock://tokenflock://botname@tokenflock://app_token/u:useridflock://app_token/g:channel_idflock://app_token/u:userid/g:channel_id Gitter gitter:// (TCP) 443 gitter://token/roomgitter://token/room1/room2/roomN Google Chat gchat:// (TCP) 443 gchat://workspace/key/token Gotify gotify:// or gotifys:// (TCP) 80 or 443 gotify://hostname/tokengotifys://hostname/token?priority=high Growl growl:// (UDP) 23053 growl://hostnamegrowl://hostname:portnogrowl://password@hostnamegrowl://password@hostname:portNote: you can also use the get parameter version which can allow the growl request to behave using the older v1.x protocol. An example would look like: growl://hostname?version=1 Home Assistant hassio:// or hassios:// (TCP) 8123 or 443 hassio://hostname/accesstokenhassio://user@hostname/accesstokenhassio://user:password@hostname:port/accesstokenhassio://hostname/optional/path/accesstoken IFTTT ifttt:// (TCP) 443 ifttt://webhooksID/Eventifttt://webhooksID/Event1/Event2/EventNifttt://webhooksID/Event1/?+Key=Valueifttt://webhooksID/Event1/?-Key=value1 Join join:// (TCP) 443 join://apikey/devicejoin://apikey/device1/device2/deviceN/join://apikey/groupjoin://apikey/groupA/groupB/groupNjoin://apikey/DeviceA/groupA/groupN/DeviceN/ KODI kodi:// or kodis:// (TCP) 8080 or 443 kodi://hostnamekodi://user@hostnamekodi://user:password@hostname:port Kumulos kumulos:// (TCP) 443 kumulos://apikey/serverkey LaMetric Time lametric:// (TCP) 443 lametric://apikey@device_ipaddrlametric://apikey@hostname:portlametric://client_id@client_secret Mailgun mailgun:// (TCP) 443 mailgun://user@hostname/apikeymailgun://user@hostname/apikey/emailmailgun://user@hostname/apikey/email1/email2/emailNmailgun://user@hostname/apikey/?name=\"From%20User\" Matrix matrix:// or matrixs:// (TCP) 80 or 443 matrix://hostnamematrix://user@hostnamematrixs://user:pass@hostname:port/#room_aliasmatrixs://user:pass@hostname:port/!room_idmatrixs://user:pass@hostname:port/#room_alias/!room_id/#room2matrixs://token@hostname:port/?webhook=matrixmatrix://user:token@hostname/?webhook=slack&format=markdown Mattermost mmost:// or mmosts:// (TCP) 8065 mmost://hostname/authkeymmost://hostname:80/authkeymmost://user@hostname:80/authkeymmost://hostname/authkey?channel=channelmmosts://hostname/authkeymmosts://user@hostname/authkey Microsoft Teams msteams:// (TCP) 443 msteams://TokenA/TokenB/TokenC/ MQTT mqtt:// or mqtts:// (TCP) 1883 or 8883 mqtt://hostname/topicmqtt://user@hostname/topicmqtts://user:pass@hostname:9883/topic Nextcloud ncloud:// or nclouds:// (TCP) 80 or 443 ncloud://adminuser:pass@host/Usernclouds://adminuser:pass@host/User1/User2/UserN Notica notica:// (TCP) 443 notica://Token/ Notifico notifico:// (TCP) 443 notifico://ProjectID/MessageHook/ Office 365 o365:// (TCP) 443 o365://TenantID:AccountEmail/ClientID/ClientSecreto365://TenantID:AccountEmail/ClientID/ClientSecret/TargetEmailo365://TenantID:AccountEmail/ClientID/ClientSecret/TargetEmail1/TargetEmail2/TargetEmailN OneSignal onesignal:// (TCP) 443 onesignal://AppID@APIKey/PlayerIDonesignal://TemplateID:AppID@APIKey/UserIDonesignal://AppID@APIKey/#IncludeSegmentonesignal://AppID@APIKey/Email Opsgenie opsgenie:// (TCP) 443 opsgenie://APIKeyopsgenie://APIKey/UserIDopsgenie://APIKey/#Teamopsgenie://APIKey/*Scheduleopsgenie://APIKey/^Escalation ParsePlatform parsep:// or parseps:// (TCP) 80 or 443 parsep://AppID:MasterKey@Hostnameparseps://AppID:MasterKey@Hostname PopcornNotify popcorn:// (TCP) 443 popcorn://ApiKey/ToPhoneNopopcorn://ApiKey/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/popcorn://ApiKey/ToEmailpopcorn://ApiKey/ToEmail1/ToEmail2/ToEmailN/popcorn://ApiKey/ToPhoneNo1/ToEmail1/ToPhoneNoN/ToEmailN Prowl prowl:// (TCP) 443 prowl://apikeyprowl://apikey/providerkey PushBullet pbul:// (TCP) 443 pbul://accesstokenpbul://accesstoken/#channelpbul://accesstoken/A_DEVICE_IDpbul://accesstoken/email@address.compbul://accesstoken/#channel/#channel2/email@address.net/DEVICE Pushjet pjet:// or pjets:// (TCP) 80 or 443 pjet://hostname/secretpjet://hostname:port/secretpjets://secret@hostname/secretpjets://hostname:port/secret Push (Techulus) push:// (TCP) 443 push://apikey/ Pushed pushed:// (TCP) 443 pushed://appkey/appsecret/pushed://appkey/appsecret/#ChannelAliaspushed://appkey/appsecret/#ChannelAlias1/#ChannelAlias2/#ChannelAliasNpushed://appkey/appsecret/@UserPushedIDpushed://appkey/appsecret/@UserPushedID1/@UserPushedID2/@UserPushedIDN Pushover pover:// (TCP) 443 pover://user@tokenpover://user@token/DEVICEpover://user@token/DEVICE1/DEVICE2/DEVICENNote: you must specify both your user_id and token PushSafer psafer:// or psafers:// (TCP) 80 or 443 psafer://privatekeypsafers://privatekey/DEVICEpsafer://privatekey/DEVICE1/DEVICE2/DEVICEN Reddit reddit:// (TCP) 443 reddit://user:password@app_id/app_secret/subredditreddit://user:password@app_id/app_secret/sub1/sub2/subN Rocket.Chat rocket:// or rockets:// (TCP) 80 or 443 rocket://user:password@hostname/RoomID/Channelrockets://user:password@hostname:443/#Channel1/#Channel1/RoomIDrocket://user:password@hostname/#Channelrocket://webhook@hostnamerockets://webhook@hostname/@User/#Channel Ryver ryver:// (TCP) 443 ryver://Organization/Tokenryver://botname@Organization/Token SendGrid sendgrid:// (TCP) 443 sendgrid://APIToken:FromEmail/sendgrid://APIToken:FromEmail/ToEmailsendgrid://APIToken:FromEmail/ToEmail1/ToEmail2/ToEmailN/ SimplePush spush:// (TCP) 443 spush://apikeyspush://salt:password@apikeyspush://apikey?event=Apprise Slack slack:// (TCP) 443 slack://TokenA/TokenB/TokenC/slack://TokenA/TokenB/TokenC/Channelslack://botname@TokenA/TokenB/TokenC/Channelslack://user@TokenA/TokenB/TokenC/Channel1/Channel2/ChannelN SMTP2Go smtp2go:// (TCP) 443 smtp2go://user@hostname/apikeysmtp2go://user@hostname/apikey/emailsmtp2go://user@hostname/apikey/email1/email2/emailNsmtp2go://user@hostname/apikey/?name=\"From%20User\" Streamlabs strmlabs:// (TCP) 443 strmlabs://AccessToken/strmlabs://AccessToken/?name=name&identifier=identifier&amount=0&currency=USD SparkPost sparkpost:// (TCP) 443 sparkpost://user@hostname/apikeysparkpost://user@hostname/apikey/emailsparkpost://user@hostname/apikey/email1/email2/emailNsparkpost://user@hostname/apikey/?name=\"From%20User\" Spontit spontit:// (TCP) 443 spontit://UserID@APIKey/spontit://UserID@APIKey/Channelspontit://UserID@APIKey/Channel1/Channel2/ChannelN Syslog syslog:// (UDP) 514 (if hostname specified) syslog://syslog://Facilitysyslog://hostnamesyslog://hostname/Facility Telegram tgram:// (TCP) 443 tgram://bottoken/ChatIDtgram://bottoken/ChatID1/ChatID2/ChatIDN Twitter twitter:// (TCP) 443 twitter://CKey/CSecret/AKey/ASecrettwitter://user@CKey/CSecret/AKey/ASecrettwitter://CKey/CSecret/AKey/ASecret/User1/User2/User2twitter://CKey/CSecret/AKey/ASecret?mode=tweet Twist twist:// (TCP) 443 twist://pasword:logintwist://password:login/#channeltwist://password:login/#team:channeltwist://password:login/#team:channel1/channel2/#team3:channel XBMC xbmc:// or xbmcs:// (TCP) 8080 or 443 xbmc://hostnamexbmc://user@hostnamexbmc://user:password@hostname:port XMPP xmpp:// or xmpps:// (TCP) 5222 or 5223 xmpp://password@hostnamexmpp://user:password@hostnamexmpps://user:password@hostname:port?jid=user@hostname/resourcexmpps://password@hostname/target@myhost, target2@myhost/resource Webex Teams (Cisco) wxteams:// (TCP) 443 wxteams://Token Zulip Chat zulip:// (TCP) 443 zulip://botname@Organization/Tokenzulip://botname@Organization/Token/Streamzulip://botname@Organization/Token/Email SMS Notification Support Notification Service Service ID Default Port Example Syntax AWS SNS sns:// (TCP) 443 sns://AccessKeyID/AccessSecretKey/RegionName/+PhoneNosns://AccessKeyID/AccessSecretKey/RegionName/+PhoneNo1/+PhoneNo2/+PhoneNoNsns://AccessKeyID/AccessSecretKey/RegionName/Topicsns://AccessKeyID/AccessSecretKey/RegionName/Topic1/Topic2/TopicN ClickSend clicksend:// (TCP) 443 clicksend://user:pass@PhoneNoclicksend://user:pass@ToPhoneNo1/ToPhoneNo2/ToPhoneNoN D7 Networks d7sms:// (TCP) 443 d7sms://user:pass@PhoneNod7sms://user:pass@ToPhoneNo1/ToPhoneNo2/ToPhoneNoN DingTalk dingtalk:// (TCP) 443 dingtalk://token/dingtalk://token/ToPhoneNodingtalk://token/ToPhoneNo1/ToPhoneNo2/ToPhoneNo1/ Kavenegar kavenegar:// (TCP) 443 kavenegar://ApiKey/ToPhoneNokavenegar://FromPhoneNo@ApiKey/ToPhoneNokavenegar://ApiKey/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN MessageBird msgbird:// (TCP) 443 msgbird://ApiKey/FromPhoneNomsgbird://ApiKey/FromPhoneNo/ToPhoneNomsgbird://ApiKey/FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ MSG91 msg91:// (TCP) 443 msg91://AuthKey/ToPhoneNomsg91://SenderID@AuthKey/ToPhoneNomsg91://AuthKey/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Nexmo nexmo:// (TCP) 443 nexmo://ApiKey:ApiSecret@FromPhoneNonexmo://ApiKey:ApiSecret@FromPhoneNo/ToPhoneNonexmo://ApiKey:ApiSecret@FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Sinch sinch:// (TCP) 443 sinch://ServicePlanId:ApiToken@FromPhoneNosinch://ServicePlanId:ApiToken@FromPhoneNo/ToPhoneNosinch://ServicePlanId:ApiToken@FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/sinch://ServicePlanId:ApiToken@ShortCode/ToPhoneNosinch://ServicePlanId:ApiToken@ShortCode/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Twilio twilio:// (TCP) 443 twilio://AccountSid:AuthToken@FromPhoneNotwilio://AccountSid:AuthToken@FromPhoneNo/ToPhoneNotwilio://AccountSid:AuthToken@FromPhoneNo/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/twilio://AccountSid:AuthToken@FromPhoneNo/ToPhoneNo?apikey=Keytwilio://AccountSid:AuthToken@ShortCode/ToPhoneNotwilio://AccountSid:AuthToken@ShortCode/ToPhoneNo1/ToPhoneNo2/ToPhoneNoN/ Desktop Notification Support Notification Service Service ID Default Port Example Syntax Linux DBus Notifications dbus://qt://glib://kde:// n/a dbus://qt://glib://kde:// Linux Gnome Notifications gnome:// n/a gnome:// MacOS X Notifications macosx:// n/a macosx:// Windows Notifications windows:// n/a windows:// Email Support Service ID Default Port Example Syntax mailto:// (TCP) 25 mailto://userid:pass@domain.commailto://domain.com?user=userid&pass=passwordmailto://domain.com:2525?user=userid&pass=passwordmailto://user@gmail.com&pass=passwordmailto://mySendingUsername:mySendingPassword@example.com?to=receivingAddress@example.commailto://userid:password@example.com?smtp=mail.example.com&from=noreply@example.com&name=no%20reply mailtos:// (TCP) 587 mailtos://userid:pass@domain.commailtos://domain.com?user=userid&pass=passwordmailtos://domain.com:465?user=userid&pass=passwordmailtos://user@hotmail.com&pass=passwordmailtos://mySendingUsername:mySendingPassword@example.com?to=receivingAddress@example.commailtos://userid:password@example.com?smtp=mail.example.com&from=noreply@example.com&name=no%20reply Apprise have some email services built right into it (such as yahoo, fastmail, hotmail, gmail, etc) that greatly simplify the mailto:// service. See more details here. Custom Notifications Post Method Service ID Default Port Example Syntax JSON json:// or jsons:// (TCP) 80 or 443 json://hostnamejson://user@hostnamejson://user:password@hostname:portjson://hostname/a/path/to/post/to XML xml:// or xmls:// (TCP) 80 or 443 xml://hostnamexml://user@hostnamexml://user:password@hostname:portxml://hostname/a/path/to/post/to Installation The easiest way is to install this package is from pypi: Command Line A small command line tool is also provided with this package called apprise. If you know the server url's you wish to notify, you can simply provide them all on the command line and send your notifications that way: # Send a notification to as many servers as you want # as you can easily chain one after another (the -vv provides some # additional verbosity to help let you know what is going on): apprise -vv -t 'my title' -b 'my notification body' \\ 'mailto://myemail:mypass@gmail.com' \\ 'pbul://o.gn5kj6nfhv736I7jC3cj3QLRiyhgl98b' # If you don't specify a --body (-b) then stdin is used allowing # you to use the tool as part of your every day administration: cat /proc/cpuinfo | apprise -vv -t 'cpu info' \\ 'mailto://myemail:mypass@gmail.com' # The title field is totally optional uptime | apprise -vv \\ 'discord:///4174216298/JHMHI8qBe7bk2ZwO5U711o3dV_js' Configuration Files No one wants to put their credentials out for everyone to see on the command line. No problem apprise also supports configuration files. It can handle both a specific YAML format or a very simple TEXT format. You can also pull these configuration files via an HTTP query too! You can read more about the expected structure of the configuration files here. # By default if no url or configuration is specified aprise will attempt to load # configuration files (if present): # ~/.apprise # ~/.apprise.yml # ~/.config/apprise # ~/.config/apprise.yml # Windows users can store their default configuration files here: # %APPDATA%/Apprise/apprise # %APPDATA%/Apprise/apprise.yml # %LOCALAPPDATA%/Apprise/apprise # %LOCALAPPDATA%/Apprise/apprise.yml # If you loaded one of those files, your command line gets really easy: apprise -vv -t 'my title' -b 'my notification body' # If you want to deviate from the default paths or specify more than one, # just specify them using the --config switch: apprise -vv -t 'my title' -b 'my notification body' \\ --config=/path/to/my/config.yml # Got lots of configuration locations? No problem, you can specify them all: # Apprise can even fetch the configuration from over a network! apprise -vv -t 'my title' -b 'my notification body' \\ --config=/path/to/my/config.yml \\ --config=https://localhost/my/apprise/config Attaching Files Apprise also supports file attachments too! Specify as many attachments to a notification as you want. # Send a funny image you found on the internet to a colleague: apprise -vv --title 'Agile Joke' \\ --body 'Did you see this one yet?' \\ --attach https://i.redd.it/my2t4d2fx0u31.jpg \\ 'mailto://myemail:mypass@gmail.com' # Easily send an update from a critical server to your dev team apprise -vv --title 'system crash' \\ --body 'I do not think Jim fixed the bug; see attached...' \\ --attach /var/log/myprogram.log \\ --attach /var/debug/core.2345 \\ --tag devteam Developers To send a notification from within your python application, just do the following: import apprise # Create an Apprise instance apobj = apprise.Apprise() # Add all of the notification services by their server url. # A sample email notification: apobj.add('mailto://myuserid:mypass@gmail.com') # A sample pushbullet notification apobj.add('pbul://o.gn5kj6nfhv736I7jC3cj3QLRiyhgl98b') # Then notify these services any time you desire. The below would # notify all of the services loaded into our Apprise object. apobj.notify( body='what a great notification service!', title='my notification title', ) Configuration Files Developers need access to configuration files too. The good news is their use just involves declaring another object (called AppriseConfig) that the Apprise object can ingest. You can also freely mix and match config and notification entries as often as you wish! You can read more about the expected structure of the configuration files here. import apprise # Create an Apprise instance apobj = apprise.Apprise() # Create an Config instance config = apprise.AppriseConfig() # Add a configuration source: config.add('/path/to/my/config.yml') # Add another... config.add('https://myserver:8080/path/to/config') # Make sure to add our config into our apprise object apobj.add(config) # You can mix and match; add an entry directly if you want too # In this entry we associate the 'admin' tag with our notification apobj.add('mailto://myuser:mypass@hotmail.com', tag='admin') # Then notify these services any time you desire. The below would # notify all of the services that have not been bound to any specific # tag. apobj.notify( body='what a great notification service!', title='my notification title', ) # Tagging allows you to specifically target only specific notification # services you've loaded: apobj.notify( body='send a notification to our admin group', title='Attention Admins', # notify any services tagged with the 'admin' tag tag='admin', ) # If you want to notify absolutely everything (reguardless of whether # it's been tagged or not), just use the reserved tag of 'all': apobj.notify( body='send a notification to our admin group', title='Attention Admins', # notify absolutely everything loaded, reguardless on wether # it has a tag associated with it or not: tag='all', ) Attaching Files Attachments are very easy to send using the Apprise API: import apprise # Create an Apprise instance apobj = apprise.Apprise() # Add at least one service you want to notify apobj.add('mailto://myuser:mypass@hotmail.com') # Then send your attachment. apobj.notify( title='A great photo of our family', body='The flash caused Jane to close her eyes! hah! :)', attach='/local/path/to/my/DSC_003.jpg', ) # Send a web based attachment too! In the below example, we connect to a home # security camera and send a live image to an email. By default remote web # content is cached but for a security camera, we might want to call notify # again later in our code so we want our last image retrieved to expire(in # this case after 3 seconds). apobj.notify( title='Latest security image', attach='http:/admin:password@hikvision-cam01/ISAPI/Streaming/channels/101/picture?cache=3' ) To send more than one attachment, just use a list, set, or tuple instead: import apprise # Create an Apprise instance apobj = apprise.Apprise() # Add at least one service you want to notify apobj.add('mailto://myuser:mypass@hotmail.com') # Now add all of the entries we're intrested in: attach = ( # ?name= allows us to rename the actual jpeg as found on the site # to be another name when sent to our receipient(s) 'https://i.redd.it/my2t4d2fx0u31.jpg?name=FlyingToMars.jpg', # Now add another: '/path/to/funny/joke.gif', ) # Send your multiple attachments with a single notify call: apobj.notify( title='Some good jokes.', body='Hey guys, check out these!', attach=attach, ) Want To Learn More? If you're interested in reading more about this and other methods on how to customize your own notifications, please check out the following links: Using the CLI Development API Troubleshooting Configuration File Help Apprise API/Web Interface Showcase Want to help make Apprise better? Contribute to the Apprise Code Base Sponsorship and Donations ",
        "6 months ago [I posted about Apprise here](<a href=\"https://news.ycombinator.com/item?id=19023196\" rel=\"nofollow\">https://news.ycombinator.com/item?id=19023196</a>) and got a lot of amazing and encouraging feedback!  I since took just about everyone's comments and ideas at the time and implemented most of them.<p>Apprise now supports over 40+ different notification services, including configuration files that can be read from disk and the cloud! The library remains incredible light weight and easy to use.<p>I just wanted to share an almost completed solution and hope to hit you all up for more of your thoughts and advice!",
        "Worked about 10 years for a company named Appriss - <a href=\"https://appriss.com\" rel=\"nofollow\">https://appriss.com</a> - no real comment there other than to share the founding CEO used to say the board was presented with a bunch of names \"<i>and picked the worst one</i>\".<p>No real comment other than that, but nostalgia alone will make me play with this, so thanks for sharing."
      ],
      "relevant": "false"
    },
    {
      "id": 21693860,
      "title": "Amazon EKS on AWS Fargate Now Generally Available",
      "search": [
        "Amazon EKS on AWS Fargate Now Generally Available",
        "https://aws.amazon.com/blogs/aws/amazon-eks-on-aws-fargate-now-generally-available/",
        "Starting today, you can start using Amazon Elastic Kubernetes Service to run Kubernetes pods on AWS Fargate. EKS and Fargate make it straightforward to run Kubernetes-based applications on AWS by removing the need to provision and manage infrastructure for pods. With AWS Fargate, customers dont need to be experts in Kubernetes operations to run a cost-optimized and highly-available cluster. Fargate eliminates the need for customers to create or manage EC2 instances for their Amazon EKS clusters. Customers no longer have to worry about patching, scaling, or securing a cluster of EC2 instances to run Kubernetes applications in the cloud. Using Fargate, customers define and pay for resources at the pod-level. This makes it easy to right-size resource utilization for each application and allow customers to clearly see the cost of each pod. Im now going to use the rest of this blog to explore this new feature further and deploy a simple Kubernetes-based application using Amazon EKS on Fargate. Lets Build a Cluster The simplest way to get a cluster set up is to useeksctl, the official CLI tool for EKS. The command below creates a cluster calleddemo-newsblogwith no worker nodes. eksctl create cluster --name demo-newsblog --region eu-west-1 --fargate This single command did quite a lot under the hood. Not only did it create a cluster for me, amongst other things, it also created a Fargate profile. A Fargate profile, lets me specify which Kubernetes pods I want to run on Fargate, which subnets my pods run in, and provides the IAM execution role used by the Kubernetes agent to download container images to the pod and perform other actions on my behalf. Understanding Fargate profiles is key to understanding how this feature works. So I am going to delete the Fargate profile that was automatically created for me and recreate it manually. To create a Fargate profile, I head over to theAmazon Elastic Kubernetes Service (EKS) console and choose the clusterdemo-newsblog. On the details,UnderFargate profiles, I chooseAdd Fargate profile. I then need to configure my new Fargate profile. For the name, I enter demo-default. In the Pod execution role, only IAM roles with theeks-fargate-pods.amazonaws.comservice principal are shown. The eksctl tool creates an IAM role called AmazonEKSFargatePodExecutionRole, thedocumentationshows how this role can be created from scratch. In the Subnets section, by default, all subnets in my clusters VPC are selected. However, only private subnets are supported for Fargate pods, so I deselect the two public subnets. When I click next, I am taken to the Pod selectors screen. Here it asks me to enter a namespace. I add default, meaning that I want any pods that are created in the default Kubernetes namespace to run on Fargate. Its important to understand that I dont have to modify my Kubernetes app to get the pods running on Fargate, I just need a Fargate Profile if a pod in my Kubernetes app matches the namespace defined in my profile, that pod will run on Fargate. There is also a Match labels feature here, which I am not using. This allows you to specify the labels of the pods that you want to select, so you can get even more specific with which pods run on this profile. Finally, I click Next and then Create. It takes a minute for the profile to create and become active. In this demo, I also want everything to run on Fargate, including theCoreDNS pods that are part of Kubernetes. To get them running on Fargate, I will add a second Fargate profile for everything in thekube-systemnamespace. This time, to add a bit of variety to the demo, I will use the command line to create my profile. Technically, I do not need to create a second profile for this. I could have added an additional namespace to the first profile, but this way, I get to explore an alternative way of creating a profile. First, I create the file below and save it as demo-kube-system-profile.json. { \"fargateProfileName\": \"demo-kube-system\", \"clusterName\": \"demo-news-blog\", \"podExecutionRoleArn\": \"arn:aws:iam::xxx:role/AmazonEKSFargatePodExecutionRole\", \"subnets\": [ \"subnet-0968a124a4e4b0afe\", \"subnet-0723bbe802a360eb9\" ], \"selectors\": [ { \"namespace\": \"kube-system\" } ] } I then navigate to the folder that contains the file above and run thecreate-fargate-profile command in my terminal. aws eks create-fargate-profile --cli-input-json file://demo-kube-system-profile.json I am now ready to deploy a container to my cluster. To keep things simple, I deploy a single instance of nginx using the following kubectl command. kubectl create deployment demo-app --image=nginx I then check to see the state of my pods by running the get pods command. kubectl get pods NAME READY STATUS RESTARTS AGE demo-app-6dbfc49497-67dxk 0/1 Pending 0 13s If I run get nodes I have three nodes(two for coreDNS and one for nginx). These nodes represent the compute resources that have instantiated for me to run my pods. kubectl get nodes NAME STATUS ROLES AGE VERSION fargate-ip-192-168-218-51.eu-west-1.compute.internal Ready <none> 4m45s v1.14.8-eks fargate-ip-192-168-221-91.eu-west-1.compute.internal Ready <none> 2m20s v1.14.8-eks fargate-ip-192-168-243-74.eu-west-1.compute.internal Ready <none> 4m40s v1.14.8-eks After a short time, I rerun the get pods command, and my demo-appnow has a status ofRunning. Meaning my container has been successfully deployed onto Fargate. kubectl get pods NAME READY STATUS RESTARTS AGE demo-app-6dbfc49497-67dxk 1/1 Running 0 3m52s Pricing and Limitations With AWS Fargate, you pay only for the amount of vCPU and memory resources that your pod needs to run. This includes the resources the pod requests in addition to a small amount of memory needed to run Kubernetes components alongside the pod. Pods running on Fargate follow the existing pricing model. vCPU and memory resources are calculated from the time your pods container images are pulled until the pod terminates, rounded up to the nearest second. A minimum charge for 1 minute applies. Additionally, you pay the standard cost for each EKS cluster you run, $0.20 per hour. There are currently a few limitations that you should be aware of: There is a maximum of4 vCPU and 30Gb memory per pod. Currently there is no support for stateful workloads that require persistent volumes or file systems. You cannot run Daemonsets, Privileged pods, or pods that use HostNetwork or HostPort. The only load balancer you can use is an Application Load Balancer. Get Started Today If you want to explore Amazon EKS on AWS Fargate yourself, you can try it now by heading on over to the EKS console in the following regions: US East (N. Virginia), US East (Ohio), Europe (Ireland), and Asia Pacific (Tokyo). Martin ",
        "Great to see - but amazon marketing is approaching Apple levels. Lots of talk of “only service of its kind” where things like Azure Virtual Nodes (ACI + AKS) and Google Cloud Run have been GA for months",
        "I just use fargate directly (ECS I guess) - it works pretty nicely for small apps.<p>I was reading someone wanted EKS but the $180/month for management layer was way too much. Do people use EKS for tiny projects? It seems like a lot of complexity to carry around."
      ],
      "relevant": "false"
    },
    {
      "id": 19108787,
      "title": "Why are we templating YAML?",
      "search": [
        "Why are we templating YAML?",
        "https://leebriggs.co.uk/blog/2019/02/07/why-are-we-templating-yaml.html",
        "Published Feb 7, 2019 by Lee Briggs #kubernetes #configuration mgmt #jsonnet #helm #kr8 I was at cfgmgmtcamp 2019 in Ghent, and did a talk which I think was well received about the need for some Kubernetes configuration management as well as the solution we built for it at $work, kr8. I made a statement during the talk which ignited some fairly fierce discussion both online, and at the conference: \"If you're starting to template yaml, ask yourself the question: why am I not *generating* json?\" - @briggsl spitting straight fire at #cfgmgmtcamp eric sorenson (@ahpook) February 5, 2019 To put this into my own words: At some point, we decided it was okay for us to template yaml. When did this happen? How is this acceptable? After some conversation, I figured it was probably best to back up my claims in some way. This blog post is going to try to do that. The configuration problem Once the applications and infrastructure youre going to manage grows past a certain size, you inevitably end up in some form of configuration complexity hell. If youre only deploying 1 or maybe 2 things, you can write a yaml configuration file and be done with it. However once you grow beyond that, you need to figure out how to manage this complexity. Its incredibly likely that the reason you have multiple configuration files is because the $thing that uses that config is slightly different from its companions. Examples of this include: Applications deployed in different environments, like dev, stg and prod Applications deployed in different regions, like Europe or North American Obviously, not all the configuration is different here, but its likely the configuration differs enough that you want to be able to differentiate between the two. This configuration complexity has been well known for Operators (System Administrators, DevOps engineers, whatever you want to call them) for some years now. An entire discpline grew up around this in Configuration Management, and each tool solved this problem in their own way, but ultimately, they used YAML to get the job done. My favourite method has always been hiera which comes bundled with Puppet. Having the ability to hierarchically look up the variables of specific config needs is incredibly powerful and flexible, and has generally meant you dont actually need to do any templating of yaml at all, except perhaps for embedding Puppet facts into the yaml. Did we go backwards? Then, as our industries needs moved above the operating system and into cloud computing, we had a whole new data plane to configure. The tooling to configure this changed, and tools like CloudFormation and Helm appeared. These tools are excellent configuration tools, but I firmly believe we (as an industry) got something really, really wrong when we designed them. To examine that, lets take a look at example of a helm chart taking a custom parameter Helm Charts Helm charts can take external parameters defined by an values.yaml file which you specify when rendering the chart. A simple example might look like this: Lets say my external parameter is simple - its a string. Itd look a bit like this: image: \"{{ .Values.image }}\" Thats not so bad right? You just specify a value for image in your values.yaml and youre on your way. The real problem starts to get highlighted when you want to do more complicated and complex things. In this particular example, youre doing okay because you know you have to specify an image for a Kubernetes deployment. However, what if youre working with something like an optional field? Well, then it gets a little more unwieldy: {{- with .resourceGroup }} resourceGroup: {{ . }} {{- end }} Optional values just make things ugly in templating languages, and you cant just leave the value blank, so you have to resort to ugly loops and conditionals that are probably going to bite you later. Lets say you need to go a step further, and you need to push an array or map into the config. With helm, youd do something like this. {{- with .Values.podAnnotations }} annotations: {{ toYaml . | indent 8 }} {{- end }} Firstly, lets ignore the madness of having a templating function toYaml to convert yaml to yaml and focus more on the whitespace issue here. YAML has strict requirements and whitespace implementation rules. The following, for example, is not valid or complete yaml: something: nothing hello: goodbye Generally, if youre handwriting something, this isnt necessarily a problem because you just hit backspace twice and its fixed. However, if youre generating YAML using a templating system, you cant do that - and if youre operating above 5 or 10 configuration files, you probably want to be generating your config rather than writing it. So, in the above example, you want to embed the values of .Values.podAnnotations under the annotations field, which is indented already. So youre having to not only indent your values, but indent them correctly. What makes this even more confusing is that the go parser doesnt actually know anything about YAML at all, so if you try to keep the syntax clean and indent the templates like this: {{- with .Values.podAnnotations }} annotations: {{ toYaml . | indent 6 }} {{- end }} You actually cant do that, because the templating system gets confused. This is a singular example of the complexity and difficulty you end up facing when generating config data in YAML, but when you really start to do more complex work, it really starts to become obvious that this isnt the way to go. Needless to say, this isnt what I want to spend my time doing. If fiddling around with whitespace requirements in a templating system doing something its not really designed for is what suits you, then Im not going to stop you. I also dont want to spend my time writing configuration in JSON without comments and accidentally missing commas all over the shop. We (as an industry) decided a long time ago that shit wasnt going to work and thats why YAML exists. So what should we do instead? Thats where jsonnet comes in. JSON, Jsonnet & YAML Before we actually talk about Jsonnet, its worth reminding people of a very important (but oft forgotten point). YAML is a superset of JSON and converting between the two is trivial. Many applications and programming languages will parse JSON and YAML natively, and many can convert between the two very simple. For example, in Python: python -c 'import json, sys, yaml ; y=yaml.safe_load(sys.stdin.read()) ; print(json.dumps(y))' So with that in mind, lets talk about Jsonnet. Welcome to the church of Jsonnet Jsonnet is a relatively new, little known (outside the Kubernetes community?) language that calls itself a data templating language. Its definitely a good exercise to read and consume the Jsonnet design rationale page to get an idea why it exists, but if I was going to define in a nutshell what its purpose is - its to generate JSON config. So, how does it help, exactly? Well, lets take our earlier example - we want to generate some JSON config specifying a parameter (ie, the image string). We can do that very very easily with Jsonnet using external variables. Firstly, lets define some Jsonnet: { image: std.extVar('image'), } Then, we can generate it using the Jsonnet command line tool, passing in the external variable as we need to: jsonnet image.jsonnet -V image=\"my-image\" { \"image\": \"my-image\" } Easy! Optional fields Before, I noted that if you wanted to define an optional field, with YAML templating you had to define if statements for everything. With Jsonnet, youre just defining code! // define a variable - yes, jsonnet also has comments local rg = null; { image: std.extVar('image'), // if the variable is null, this will be blank [if rg != null then 'resourceGroup']: rg, } The output here, because our variable is null, means that we never actually populate resourceGroup. If you specify a value, it will appear: jsonnet image.jsonnet -V image=\"my-image\" { \"image\": \"my-image\" } Maps and parameters Okay, now lets look at our previous annotation example. We want to define some pod annotations, which takes a YAML map as its input. You want this map to be configurable by specifying external data, and obviously doing that on the command line sucks (youd be very unlikely to specify this with Helm on the command line, for example) so generally youd use Jsonnet imports to this. Im going to specify this config as a variable and then load that variable into the annotation: local annotations = { 'nginx.ingress.kubernetes.io/app-root': '/', 'nginx.ingress.kubernetes.io/enable-cors': true, }; { metadata: { // annotations are nested under the metadata of a pod annotations: annotations, }, } This might just be my bias towards Jsonnet talking, but this is so dramatically easier than faffing about with indentation that I cant even begin to describe it. Additional goodies The final thing I wanted to quickly explore, which is something that I feel cant really be done with Helm and other yaml templating tools, is the concept of manipulating existing objects in config. Lets take our example above with the annotations, and look at the result file: { \"metadata\": { \"annotations\": { \"nginx.ingress.kubernetes.io/app-root\": \"/\", \"nginx.ingress.kubernetes.io/enable-cors\": true } } } Now, lets say for example I wanted to append a set of annotations to this annotations map. In any templating system, Id probably have to rewrite the whole map. Jsonnet makes this trivial. I can simply use the + operator to add something to this. Heres a (poor) example: local annotations = { 'nginx.ingress.kubernetes.io/app-root': '/', 'nginx.ingress.kubernetes.io/enable-cors': true, }; { metadata: { annotations: annotations, }, } + { // this adds another JSON object metadata+: { // I'm using the + operator, so we'll append to the existing metadata annotations+: { // same as above something: 'nothing', }, }, } The end result is this: { \"metadata\": { \"annotations\": { \"nginx.ingress.kubernetes.io/app-root\": \"/\", \"nginx.ingress.kubernetes.io/enable-cors\": true, \"something\": \"nothing\" } } } Obviously, in this case, its more code to this, but as your example get more complex, it becomes extremely useful to be able to manipulate objects this way. Kr8 We use all of these methods in kr8 to make creating and manipulating configuration for multiple Kubernetes clusters easy and simple. I highly recommend you check it out if any of the concepts youve found here have found you nodding your head. ",
        "I know I'm in a minority, but I really dislike YAML... I recently did a lot of Ansible and boy, at the beginning, I was just struggling a lot. Syntactic whitespace kills me.<p>I don't like it in Python either, but for some reason, when I write Python, it's a lot easier. Maybe YAML is just a bit more complex (and Python has better IDE support..?)",
        "My belief is that we've been slowly building up to using general purpose languages, one small step at a time, throughout the infrastructure as code, DevOps, and SRE journeys this past 10 years. INI files, XML, JSON, and YAML aren't sufficiently expressive -- lacking for loops, conditionals, variable references, and any sort of abstraction -- so, of course, we add templates to it. But as the author (IMHO rightfully) points out, we just end up with a funky, poor approximation of a language.<p>I think this approach is a byproduct of thinking about infrastructure and configuration -- and the cloud generally -- as an \"afterthought,\" not a core part of an application's infrastructure. Containers, Kubernetes, serverless, and more hosted services all change this, and Chef, Puppet, and others laid the groundwork to think differently about what the future looks like. More developers today than ever before need to think about how to build and configure cloud software.<p>We started the Pulumi project to solve this very problem, so I'm admittedly biased, and I hope you forgive the plug -- I only mention it here because I think it contributes to the discussion. Our approach is to simply use general purpose languages like TypeScript, Python, and Go, while still having infrastructure as code. An important thing to realize is that infrastructure as code is based on the idea of a <i>goal state</i>. Using a full blown language to generate that goal state generally doesn't threaten the repeatability, determinism, or robustness of the solution, provided you've got an engine handling state management, diffing, resource CRUD, and so on. We've been able to apply this universally across AWS, Azure, GCP, <i>and</i> Kubernetes, often mixing their configuration in the same program.<p>Again, I'm biased and want to admit that, however if you're sick of YAML, it's definitely worth checking out. We'd love your feedback:<p>- Project website: <a href=\"https://pulumi.io/\" rel=\"nofollow\">https://pulumi.io/</a><p>- All open source on GitHub: <a href=\"https://github.com/pulumi/pulumi\" rel=\"nofollow\">https://github.com/pulumi/pulumi</a><p>- Example of abstractions: <a href=\"https://blog.pulumi.com/the-fastest-path-to-deploying-kubernetes-on-aws-with-eks-and-pulumi\" rel=\"nofollow\">https://blog.pulumi.com/the-fastest-path-to-deploying-kubern...</a><p>- Example of serverless as event handlers: <a href=\"https://blog.pulumi.com/lambdas-as-lambdas-the-magic-of-simple-serverless-functions\" rel=\"nofollow\">https://blog.pulumi.com/lambdas-as-lambdas-the-magic-of-simp...</a><p>Pulumi may not be <i>the</i> solution for everyone, but I'm fairly optimistic that this is where we're all heading.<p>Joe"
      ],
      "relevant": "true"
    },
    {
      "id": 19898568,
      "title": "Show HN: Cloudmarker – Cloud monitoring tool and framework",
      "search": [
        "Show HN: Cloudmarker – Cloud monitoring tool and framework",
        "https://github.com/cloudmarker/cloudmarker",
        "Cloudmarker Cloudmarker is a cloud monitoring tool and framework. Contents Table of Contents: Contents What is Cloudmarker? Why Cloudmarker? Features Wishlist Install Develop Resources Support License What is Cloudmarker? Cloudmarker is a cloud monitoring tool and framework. It can be used as a ready-made tool that audits your Azure or GCP cloud environments as well as a framework that allows you to develop your own cloud monitoring software to audit your clouds. As a monitoring tool, it performs the following actions: Retrieves data about each configured cloud using the cloud APIs. Saves or indexes the retrieved data into each configured storage system or indexing engine. Analyzes the data for potential issues and generates events that represent the detected issues. Saves the events to configured storage or indexing engines as well as sends the events as alerts to alerting destinations. Each of the above four aspects of the tool can be configured via a configuration file. For example, the tool can be configured to pull data from Azure and index its data in Elasticsearch while it also pulls data from GCP and indexes the GCP data in MongoDB. Similarly, it is possible to configure the tool to check for unencrypted disks in Azure, generate events for it, and send them as alerts by email while it checks for insecure firewall rules in both Azure and GCP, generate events for them, and save those events in MongoDB. This degree of flexibility to configure audits for different clouds in different ways comes from the fact that Cloudmarker is designed as a combination of lightweight framework and a bunch of plugins that do the heavylifting for retrieving cloud data, storing the data, analyzing the data, generating events, and sending alerts. These four types of plugins are formally known as cloud plugins, store plugins, event plugins, and alert plugins, respectively. As a result of this plugin-based architecture, Cloudmarker can also be used as a framework to develop your own plugins that extend its capabilities by adding support for new types of clouds or data sources, storage or indexing engines, event generation, and alerting destinations. Why Cloudmarker? One might wonder why we need a new project like this when similar projects exist. When we began working on this project in 2017, we were aware of similar tools that supported AWS and GCP but none that supported Azure at that time. As a result, we wrote our own tool to support Azure. We later added support for GCP as well. What began as a tiny proof of concept gradually turned into a fair amount of code, so we thought, we might as well share this project online, so that others could use it and see if they find value in it. So far, some of the highlights of this project are: It is simple. It is easy to understand how to use the four types of plugins (clouds, stores, events, and alerts) to perform an audit. It is excellent at creating an inventory of the cloud environment. The data inventory it creates is easy to query. It is good at detecting insecure firewall rules and unencrypted disks. New detection mechanisms are coming up. We also realize that we can add a lot more functionality to this project to make it more powerful too. See the Wishlist section below to see new features we would like to see in this project. Our project is hosted on GitHub at https://github.com/cloudmarker/cloudmarker. Contributions and pull requests are welcome. We hope that you would give this project a shot, see if it addresses your needs, and provide us some feedback by posting a comment in our feedback thread or by creating a new issue. Features Since Cloudmarker is not just a tool but also a framework, a lot of its functionality can be extended by writing plugins. However, Cloudmarker also comes bundled with a default set of plugins that can be used as is without writing a single line of code. Here is a brief overview of the features that come bundled with Cloudmarker: Perform scheduled or ad hoc audits of cloud environment. Retrieve data from Azure and GCP. Store or index retrieved data in Elasticsearch, MongoDB, Splunk, and the file system. Look for insecure firewall rules and generate firewall rule events. Look for unencrypted disks (Azure only) and generate events. Send alerts for events via email and Slack as well as save alerts in one of the supported storage or indexing engines (see the third point above). Normalize firewall rules from Azure and GCP which are in different formats to a common object model (\"com\") so that a single query or event rule can search for or detect issues in firewall rules from both clouds. Wishlist Add more event plugins to detect different types of insecure configuration. Normalize other types of data into a common object model (\"com\") just like we do right now for firewall rules. Install Perform the following steps to set up Cloudmarker. Create a virtual Python environment and install Cloudmarker in it: python3 -m venv venv . venv/bin/activate pip3 install cloudmarker Run sanity test: The above command runs a mock audit with mock plugins that generate some mock data. The mock data generated can be found at /tmp/cloudmarker/. Logs from the tool are written to the standard output as well as to /tmp/cloudmarker.log. The -n or --now option tells Cloudmarker to run right now instead of waiting for a scheduled run. To learn how to configure and use Cloudmarker with Azure or GCP clouds, see Cloudmarker Tutorial. Develop This section describes how to set up a development environment for Cloudmarker. This section is useful for those who would like to contribute to Cloudmarker or run Cloudmarker directly from its source. We use primarily three tools to perform development on this project: Python 3, Git, and Make. Your system may already have these tools. But if not, here are some brief instructions on how they can be installed. On macOS, if you have Homebrew installed, then these tools can be be installed easily with the following command: On a Debian GNU/Linux system or in another Debian-based Linux distribution, they can be installed with the following commands: apt-get update apt-get install python3 python3-venv git make On a CentOS Linux distribution, they can be installed with these commands: yum install centos-release-scl yum install git make rh-python36 scl enable rh-python36 bash Note: The scl enable command starts a new shell for you to use Python 3. On any other system, we hope you can figure out how to install these tools yourself. Clone the project repository and enter its top-level directory: git clone https://github.com/cloudmarker/cloudmarker.git cd cloudmarker Create a virtual Python environment for development purpose: This creates a virtual Python environment at ~/.venv/cloudmarker. Additionally, it also creates a convenience script named venv in the current directory to easily activate the virtual Python environment which we will soon see in the next point. To undo this step at anytime in future, i.e., delete the virtual Python environment directory, either enter rm -rf venv ~/.venv/cloudmarker or enter make rmvenv. Activate the virtual Python environment: In the top-level directory of the project, enter this command: python3 -m cloudmarker -n This generates mock data at /tmp/cloudmarker. This step serves as a sanity check that ensures that the development environment is correctly set up and that the Cloudmarker audit framework is running properly. Now that the project is set up correctly, you can create a cloudmarker.yaml to configure Cloudmarker to scan/audit your cloud or you can perform more development on the Cloudmarker source code. See Cloudmarker Tutorial for more details. If you have set up a development environment to perform more development on Cloudmarker, please consider sending a pull request to us if you think your development work would be useful to the community. Before sending a pull request, please run the unit tests, code coverage, linters, and document generator to ensure that no existing test has been broken and the pull request adheres to our coding conventions: make test make coverage make lint make docs To run these four targets in one shot, enter this \"shortcut\" target: Open htmlcov/index.html with a web browser to view the code coverage report. Open docs/_build/html/index.html with a web browser to view the generated documentation. Resources Here is a list of useful links about this project: Documentation on Read The Docs Latest release on PyPI Source code on GitHub Issue tracker on GitHub Changelog on GitHub Cloudmarker channel on Slack Invitation to Cloudmarker channel on Slack Support To report bugs, suggest improvements, or ask questions, please create a new issue at http://github.com/cloudmarker/cloudmarker/issues. License This is free software. You are permitted to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of it, under the terms of the MIT License. See LICENSE.rst for the complete license. This software is provided WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See LICENSE.rst for the complete disclaimer. ",
        "What is the generally the best language while working on more than one cloud? If I want to deploy VMs into the major three clouds (AWS/GCP/Azure), is Python a good language for automation or am I better off with Java?",
        "I am trying this tool. First impression: The plugin framework works like charm. Creating a small plugin to get GitHub org repo info. Working with only two files. Plugin is one file and configuration yaml is other file.<p>I am not Python developer. I develop in Powershell. So don't judge my code. :)<p>I put this in ghcloud.py.<p><pre><code>  import urllib.request\n  import json\n\n  class GhCloud:\n      def __init__(self, org):\n          self.org = org\n\n      def read(self):\n          url = 'https://api.github.com/orgs/%s/repos' % self.org\n          data = json.loads(urllib.request.urlopen(url).read().decode('utf-8'))\n          for d in data: yield d\n\n      def done(self):\n          pass\n</code></pre>\nI put this in cloudmarker.yaml.<p><pre><code>  plugins:\n    ghplug:\n      plugin: ghcloud.GhCloud\n      params:\n        org: python\n\n  audits:\n    ghmon:\n      clouds:\n      - ghplug\n      stores:\n      - esstore\n      - filestore\n\n  run:\n  - ghmon\n</code></pre>\nI run tool.<p><pre><code>  PYTHONPATH=. cloudmarker --now\n</code></pre>\nCloudmarker runs my ghcloud.py and puts data into localhost:9200 and /tmp automatically. I can do it without cloning Cloudmarker code. I only hack my code and Cloudmarker runs it."
      ],
      "relevant": "false"
    },
    {
      "id": 21509373,
      "title": "Usql – A Universal CLI for Databases",
      "search": [
        "Usql – A Universal CLI for Databases",
        "https://github.com/xo/usql",
        "Installing | Building | Using | Database Support | Features and Compatibility | Releases | Contributing usql is a universal command-line interface for PostgreSQL, MySQL, Oracle Database, SQLite3, Microsoft SQL Server, and many other databases including NoSQL and non-relational databases! usql provides a simple way to work with SQL and NoSQL databases via a command-line inspired by PostgreSQL's psql. usql supports most of the core psql features, such as variables, backticks, and commands and has additional features that psql does not, such as syntax highlighting, context-based completion, and multiple database support. Database administrators and developers that would prefer to work with a tool like psql with non-PostgreSQL databases, will find usql intuitive, easy-to-use, and a great replacement for the command-line clients/tools for other databases. Installing usql can be installed via Release, via Homebrew, via Scoop or via Go: Installing via Release Download a release for your platform Extract the usql or usql.exe file from the .tar.bz2 or .zip file Move the extracted executable to somewhere on your $PATH (Linux/macOS) or %PATH% (Windows) macOS Notes The recommended installation method on macOS is via brew (see below). If the following or similar error is encountered when attempting to run usql: $ usql dyld: Library not loaded: /usr/local/opt/icu4c/lib/libicuuc.68.dylib Referenced from: /Users/user/.local/bin/usql Reason: image not found Abort trap: 6 Then the ICU lib needs to be installed. This can be accomplished using brew: Installing via Homebrew (macOS and Linux) usql is available in the xo/xo tap, and can be installed in the usual way with the brew command: # install usql with \"most\" drivers $ brew install xo/xo/usql Additional support for ODBC databases can be installed by passing --with-odbc option during install: # install usql with odbc support $ brew install --with-odbc usql Installing via Scoop (Windows) usql can be installed using Scoop: # install scoop if not already installed iex (new-object net.webclient).downloadstring('https://get.scoop.sh') scoop install usql Installing via Go usql can be installed in the usual Go fashion: # install usql from master branch with basic database support # includes PostgreSQL, Oracle Database, MySQL, MS SQL, and SQLite3 drivers $ go install github.com/xo/usql@master Building When building usql with Go, only drivers for PostgreSQL, MySQL, SQLite3 and Microsoft SQL Server will be enabled by default. Other databases can be enabled by specifying the build tag for their database driver. Additionally, the most and all build tags include most, and all SQL drivers, respectively: # install all drivers $ go install -tags all github.com/xo/usql@master # install with most drivers (excludes unsupported drivers) $ go install -tags most github.com/xo/usql@master # install with base drivers and additional support for Oracle Database and ODBC $ go install -tags 'godror odbc' github.com/xo/usql@master For every build tag <driver>, there is also a no_<driver> build tag disabling the driver: # install all drivers excluding avatica and couchbase $ go install -tags 'all no_avatica no_couchbase' github.com/xo/usql@master Release Builds Release builds are built with the most build tag. Additional SQLite3 build tags are also specified for releases. Embedding An effort has been made to keep usql's packages modular, and reusable by other developers wishing to leverage the usql code base. As such, it is possible to embed or create a SQL command-line interface (e.g, for use by some other project as an \"official\" client) using the core usql source tree. Please refer to main.go to see how usql puts together its packages. usql's code is also well-documented -- please refer to the Go reference for an overview of the various packages and APIs. Database Support usql works with all Go standard library compatible SQL drivers supported by github.com/xo/dburl. The list of drivers that usql was built with can be displayed using the \\drivers command: $ cd $GOPATH/src/github.com/xo/usql $ export GO111MODULE=on # build excluding the base drivers, and including cassandra and moderncsqlite $ go build -tags 'no_postgres no_oracle no_sqlserver no_sqlite3 cassandra moderncsqlite' # show built driver support $ ./usql -c '\\drivers' Available Drivers: cql [ca, scy, scylla, datastax, cassandra] memsql (mysql) [me] moderncsqlite [mq, sq, file, sqlite, sqlite3, modernsqlite] mysql [my, maria, aurora, mariadb, percona] tidb (mysql) [ti] vitess (mysql) [vt] The above shows that usql was built with only the mysql, cassandra (ie, cql), and moderncsqlite drivers. The output above reflects information about the drivers available to usql, specifically the internal driver name, its primary URL scheme, the driver's available scheme aliases (shown in [...]), and the real/underlying driver (shown in (...)) for wire compatible drivers. Supported Database Schemes and Aliases The following are the Go SQL drivers that usql supports, the associated database, scheme / build tag, and scheme aliases: Database Scheme / Tag Scheme Aliases Driver Package / Notes Microsoft SQL Server sqlserver ms, mssql github.com/denisenkom/go-mssqldb MySQL mysql my, maria, aurora, mariadb, percona github.com/go-sql-driver/mysql Oracle Database oracle or, ora, oci, oci8, odpi, odpi-c github.com/sijms/go-ora/v2 PostgreSQL postgres pg, pgsql, postgresql github.com/lib/pq SQLite3 sqlite3 sq, file, sqlite github.com/mattn/go-sqlite3 Alibaba MaxCompute maxcompute mc sqlflow.org/gomaxcompute Apache Avatica avatica av, phoenix github.com/apache/calcite-avatica-go/v5 Apache H2 h2 github.com/jmrobles/h2go Apache Ignite ignite ig, gridgain github.com/amsokol/ignite-go-client/sql AWS Athena athena s3, aws github.com/uber/athenadriver/go Cassandra cassandra ca, scy, scylla, datastax, cql github.com/MichaelS11/go-cql-driver ClickHouse clickhouse ch github.com/ClickHouse/clickhouse-go Couchbase couchbase n1, n1ql github.com/couchbase/go_n1ql CSVQ csvq cs, csv, tsv, json github.com/mithrandie/csvq-driver Cznic QL ql cznic, cznicql modernc.org/ql Exasol exasol ex, exa github.com/exasol/exasol-driver-go Firebird firebird fb, firebirdsql github.com/nakagami/firebirdsql Genji genji gj github.com/genjidb/genji/driver Google BigQuery bigquery bq gorm.io/driver/bigquery/driver Google Spanner spanner sp github.com/cloudspannerecosystem/go-sql-spanner Microsoft ADODB adodb ad, ado github.com/mattn/go-adodb ModernC SQLite3 moderncsqlite mq, modernsqlite modernc.org/sqlite MySQL MyMySQL mymysql zm, mymy github.com/ziutek/mymysql/godrv Netezza netezza nz, nzgo github.com/IBM/nzgo PostgreSQL PGX pgx px github.com/jackc/pgx/v4/stdlib Presto presto pr, prs, prestos, prestodb, prestodbs github.com/prestodb/presto-go-client/presto SAP ASE sapase ax, ase, tds github.com/thda/tds SAP HANA saphana sa, sap, hana, hdb github.com/SAP/go-hdb/driver Trino trino tr, trs, trinos github.com/trinodb/trino-go-client/trino Vertica vertica ve github.com/vertica/vertica-sql-go VoltDB voltdb vo, vdb, volt github.com/VoltDB/voltdb-client-go/voltdbclient Apache Hive hive hi sqlflow.org/gohive Apache Impala impala im github.com/bippio/go-impala Azure CosmosDB cosmos cm github.com/btnguyen2k/gocosmos GO DRiver for ORacle godror gr github.com/godror/godror ODBC odbc od github.com/alexbrainman/odbc Snowflake snowflake sf github.com/snowflakedb/gosnowflake Amazon Redshift postgres rs, redshift github.com/lib/pq CockroachDB postgres cr, cdb, crdb, cockroach, cockroachdb github.com/lib/pq OLE ODBC adodb oo, ole, oleodbc github.com/mattn/go-adodb SingleStore MemSQL mysql me, memsql github.com/go-sql-driver/mysql TiDB mysql ti, tidb github.com/go-sql-driver/mysql Vitess Database mysql vt, vitess github.com/go-sql-driver/mysql NO DRIVERS no_base no base drivers (useful for development) MOST DRIVERS most all stable drivers ALL DRIVERS all all drivers NO <TAG> no_<tag> exclude driver with <tag> Requires CGO Wire compatible (see respective driver) Any of the protocol schemes/aliases shown above can be used in conjunction when connecting to a database via the command-line or with the \\connect command: # connect to a vitess database: $ usql vt://user:pass@host:3306/mydatabase $ usql (not connected)=> \\c vitess://user:pass@host:3306/mydatabase See the section below on connecting to databases for further details building DSNs/URLs for use with usql. Using After installing, usql can be used similarly to the following: # connect to a postgres database $ usql postgres://booktest@localhost/booktest # connect to an oracle database $ usql oracle://user:pass@host/oracle.sid # connect to a postgres database and run the commands contained in script.sql $ usql pg://localhost/ -f script.sql Command-line Options Supported command-line options: $ usql --help usql, the universal command-line interface for SQL databases Usage: usql [OPTIONS]... [DSN] Arguments: DSN database url Options: -c, --command=COMMAND ... run only single command (SQL or internal) and exit -f, --file=FILE ... execute commands from file and exit -w, --no-password never prompt for password -X, --no-rc do not read start up file -o, --out=OUT output file -W, --password force password prompt (should happen automatically) -1, --single-transaction execute as a single transaction (if non-interactive) -v, --set=, --variable=NAME=VALUE ... set variable NAME to VALUE -P, --pset=VAR[=ARG] ... set printing option VAR to ARG (see \\pset command) -F, --field-separator=FIELD-SEPARATOR ... field separator for unaligned output (default, \"|\") -R, --record-separator=RECORD-SEPARATOR ... record separator for unaligned output (default, \\n) -T, --table-attr=TABLE-ATTR ... set HTML table tag attributes (e.g., width, border) -A, --no-align unaligned table output mode -H, --html HTML table output mode -t, --tuples-only print rows only -x, --expanded turn on expanded table output -z, --field-separator-zero set field separator for unaligned output to zero byte -0, --record-separator-zero set record separator for unaligned output to zero byte -J, --json JSON output mode -C, --csv CSV output mode -G, --vertical vertical output mode -V, --version display version and exit Connecting to Databases usql opens a database connection by parsing a URL and passing the resulting connection string to a database driver. Database connection strings (aka \"data source name\" or DSNs) have the same parsing rules as URLs, and can be passed to usql via command-line, or to the \\connect or \\c commands. Connection strings look like the following: driver+transport://user:pass@host/dbname?opt1=a&opt2=b driver:/path/to/file /path/to/file Where the above are: Component Description driver driver scheme name or scheme alias transport tcp, udp, unix or driver name (for ODBC and ADODB) user username pass password host hostname dbname database name, instance, or service name/ID ?opt1=a&... additional database driver options (see respective SQL driver for available options) /path/to/file a path on disk Some databases, such as Microsoft SQL Server, or Oracle Database support a path component (ie, /dbname) in the form of /instance/dbname, where /instance is the optional service identifier (aka \"SID\") or database instance Driver Aliases usql supports the same driver names and aliases from the dburl package. Most databases have at least one or more alias - please refer to the dburl documentation for all supported aliases. Short Aliases All database drivers have a two character short form that is usually the first two letters of the database driver. For example, pg for postgres, my for mysql, ms for sqlserver (formerly known as mssql), or for oracle, or sq for sqlite3. Passing Driver Options Driver options are specified as standard URL query options in the form of ?opt1=a&obt2=b. Please refer to the relevant database driver's documentation for available options. Paths on Disk If a URL does not have a driver: scheme, usql will check if it is a path on disk. If the path exists, usql will attempt to use an appropriate database driver to open the path. If the specified path is a Unix Domain Socket, usql will attempt to open it using the MySQL driver. If the path is a directory, usql will attempt to open it using the PostgreSQL driver. If the path is a regular file, usql will attempt to open the file using the SQLite3 driver. Driver Defaults As with URLs, most components in the URL are optional and many components can be left out. usql will attempt connecting using defaults where possible: # connect to postgres using the local $USER and the unix domain socket in /var/run/postgresql $ usql pg:// Please see documentation for the database driver you are connecting with for more information. Connection Examples The following are example connection strings and additional ways to connect to databases using usql: # connect to a postgres database $ usql pg://user:pass@host/dbname $ usql pgsql://user:pass@host/dbname $ usql postgres://user:pass@host:port/dbname $ usql pg:// $ usql /var/run/postgresql $ usql pg://user:pass@host/dbname?sslmode=disable # Connect without SSL # connect to a mysql database $ usql my://user:pass@host/dbname $ usql mysql://user:pass@host:port/dbname $ usql my:// $ usql /var/run/mysqld/mysqld.sock # connect to a sqlserver database $ usql sqlserver://user:pass@host/instancename/dbname $ usql ms://user:pass@host/dbname $ usql ms://user:pass@host/instancename/dbname $ usql mssql://user:pass@host:port/dbname $ usql ms:// # connect to a sqlserver database using Windows domain authentication $ runas /user:ACME\\wiley /netonly \"usql mssql://host/dbname/\" # connect to a oracle database $ usql or://user:pass@host/sid $ usql oracle://user:pass@host:port/sid $ usql or:// # connect to a cassandra database $ usql ca://user:pass@host/keyspace $ usql cassandra://host/keyspace $ usql cql://host/ $ usql ca:// # connect to a sqlite database that exists on disk $ usql dbname.sqlite3 # NOTE: when connecting to a SQLite database, if the \"<driver>://\" or # \"<driver>:\" scheme/alias is omitted, the file must already exist on disk. # # if the file does not yet exist, the URL must incorporate file:, sq:, sqlite3:, # or any other recognized sqlite3 driver alias to force usql to create a new, # empty database at the specified path: $ usql sq://path/to/dbname.sqlite3 $ usql sqlite3://path/to/dbname.sqlite3 $ usql file:/path/to/dbname.sqlite3 # connect to a adodb ole resource (windows only) $ usql adodb://Microsoft.Jet.OLEDB.4.0/myfile.mdb $ usql \"adodb://Microsoft.ACE.OLEDB.12.0/?Extended+Properties=\\\"Text;HDR=NO;FMT=Delimited\\\"\" # connect with ODBC driver (requires building with odbc tag) $ cat /etc/odbcinst.ini [DB2] Description=DB2 driver Driver=/opt/db2/clidriver/lib/libdb2.so FileUsage = 1 DontDLClose = 1 [PostgreSQL ANSI] Description=PostgreSQL ODBC driver (ANSI version) Driver=psqlodbca.so Setup=libodbcpsqlS.so Debug=0 CommLog=1 UsageCount=1 # connect to db2, postgres databases using ODBC $ usql odbc+DB2://user:pass@localhost/dbname $ usql odbc+PostgreSQL+ANSI://user:pass@localhost/dbname?TraceFile=/path/to/trace.log Executing Queries and Commands The interactive intrepreter reads queries and meta (\\ ) commands, sending the query to the connected database: $ usql sqlite://example.sqlite3 Connected with driver sqlite3 (SQLite3 3.17.0) Type \"help\" for help. sq:example.sqlite3=> create table test (test_id int, name string); CREATE TABLE sq:example.sqlite3=> insert into test (test_id, name) values (1, 'hello'); INSERT 1 sq:example.sqlite3=> select * from test; test_id | name +---------+-------+ 1 | hello (1 rows) sq:example.sqlite3=> select * from test sq:example.sqlite3-> \\p select * from test sq:example.sqlite3-> \\g test_id | name +---------+-------+ 1 | hello (1 rows) sq:example.sqlite3=> \\c postgres://booktest@localhost error: pq: 28P01: password authentication failed for user \"booktest\" Enter password: Connected with driver postgres (PostgreSQL 9.6.6) pg:booktest@localhost=> select * from authors; author_id | name +-----------+----------------+ 1 | Unknown Master 2 | blah 3 | aoeu (3 rows) pg:booktest@localhost=> Commands may accept one or more parameter, and can be quoted using either ' or \". Command parameters may also be backtick'd. Backslash Commands Currently available commands: $ usql Type \"help\" for help. (not connected)=> \\? General \\q quit usql \\copyright show usql usage and distribution terms \\drivers display information about available database drivers Query Execute \\g [(OPTIONS)] [FILE] or ; execute query (and send results to file or |pipe) \\crosstabview [(OPTIONS)] [COLUMNS] execute query and display results in crosstab \\G [(OPTIONS)] [FILE] as \\g, but forces vertical output mode \\gexec execute query and execute each value of the result \\gset [PREFIX] execute query and store results in usql variables \\gx [(OPTIONS)] [FILE] as \\g, but forces expanded output mode \\watch [(OPTIONS)] [DURATION] execute query every specified interval Query Buffer \\e [FILE] [LINE] edit the query buffer (or file) with external editor \\p show the contents of the query buffer \\raw show the raw (non-interpolated) contents of the query buffer \\r reset (clear) the query buffer \\w FILE write query buffer to file Help \\? [commands] show help on backslash commands \\? options show help on usql command-line options \\? variables show help on special variables Input/Output \\echo [-n] [STRING] write string to standard output (-n for no newline) \\qecho [-n] [STRING] write string to \\o output stream (-n for no newline) \\warn [-n] [STRING] write string to standard error (-n for no newline) \\o [FILE] send all query results to file or |pipe \\i FILE execute commands from file \\ir FILE as \\i, but relative to location of current script Informational \\d[S+] [NAME] list tables, views, and sequences or describe table, view, sequence, or index \\da[S+] [PATTERN] list aggregates \\df[S+] [PATTERN] list functions \\di[S+] [PATTERN] list indexes \\dm[S+] [PATTERN] list materialized views \\dn[S+] [PATTERN] list schemas \\ds[S+] [PATTERN] list sequences \\dt[S+] [PATTERN] list tables \\dv[S+] [PATTERN] list views \\l[+] list databases \\ss[+] [TABLE|QUERY] [k] show stats for a table or a query Formatting \\pset [NAME [VALUE]] set table output option \\a toggle between unaligned and aligned output mode \\C [STRING] set table title, or unset if none \\f [STRING] show or set field separator for unaligned query output \\H toggle HTML output mode \\T [STRING] set HTML <table> tag attributes, or unset if none \\t [on|off] show only rows \\x [on|off|auto] toggle expanded output Transaction \\begin begin a transaction \\commit commit current transaction \\rollback rollback (abort) current transaction Connection \\c URL connect to database with url \\c DRIVER PARAMS... connect to database with SQL driver and parameters \\Z close database connection \\password [USERNAME] change the password for a user \\conninfo display information about the current database connection Operating System \\cd [DIR] change the current working directory \\setenv NAME [VALUE] set or unset environment variable \\! [COMMAND] execute command in shell or start interactive shell \\timing [on|off] toggle timing of commands Variables \\prompt [-TYPE] <VAR> [PROMPT] prompt user to set variable \\set [NAME [VALUE]] set internal variable, or list all if no parameters \\unset NAME unset (delete) internal variable Features and Compatibility The usql project's goal is to support all standard psql commands and features. Pull Requests are always appreciated! Variables and Interpolation usql supports client-side interpolation of variables that can be \\set and \\unset: $ usql (not connected)=> \\set (not connected)=> \\set FOO bar (not connected)=> \\set FOO = 'bar' (not connected)=> \\unset FOO (not connected)=> \\set (not connected)=> A \\set variable, NAME, will be directly interpolated (by string substitution) into the query when prefixed with : and optionally surrounded by quotation marks (' or \"): pg:booktest@localhost=> \\set FOO bar pg:booktest@localhost=> select * from authors where name = :'FOO'; author_id | name +-----------+------+ 7 | bar (1 rows) The three forms, :NAME, :'NAME', and :\"NAME\", are used to interpolate a variable in parts of a query that may require quoting, such as for a column name, or when doing concatenation in a query: pg:booktest@localhost=> \\set TBLNAME authors pg:booktest@localhost=> \\set COLNAME name pg:booktest@localhost=> \\set FOO bar pg:booktest@localhost=> select * from :TBLNAME where :\"COLNAME\" = :'FOO' pg:booktest@localhost-> \\p select * from authors where \"name\" = 'bar' pg:booktest@localhost-> \\raw select * from :TBLNAME where :\"COLNAME\" = :'FOO' pg:booktest@localhost-> \\g author_id | name +-----------+------+ 7 | bar (1 rows) pg:booktest@localhost=> Note: variables contained within other strings will NOT be interpolated: pg:booktest@localhost=> select ':FOO'; ?column? +----------+ :FOO (1 rows) pg:booktest@localhost=> \\p select ':FOO'; pg:booktest@localhost=> Backtick'd parameters Meta (\\ ) commands support backticks on parameters: (not connected)=> \\echo Welcome `echo $USER` -- 'currently:' \"(\" `date` \")\" Welcome ken -- currently: ( Wed Jun 13 12:10:27 WIB 2018 ) (not connected)=> Backtick'd parameters will be passed to the user's SHELL, exactly as written, and can be combined with \\set: pg:booktest@localhost=> \\set MYVAR `date` pg:booktest@localhost=> \\set MYVAR = 'Wed Jun 13 12:17:11 WIB 2018' pg:booktest@localhost=> \\echo :MYVAR Wed Jun 13 12:17:11 WIB 2018 pg:booktest@localhost=> Passwords usql supports reading passwords for databases from a .usqlpass file contained in the user's HOME directory at startup: $ cat $HOME/.usqlpass # format is: # protocol:host:port:dbname:user:pass postgres:*:*:*:booktest:booktest $ usql pg:// Connected with driver postgres (PostgreSQL 9.6.9) Type \"help\" for help. pg:booktest@=> Note: the .usqlpass file cannot be readable by other users. Please set the permissions accordingly: Runtime Configuration (RC) File usql supports executing a .usqlrc contained in the user's HOME directory: $ cat $HOME/.usqlrc \\echo WELCOME TO THE JUNGLE `date` \\set SYNTAX_HL_STYLE paraiso-dark $ usql WELCOME TO THE JUNGLE Thu Jun 14 02:36:53 WIB 2018 Type \"help\" for help. (not connected)=> \\set SYNTAX_HL_STYLE = 'paraiso-dark' (not connected)=> The .usqlrc file is read by usql at startup in the same way as a file passed on the command-line with -f / --file. It is commonly used to set startup environment variables and settings. You can temporarily disable the RC-file by passing -X or --no-rc on the command-line: Host Connection Information By default, usql displays connection information when connecting to a database. This might cause problems with some databases or connections. This can be disabled by setting the system environment variable USQL_SHOW_HOST_INFORMATION to false: $ export USQL_SHOW_HOST_INFORMATION=false $ usql pg://booktest@localhost Type \"help\" for help. pg:booktest@=> SHOW_HOST_INFORMATION is a standard usql variable, and can be \\set or \\unset. Additionally, it can be passed via the command-line using -v or --set: $ usql --set SHOW_HOST_INFORMATION=false pg:// Type \"help\" for help. pg:booktest@=> \\set SHOW_HOST_INFORMATION true pg:booktest@=> \\connect pg:// Connected with driver postgres (PostgreSQL 9.6.9) pg:booktest@=> Syntax Highlighting Interactive queries will be syntax highlighted by default, using Chroma. There are a number of variables that control syntax highlighting: Variable Default Values Description SYNTAX_HL true true or false enables syntax highlighting SYNTAX_HL_FORMAT dependent on terminal support formatter name Chroma formatter name SYNTAX_HL_OVERRIDE_BG true true or false enables overriding the background color of the chroma styles SYNTAX_HL_STYLE monokai style name Chroma style name Time Formatting Some databases support time/date columns that support formatting. By default, usql formats time/date columns as RFC3339Nano, and can be set using \\pset time <FORMAT>: $ usql pg:// Connected with driver postgres (PostgreSQL 13.2 (Debian 13.2-1.pgdg100+1)) Type \"help\" for help. pg:postgres@=> \\pset time RFC3339Nano pg:postgres@=> select now(); now ----------------------------- 2021-05-01T22:21:44.710385Z (1 row) pg:postgres@=> \\pset time Kitchen Time display is \"Kitchen\" (\"3:04PM\"). pg:postgres@=> select now(); now --------- 10:22PM (1 row) pg:postgres@=> Any Go supported time format or the standard Go const name (for example, Kitchen, in the above). Constants Constant Name Value ANSIC Mon Jan _2 15:04:05 2006 UnixDate Mon Jan _2 15:04:05 MST 2006 RubyDate Mon Jan 02 15:04:05 -0700 2006 RFC822 02 Jan 06 15:04 MST RFC822Z 02 Jan 06 15:04 -0700 RFC850 Monday, 02-Jan-06 15:04:05 MST RFC1123 Mon, 02 Jan 2006 15:04:05 MST RFC1123Z Mon, 02 Jan 2006 15:04:05 -0700 RFC3339 2006-01-02T15:04:05Z07:00 RFC3339Nano 2006-01-02T15:04:05.999999999Z07:00 Kitchen 3:04PM Stamp Jan _2 15:04:05 StampMilli Jan _2 15:04:05.000 StampMicro Jan _2 15:04:05.000000 StampNano Jan _2 15:04:05.000000000 Copy usql implements the \\copy command that reads data from a database connection and writes it into another one. It requires 4 parameters: source connection string destination connection string source query destination table name, optionally with columns Connection strings support same syntax as in \\connect. Source query needs to be quoted. Source query must select same number of columns and in same order as they're defined in the destination table, unless they're specified for the destination, as table_name(column1, column2, ...). Quote the whole expression, if it contains spaces. \\copy does not attempt to perform any data type conversion. Use CAST in the source query to ensure data types compatible with destination table. Some drivers may have limited data type support, and they might not work at all when combined with other limited drivers. Unlike psql, \\copy in usql cannot read data directly from files. Drivers like csvq can help with this, since they support reading CSV and JSON files. $ cat books.csv book_id,author_id,isbn,title,year,available,tags 3,1,3,one,2018,\"2018-06-01 00:00:00\",{} 4,2,4,two,2019,\"2019-06-01 00:00:00\",{} $ usql -c \"\\copy csvq://. sqlite3://test.db 'select * from books' 'books'\" Copied 2 rows Note that it might be a better idea to use tools dedicated to the destination database to load data in a robust way. \\copy reads data from plain SELECT queries. Most drivers that have \\copy enabled use INSERT statements, except for PostgreSQL ones, which use COPY TO. Because data needs to be downloaded from one database and uploaded into another, don't expect same performance as in psql. For loading large amount of data efficiently, use tools native to the destination database. You can use \\copy with variables. Better yet, put those \\set commands in your runtime configuration file at $HOME/.usqlrc and passwords at $HOME/.usqlpass. $ usql Type \"help\" for help. (not connected)=> \\set pglocal postgres://postgres@localhost:49153?sslmode=disable (not connected)=> \\set oralocal godror://system@localhost:1521/orasid (not connected)=> \\copy :pglocal :oralocal 'select staff_id, first_name from staff' 'staff(staff_id, first_name)' Contributing usql is currently a WIP, and is aiming towards a 1.0 release soon. Well-written PRs are always welcome -- and there is a clear backlog of issues marked help wanted on the GitHub issue tracker! Please pick up an issue today, and submit a PR tomorrow! For more technical details, see CONTRIBUTING.md. Related Projects dburl - Go package providing a standard, URL-style mechanism for parsing and opening database connection URLs xo - Go command-line tool to generate Go code from a database schema ",
        "I have been looking for a faster alternative to pgcli [1] ( with  features like auto-complete for table names and columns) and was getting excited.<p>Sadly this is \"just\" a plain old CLI.<p>Also: had to install with `go get -tags \"no_sqlite3\" -u github.com/xo/usql` since the sqlite3 package did not build.<p>[1] <a href=\"https://www.pgcli.com/\" rel=\"nofollow\">https://www.pgcli.com/</a>",
        "I was really hoping for \\dt but looks like they haven't implemented it yet. The number one most common thing I look up when I'm using a SQL variant is how to show the tables. I suppose there would be complications with NoSQL but you could just show available collections or whatever else it maps to in that case."
      ],
      "relevant": "true"
    },
    {
      "id": 21196709,
      "title": "Jtc – CLI tool to extract, manipulate and transform source JSON",
      "search": [
        "Jtc – CLI tool to extract, manipulate and transform source JSON",
        "https://github.com/ldn-softdev/jtc",
        "jtc - cli tool to extract, manipulate and transform source JSON jtc stand for: JSON transformational chains (used to be JSON test console). jtc offers a powerful way to select one or multiple elements from a source JSON and apply various actions on the selected elements at once (wrap selected elements into a new JSON, filter in/out, sort elements, update elements, insert new elements, remove, copy, move, compare, transform, swap around and many other operations). Enhancement requests and/or questions are more than welcome: ldn.softdev@gmail.com Content: Short description Compilation and installation options Linux and MacOS precompiled binaries Installing via MacPorts Installation on Linux distributions Manual installation Release Notes Quick-start guide list all URLs dump all bookmark names dump all URL's names dump all the URLs and corresponding names Debugging and validating JSON Complete User Guide C++ class and interface usage primer jtc vs jq utility ideology learning curve handling irregular JSONs solutions input invariance programming model JSON numerical fidelity performance compare jtc based solutions with jq's Short description - jtc is a simple yet very powerful and efficient cli utility tool to process and manipulate JSON data jtc offers following features (a short list of main features): simple user interface allowing applying a bulk of changes in a single or chained sets of commands featured walk-path interface lets extracting any combination of data from sourced JSON trees extracted data is representable either as found, or could be encapsulated in JSON array/object or transformed using templates support Regular Expressions when searching source JSON fast and efficient processing of very large JSON files (various built-in search caches) insert/update operations optionally may undergo shell cli evaluation support in-place modifications of the input/source JSON file features namespaces, facilitating interpolation of preserved JSON values in templates supports buffered and streamed modes of input read sports concurrent input JSON reading/parsing (on multi-core CPU) written entirely in C++14, no dependencies (STL only, idiomatic C++, no memory leaks) extensively debuggable conforms JSON specification (json.org) The walk-path feature is easy to understand - it's only made of 2 kinds of lexemes traversing JSON tree, which could be mixed up in any order: subscripts - enclosed into [, ], subscripts let traversing JSON tree downwards (towards the leaves) and upwards (towards the root) search lexemes - encased as <..> or >..< (for a recursive and non-recursive searches respectively); search lexemes facilitate various match criteria defined by an optional suffix and/or quantifier There's also a 3rd kind of lexemes - directives: they typically facilitate other functions like working with namespaces, controlling walk-path execution, etc; directives are syntactically similar to the search lexemes All lexemes can be iterable: iterable subscripts let iterating over children of currently addressed JSON iterables nodes (arrays/objects), while iterable search lexemes let iterating over all (recursive) matches for a given search criteria A walk-path may have an arbitrary number of lexemes -the tool accepts a virtually unlimited number of walk paths. See below more detailed explanation with examples Compilation and installation options For compiling, c++14 (or later) is required. To compile under different platforms: MacOS/BSD: c++ -o jtc -Wall -std=c++14 -Ofast jtc.cpp Linux: non-relocatable (dynamically linked) image: c++ -o jtc -Wall -std=gnu++14 -Ofast -pthread -lpthread jtc.cpp relocatable (statically linked) image: c++ -o jtc -Wall -std=gnu++14 -Ofast -static -Wl,--whole-archive -lrt -pthread -lpthread -Wl,--no-whole-archive jtc.cpp Debian: c++ -o jtc -Wall -std=c++14 -pthread -lpthread -Ofast jtc.cpp (ensure c++ poits to clang++-6.0 or above) Following debug related flags could be passed to jtc when compiling: -DNDEBUG: compile w/o debugs, however it's unadvisable - there's no performance gain from doing so -DNDBG_PARSER: disable debugs coming from parsing JSON (handy when deep debugging huge JSONs and want to skip parsing debugs) -DBG_FLOW: all debuggable function/method calls will disply an entry and exit points -DBG_mTS, -DBG_uTS: display absolute time-stamps in the debug: with millisecond accuracy and with microsecond accuracy respectively -DBG_dTS: used with either of 2 previous flags: makes time-stamp to display delta (since last debug message) instead of absolute stamp -DBG_CC: every call to a copy-constructor in Jnode class will reveal itself (handy for optimization debugging) Linux and MacOS precompiled binaries are available for download Choose the latest precompiled binary: latest macOS if you don't want to go through macOS security hurdle, then remove the quarantine attribute from the file after binary download, e.g. (assuming you opened terminal in the folder where downloaded binary is): bash $ mv ./jtc-macos-64.latest ./jtc bash $ chmod 754 ./jtc bash $ xattr -r -d com.apple.quarantine ./jtc latest linux 64 bit latest linux 32 bit Rename the downloaded file and give proper permissions. E.g., for the latest macOS: mv jtc-macos-64.latest jtc chmod 754 jtc Packaged installations: Installing via MacPorts On MacOS, you can install jtc via the MacPorts package manager: $ sudo port selfupdate $ sudo port install jtc Installation on Linux distributions jtc is packaged in the following Linux distributions and can be installed via the package manager. Fedora: jtc is present in Fedora 31 and later: openSUSE: jtc can be installed on openSUSE Tumbleweed via zypper: or on Leap 15.0 and later by adding the utilities repository and installing jtc via zypper. Manual installation: download jtc-master.zip, unzip it, descend into unzipped folder, compile using an appropriate command, move compiled file into an install location. here're the example steps for MacOS: say, jtc-master.zip has been downloaded to a folder and the terminal app is open in that folder: unzip jtc-master.zip cd jtc-master c++ -o jtc -Wall -std=c++17 -Ofast jtc.cpp sudo mv ./jtc /usr/local/bin/ Release Notes See the latest Release Notes Quick-start guide: run the command jtc -g to read the mini USER-GUIDE, with walk path syntax, usage notes, short examples read the examples just below see stackoverflow-json for lots of worked examples based on Stack Overflow questions refer to the complete User Guide for further examples and guidelines. Consider a following JSON (a mockup of a bookmark container), stored in a file Bookmarks: { \"Bookmarks\": [ { \"children\": [ { \"children\": [ { \"name\": \"The New York Times\", \"stamp\": \"2017-10-03, 12:05:19\", \"url\": \"https://www.nytimes.com/\" }, { \"name\": \"HuffPost UK\", \"stamp\": \"2017-11-23, 12:05:19\", \"url\": \"https://www.huffingtonpost.co.uk/\" } ], \"name\": \"News\", \"stamp\": \"2017-10-02, 12:05:19\" }, { \"children\": [ { \"name\": \"Digital Photography Review\", \"stamp\": \"2017-02-27, 12:05:19\", \"url\": \"https://www.dpreview.com/\" } ], \"name\": \"Photography\", \"stamp\": \"2017-02-27, 12:05:19\" } ], \"name\": \"Personal\", \"stamp\": \"2017-01-22, 12:05:19\" }, { \"children\": [ { \"name\": \"Stack Overflow\", \"stamp\": \"2018-05-01, 12:05:19\", \"url\": \"https://stackoverflow.com/\" }, { \"name\": \"C++ reference\", \"stamp\": \"2018-06-21, 12:05:19\", \"url\": \"https://en.cppreference.com/\" } ], \"name\": \"Work\", \"stamp\": \"2018-03-06, 12:07:29\" } ] } 1. let's start with a simple thing - list all URLs: bash $ jtc -w'<url>l:' Bookmarks \"https://www.nytimes.com/\" \"https://www.huffingtonpost.co.uk/\" \"https://www.dpreview.com/\" \"https://stackoverflow.com/\" \"https://en.cppreference.com/\" Let's take a look at the walk-path <url>l:: search lexemes are enclosed in angular brackets <, > - that style provides a recursive search throughout JSON suffix l instructs to search among labels only quantifier : instructs to find all occurrences, such quantifiers makes a path iterable 2. dump all bookmark names from the Work folder: bash $ jtc -w'<Work>[-1][children][:][name]' Bookmarks \"Stack Overflow\" \"C++ reference\" Here the walk-path <Work>[-1][children][:][name] is made of following lexemes: a. <Work>: find within a JSON tree the first occurrence where the JSON string value is matching \"Work\" exactly b. [-1]: step up one tier in the JSON tree structure (i.e., address an immediate parent of the found JSON element) c. [children]: select/address a node whose label is \"children\" (it'll be a JSON array, at the same tier with Work) d. [:]: select each node in the array e. [name]: select/address a node with the label \"name\" in order to understand better how the walk-path works, let's run that series of cli in a slow-motion, gradually adding lexemes to the path one by one, perhaps with the option -l to see also the labels (if any) of the selected elements: bash $ jtc -w'<Work>' -l Bookmarks \"name\": \"Work\" bash $ jtc -w'<Work>[-1]' -l Bookmarks { \"children\": [ { \"name\": \"Stack Overflow\", \"stamp\": \"2018-05-01, 12:05:19\", \"url\": \"https://stackoverflow.com/\" }, { \"name\": \"C++ reference\", \"stamp\": \"2018-06-21, 12:05:19\", \"url\": \"https://en.cppreference.com/\" } ], \"name\": \"Work\", \"stamp\": \"2018-03-06, 12:07:29\" } bash $ jtc -w'<Work>[-1][children]' -l Bookmarks \"children\": [ { \"name\": \"Stack Overflow\", \"stamp\": \"2018-05-01, 12:05:19\", \"url\": \"https://stackoverflow.com/\" }, { \"name\": \"C++ reference\", \"stamp\": \"2018-06-21, 12:05:19\", \"url\": \"https://en.cppreference.com/\" } ] bash $ jtc -w'<Work>[-1][children][:]' -l Bookmarks { \"name\": \"Stack Overflow\", \"stamp\": \"2018-05-01, 12:05:19\", \"url\": \"https://stackoverflow.com/\" } { \"name\": \"C++ reference\", \"stamp\": \"2018-06-21, 12:05:19\", \"url\": \"https://en.cppreference.com/\" } bash $ jtc -w'<Work>[-1][children][:][name]' -l Bookmarks \"name\": \"Stack Overflow\" \"name\": \"C++ reference\" B.t.w., a better (a bit faster and more efficient) walk-path achieving the same query would be this: jtc -w'<Work>[-1][children]<name>l:' Bookmarks 3. dump all URL's names: bash $ jtc -w'<url>l:[-1][name]' Bookmarks \"The New York Times\" \"HuffPost UK\" \"Digital Photography Review\" \"Stack Overflow\" \"C++ reference\" this walk-path <url>l:[-1][name]: finds recursively (encasement <, >) each (:) JSON element with a label (l) matching url then for an each found JSON element, select its parent ([-1]) then, select a JSON (sub)element with the label \"name\" 4. dump all the URLs and their corresponding names, preferably wrap found pairs in JSON array: bash $ jtc -w'<url>l:' -w'<url>l:[-1][name]' -jl Bookmarks [ { \"name\": \"The New York Times\", \"url\": \"https://www.nytimes.com/\" }, { \"name\": \"HuffPost UK\", \"url\": \"https://www.huffingtonpost.co.uk/\" }, { \"name\": \"Digital Photography Review\", \"url\": \"https://www.dpreview.com/\" }, { \"name\": \"Stack Overflow\", \"url\": \"https://stackoverflow.com/\" }, { \"name\": \"C++ reference\", \"url\": \"https://en.cppreference.com/\" } ] yes, multiple walks (-w) are allowed option -j will wrap the walked outputs into a JSON array, but not just, option -l used together with -j will ensure relevant walks are grouped together (try without -l) if multiple walks (-w) are present, by default, walked results will be printed interleaved (if it can be interleaved) 5. Debugging and validating JSON jtc is extensively debuggable: the more times option -d is passed the more debugs will be produced. Enabling too many debugs might be overwhelming, though one specific case many would find extremely useful - when validating a failing JSON: bash $ <addressbook-sample.json jtc jtc json exception: expected_json_value If JSON is big, it's desirable to locate the parsing failure point. Passing just one -d let easily spotting the parsing failure point and its locus: bash $ <addressbook-sample.json jtc -d .display_opts(), option set[0]: -d (internally imposed: ) .init_inputs(), reading json from <stdin> .exception_locus_(), ... }| ],| \"children\": [,],| \"spouse\": null| },| {| ... .exception_spot_(), -------------------------------------------->| (offset: 967) jtc json parsing exception (<stdin>:967): expected_json_value bash $ Complete User Guide there's a lot more under the hood of jtc: various viewing options, directives allowing controlling walks, preserving parts of whole JSONs in namespaces, walking with various criteria, etc interpolating namespaces and walk results in templates and lexemes amending input JSONs via purge/swap/update/insert/move/merge operations comparing JSONs (or their parts) or their schemas various processing modes (streamed, buffered, concurrent parsing, chaining operations, etc) and more ... Refer to a complete User Guide for further examples and guidelines. C++ class and interface usage primer Refer to a Class usage primer document. jtc vs jq: jtc was inspired by the complexity of jq interface (and its DSL), aiming to provide users a tool which would let attaining the desired JSON queries in an easier, more feasible and succinct way utility ideology: jq is a stateful processor with own DSL, variables, operations, control flow logic, IO system, etc, etc jtc is a unix utility confining its functionality to operation types with its data model only (as per unix ideology). jtc performs one major operation at a time (like insertion, update, swap, etc), however multiple operations could be chained using / delimiter jq is non-idiomatic in a unix way, e.g.: one can write a program in jq language that even has nothing to do with JSON. Most of the requests (if not all) to manipulate JSONs are ad hoc type of tasks, and learning jq's DSL for ad hoc type of tasks is an overkill (that purpose is best facilitated with GPL, e.g.: Python). The number of asks on the stackoverflow to facilitate even simple queries for jq is huge - that's the proof in itself that for many people feasibility of attaining their asks with jq is a way too low, hence they default to posting their questions on the forum. jtc on the other hand is a utility (not a language), which employs a novel but powerful concept, which \"embeds\" the ask right into the walk-path. That facilitates a much higher feasibility of attaining a desired result: building a walk-path a lexeme by lexeme, one at a time, provides an immediate visual feedback and let coming up with the desired result rather quickly. learning curve: jq: before you could come up with a query to handle even a relatively simple ask, you need to become an expert in jq language, which will take some time. Coming up with the complex queries requires what it seems having a PhD in jq, or spending lots of time on stackoverflow and similar forums jtc employs only a simple (but powerful) concept of the walk-path (which is made only of 2 types of search lexemes, each type though has several variants) which is quite easy to grasp. handling irregular JSONs: jq: handling irregular JSONs for jq is not a challenge, building a query is! The more irregularities you need to handle the more challenging the query (jq program) becomes jtc was conceived with the idea of being capable of handling complex irregular JSONs with a simplified interface - that all is fitted into the concept of the walk-path, while daisy-chaining multiple operations is possible to satisfy almost every ask. solutions input invariance - most of jtc solutions would be input invariant (hardly the same could be stated for jq). Not that it's impossible to come up with invariant solutions in jq, it's just a lot more harder, while jtc with its walk-path model prompts for invariant solutions. I.e., the invariant solution will keep working even once the JSON outer format changes (the invariant solution only would stop working once the relationship between walked JSON elements changes). E.g.: consider a following query, extract format [ \"name\", \"surname\" ] from 2 types of JSON: bash $ case1='{\"Name\":\"Patrick\", \"Surname\":\"Lynch\", \"gender\":\"male\", \"age\":29}' bash $ case2='[{\"Name\":\"Patrick\", \"Surname\":\"Lynch\", \"gender\":\"male\", \"age\":29},{\"Name\":\"Alice\", \"Surname\":\"Price\", \"gender\":\"female\", \"age\":27}]' a natural, idiomatic jtc solution would be: bash $ <<<$case1 jtc -w'<Name>l:[-1]' -rT'[{{$a}},{{$b}}]' [ \"Patrick\", \"Lynch\" ] bash $ <<<$case2 jtc -w'<Name>l:[-1]' -rT'[{{$a}},{{$b}}]' [ \"Patrick\", \"Lynch\" ] [ \"Alice\", \"Price\" ] While one of the most probable jq solution would be: bash $ <<<$case1 jq -c 'if type == \"array\" then .[] else . end | [.Name, .Surname]' [\"Patrick\",\"Lynch\"] bash $ <<<$case2 jq -c 'if type == \"array\" then .[] else . end | [.Name, .Surname]' [\"Patrick\",\"Lynch\"] [\"Alice\",\"Price\"] The both solutions work correctly, however, any change in the outer encapsulation will break jq's solution , while jtc will keep working even if JSON is reshaped into an irregular structure, e.g.: #jtc: bash $ case3='{\"root\":[{\"Name\":\"Patrick\", \"Surname\":\"Lynch\", \"gender\":\"male\", \"age\":29}, {\"closed circle\":[{\"Name\":\"Alice\", \"Surname\":\"Price\", \"gender\":\"female\", \"age\":27}, {\"Name\":\"Rebecca\", \"Surname\":\"Hernandez\", \"gender\":\"female\", \"age\":28}]}]}' bash $ bash $ <<<$case3 jtc -w'<Name>l:[-1]' -rT'[{{$a}},{{$b}}]' [ \"Patrick\", \"Lynch\" ] [ \"Alice\", \"Price\" ] [ \"Rebecca\", \"Hernandez\" ] #jq: bash $ <<<$case3 jq -c 'if type == \"array\" then .[] else . end | [.Name, .Surname]' [null,null] The same property makes jtc solutions resistant to cases of incomplete data, e.g.: if we drop \"Name\" entry from one of the entries in case 2, jtc solution still works correctly: #jtc: bash $ case2='[{\"Surname\":\"Lynch\", \"gender\":\"male\", \"age\":29},{\"Name\":\"Alice\", \"Surname\":\"Price\", \"gender\":\"female\", \"age\":27}]' bash $ bash $ <<<$case2 jtc -w'<Name>l:[-1]' -rT'[{{$a}},{{$b}}]' [ \"Alice\", \"Price\" ] #jq: bash $ <<<$case2 jq -c 'if type == \"array\" then .[] else . end | [.Name, .Surname]' [null,\"Lynch\"] [\"Alice\",\"Price\"] - i.e., jtc will not assume that user would require some default substitution in case of incomplete data (but if such handling is required then the walk-path can be easily enhanced) programming model jq is written in C language, which drags all intrinsic problems the language has dated its creation (here's what I mean) jtc is written in the idiomatic C++14 using STL only. jtc does not have a single naked memory allocation operator (those few new operators required for legacy interface are implemented as guards), nor it has a single naked pointer acting as a resource holder/owner, thus jtc is guaranteed to be free of memory/resources leaks (at least one class of the problems is off the table) - STL guaranty. Also, jtc is written in a very portable way, it should not cause problems compiling it under any unix like system. JSON numerical fidelity: jq is not compliant with JSON numerical definition. What jq does, it simply converts a symbolic numerical representation to an internal binary and keeps it that way. That approach: is not compliant with JSON definition of the numerical values it has problems retaining required precision might change original representation of numericals leads to incorrect processing of some JSON streams jtc validates all JSON numericals per JSON standard and keep numbers internally in their original literal format, so it's free of all the above caveats, compare: Handling jtc jq 1.6 Invalid Json: [00] <<<'[00]' jtc <<<'[00]' jq -c . Parsing result jtc json parsing exception (<stdin>:3): missed_prior_enumeration [0] Precision test: <<<'[0.99999999999999999]' jtc -r <<<'[0.99999999999999999]' jq -c . Parsing result [ 0.99999999999999999 ] [1] Retaining original format: <<<'[0.00001]' jtc -r <<<'[0.00001]' jq -c . Parsing result [ 0.00001 ] [1e-05] Stream of atomic JSONs: <<<'{}[]\"bar\"\"foo\"00123truefalsenull' jtc -Jr <<<'{}[]\"bar\"\"foo\"00123truefalsenull' jq -sc Parsing result [ {}, [], \"bar\", \"foo\", 0, 0, 123, true, false, null ] parse error: Invalid numeric literal at line 2, column 0 performance: jq is a single-threaded process jtc engages a concurrent (multi-threaded) reading/parsing when multiple files given (the advantage could be observed on multi-core CPU, though it become noticeable only with relatively big JSONs or with relatively big number of files processed) Comparison of single-threaded performance: Here's a 4+ million node JSON file standard.json: bash $ time jtc -zz standard.json 4329975 user 6.085 sec The table below compares jtc and jq performance for similar operations (using TIMEFORMAT=\"user %U sec\"): jtc 1.76 jq 1.6 parsing JSON: parsing JSON: bash $ time jtc -t2 standard.json | md5 bash $ time jq -M . standard.json | md5 d3b56762fd3a22d664fdd2f46f029599 d3b56762fd3a22d664fdd2f46f029599 user 9.110 sec user 18.853 sec removing by key from JSON: removing by key from JSON: bash $ time jtc -t2 -pw'<attributes>l:' standard.json | md5 bash $ time jq -M 'del(..|.attributes?)' standard.json | md5 0624aec46294399bcb9544ae36a33cd5 0624aec46294399bcb9544ae36a33cd5 user 10.027 sec user 27.439 sec updating JSON recursively by label: updating JSON recursively by label: bash $ time jtc -t2 -w'<attributes>l:[-1]' -i'{\"reserved\": null}' standard.json | md5 bash $ time jq -M 'walk(if type == \"object\" and has(\"attributes\") then . + { \"reserved\" : null } else . end)' standard.json | md5 6c86462ae6b71e10e3ea114e86659ab5 6c86462ae6b71e10e3ea114e86659ab5 user 12.715 sec user 29.450 sec Comparison of jtc to jtc (single-threaded to multi-threaded parsing performance): bash $ unset TIMEFORMAT bash $ bash $ # concurrent (multi-threaded) parsing: bash $ time jtc -J / -zz standard.json standard.json standard.json standard.json standard.json 21649876 real 0m10.995s # <- compare these figures user 0m34.083s sys 0m3.288s bash $ bash $ # sequential (single-threaded) parsing: bash $ time jtc -aJ / -zz standard.json standard.json standard.json standard.json standard.json 21649876 real 0m31.717s # <- compare these figures user 0m30.125s sys 0m1.555s bash $ Machine spec used for testing: Model Name: MacBook Pro Model Identifier: MacBookPro15,1 Processor Name: Intel Core i7 Processor Speed: 2,6 GHz Number of Processors: 1 Total Number of Cores: 6 L2 Cache (per Core): 256 KB L3 Cache: 12 MB Hyper-Threading Technology: Enabled Memory: 16 GB 2400 MHz DDR4 compare jtc based solutions with jq's: Here are published some answers for JSON queries using jtc, you may compare those with jq's, as well as study the feasibility of the solutions, test relevant performance, etc Refer to a complete User Guide for further examples and guidelines. ",
        "> jtc is written in idiomatic C++ (the most powerful programming language to date)<p>Citation needed. The `jq` vs `jtc` section is interesting, but author seems a little full of himself with some of the explanations.",
        "The author has also written related tools. One to convert XML to JSON and back (<a href=\"https://github.com/ldn-softdev/jtm\" rel=\"nofollow\">https://github.com/ldn-softdev/jtm</a>) and another to convert JSON to SQLite tables (<a href=\"https://github.com/ldn-softdev/jsl\" rel=\"nofollow\">https://github.com/ldn-softdev/jsl</a>). Combining these with the hxnormalize tool ( <a href=\"https://www.w3.org/Tools/HTML-XML-utils/man1/hxnormalize.html\" rel=\"nofollow\">https://www.w3.org/Tools/HTML-XML-utils/man1/hxnormalize.htm...</a>), one can do very sophisticated manipulation on HTML web pages.<p>HTML -> XML (via hxnormalize) -> JSON (via jtm) -> process using jtc (or even jq)"
      ],
      "relevant": "true"
    }
  ]
}
